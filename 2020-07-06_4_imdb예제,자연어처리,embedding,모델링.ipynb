{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test,y_test) = imdb.load_data(num_words=2000) # 가장 많이 쓰여진 단어 2000개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train) # 영화 리뷰 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0]) # 첫번째 리뷰문서의 길이(구의 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 2, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 2, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 2, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 656, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 2, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2, 83, 68, 2, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "       ...,\n",
       "       list([1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45, 2, 84, 2, 2, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 2, 1209, 2, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 6, 2, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2, 1702, 34, 2, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2, 234, 1119, 1574, 7, 496, 4, 139, 929, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 1818, 2, 4, 2, 2, 747, 1115, 372, 1890, 1006, 541, 2, 7, 4, 59, 2, 4, 2, 2]),\n",
       "       list([1, 1446, 2, 69, 72, 2, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2, 8, 4, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 719, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 2, 2, 9, 24, 6, 78, 1099, 17, 2, 2, 21, 27, 2, 2, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2, 272, 191, 1070, 6, 2, 8, 2, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train  # texts_to_sequences 데이터로 구성되어 있음 -> 각자 길이는 다르니까 패딩작업은 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = max(y_train) + 1 \n",
    "category  # 2개..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train # 0, 1로 보아 원핫인코딩 신경쓰지 말라는 배려 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pd = pd.DataFrame(y_train)\n",
    "y_train_pd[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 4, 2, 2, 33, 2, 4, 2, 432, 111, 153, 103, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>[1, 14, 9, 6, 2, 20, 21, 1517, 7, 2, 5, 2, 86,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>[1, 2, 2, 299, 6, 1042, 37, 80, 81, 233, 8, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>[1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>[1, 1446, 2, 69, 72, 2, 13, 610, 930, 8, 12, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>[1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0      [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, ...\n",
       "1      [1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 2,...\n",
       "2      [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2...\n",
       "3      [1, 4, 2, 2, 33, 2, 4, 2, 432, 111, 153, 103, ...\n",
       "4      [1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 1...\n",
       "...                                                  ...\n",
       "24995  [1, 14, 9, 6, 2, 20, 21, 1517, 7, 2, 5, 2, 86,...\n",
       "24996  [1, 2, 2, 299, 6, 1042, 37, 80, 81, 233, 8, 40...\n",
       "24997  [1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45...\n",
       "24998  [1, 1446, 2, 69, 72, 2, 13, 610, 930, 8, 12, 5...\n",
       "24999  [1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8...\n",
       "\n",
       "[25000 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pd = pd.DataFrame(x_train)\n",
    "x_train_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2500개의 리뷰를 긍정/부정 카테고리로 매칭하는 모델\n",
    "### 범주화 2000개, 띄어쓰기 갯수는 100개까지 끊음, y는 0/1로 원핫인코딩 미시행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩시키기\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=100, padding='pre') # 너무 긴게 있을수도 있으니 maxlen 사용\n",
    "x_test = pad_sequences(x_test, maxlen=100, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n",
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train3 = pad_sequences(y_train1)\n",
    "y_test3 = pad_sequences(y_test1)\n",
    "y에서 원핫 인코딩 대신 pad_sequences 쓸 수 없는 이유 :\n",
    "pad_sequences는 2중 list, (위의 df를 보고오자)\n",
    "예를 들어 다양한 길이의 8000개 뉴스 기사데이터가 있을때 적용되는 함수임 (이러한 형태에서 사용한다는것을 기억하고 있어야 한다)\n",
    "지금 shape가 (8982,) 즉, 1차원인 y데이터에서는 1차원 -> 2차원의 0/1로 구성된 자릿값 원-핫 인코딩을 사용해줘야 한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-> 즉, 독립변수는 벡터화 시키고 (임베딩 역할)\n",
    "종속변수(분류의 경우)는 원-핫 인코딩 시켜야 한다는 말임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 질문) \n",
    "1) lstm,시계열 이런 키워드가 나왔는데 자연어처리랑 무슨 관련이 있는지? 막연히 최신 댓글이 더 영향력이 커서 그렇게 설명하신건가?\n",
    "아니면 input shape 필요없는거때문에 편리해서 쓰는건가?\n",
    "2) 종속변수도 임베딩으로 벡터화 시키면 안되나요..?!\n",
    "3) 로이터에서 pad_sequences 길이대로 임배딩 시키는게 제일 적합하다고 생각했는데 100이 아닌 1000으로 지정한 이유?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Flatten, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         200000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 317,761\n",
      "Trainable params: 317,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bitcamp\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 4s 179us/step - loss: 0.4802 - acc: 0.7500 - val_loss: 0.3740 - val_acc: 0.8348\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.3215 - acc: 0.8622 - val_loss: 0.3677 - val_acc: 0.8362\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 0.2782 - acc: 0.8827 - val_loss: 0.3810 - val_acc: 0.8364\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.2320 - acc: 0.9054 - val_loss: 0.3750 - val_acc: 0.8412\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 0.1454 - acc: 0.9449 - val_loss: 0.5424 - val_acc: 0.8192\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.0717 - acc: 0.9750 - val_loss: 0.6069 - val_acc: 0.8288\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 0.0420 - acc: 0.9847 - val_loss: 0.6482 - val_acc: 0.8352\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 0.0240 - acc: 0.9918 - val_loss: 0.7988 - val_acc: 0.8316\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 0.0217 - acc: 0.9919 - val_loss: 0.8777 - val_acc: 0.8294\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.0083 - acc: 0.9973 - val_loss: 1.1037 - val_acc: 0.8272\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.0167 - acc: 0.9952 - val_loss: 1.0410 - val_acc: 0.8224\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.0152 - acc: 0.9941 - val_loss: 1.1065 - val_acc: 0.8226\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.0144 - acc: 0.9953 - val_loss: 1.2151 - val_acc: 0.8144\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.0079 - acc: 0.9971 - val_loss: 1.4116 - val_acc: 0.8248\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 1.2798 - val_acc: 0.8334\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.0134 - acc: 0.9954 - val_loss: 1.1212 - val_acc: 0.8274\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 1.2671 - val_acc: 0.8324\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.0212 - acc: 0.9927 - val_loss: 1.0625 - val_acc: 0.8304\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.0061 - acc: 0.9983 - val_loss: 1.2380 - val_acc: 0.8354\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.3609 - val_acc: 0.8350\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 1.3712 - val_acc: 0.8332\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 8.9264e-04 - acc: 0.9998 - val_loss: 1.4944 - val_acc: 0.8348\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 7.9712e-05 - acc: 1.0000 - val_loss: 1.5743 - val_acc: 0.8396\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 2.3476e-05 - acc: 1.0000 - val_loss: 1.5985 - val_acc: 0.8402\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.7742e-05 - acc: 1.0000 - val_loss: 1.6212 - val_acc: 0.8394\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 1.4498e-05 - acc: 1.0000 - val_loss: 1.6398 - val_acc: 0.8400\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.2161e-05 - acc: 1.0000 - val_loss: 1.6545 - val_acc: 0.8400\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 1.0440e-05 - acc: 1.0000 - val_loss: 1.6713 - val_acc: 0.8402\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 9.1371e-06 - acc: 1.0000 - val_loss: 1.6863 - val_acc: 0.8404\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 8.0476e-06 - acc: 1.0000 - val_loss: 1.6994 - val_acc: 0.8400\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 7.1773e-06 - acc: 1.0000 - val_loss: 1.7131 - val_acc: 0.8400\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 6.4223e-06 - acc: 1.0000 - val_loss: 1.7248 - val_acc: 0.8396\n",
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 5.7853e-06 - acc: 1.0000 - val_loss: 1.7373 - val_acc: 0.8396\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 5.2319e-06 - acc: 1.0000 - val_loss: 1.7495 - val_acc: 0.8396\n",
      "Epoch 35/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 4.7537e-06 - acc: 1.0000 - val_loss: 1.7597 - val_acc: 0.8398\n",
      "Epoch 36/100\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 4.3345e-06 - acc: 1.0000 - val_loss: 1.7708 - val_acc: 0.8400\n",
      "Epoch 37/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 3.9639e-06 - acc: 1.0000 - val_loss: 1.7809 - val_acc: 0.8402\n",
      "Epoch 38/100\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 3.6469e-06 - acc: 1.0000 - val_loss: 1.7920 - val_acc: 0.8402\n",
      "Epoch 39/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 3.3464e-06 - acc: 1.0000 - val_loss: 1.8020 - val_acc: 0.8398\n",
      "Epoch 40/100\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 3.0838e-06 - acc: 1.0000 - val_loss: 1.8113 - val_acc: 0.8398\n",
      "Epoch 41/100\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 2.8449e-06 - acc: 1.0000 - val_loss: 1.8212 - val_acc: 0.8398\n",
      "Epoch 42/100\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 2.6339e-06 - acc: 1.0000 - val_loss: 1.8311 - val_acc: 0.8402\n",
      "Epoch 43/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 2.4371e-06 - acc: 1.0000 - val_loss: 1.8400 - val_acc: 0.8404\n",
      "Epoch 44/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 2.2629e-06 - acc: 1.0000 - val_loss: 1.8498 - val_acc: 0.8398\n",
      "Epoch 45/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 2.1015e-06 - acc: 1.0000 - val_loss: 1.8588 - val_acc: 0.8398\n",
      "Epoch 46/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.9542e-06 - acc: 1.0000 - val_loss: 1.8675 - val_acc: 0.8400\n",
      "Epoch 47/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.8212e-06 - acc: 1.0000 - val_loss: 1.8771 - val_acc: 0.8400\n",
      "Epoch 48/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.6958e-06 - acc: 1.0000 - val_loss: 1.8857 - val_acc: 0.8398\n",
      "Epoch 49/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.5812e-06 - acc: 1.0000 - val_loss: 1.8948 - val_acc: 0.8400\n",
      "Epoch 50/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 1.4774e-06 - acc: 1.0000 - val_loss: 1.9028 - val_acc: 0.8400\n",
      "Epoch 51/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.3795e-06 - acc: 1.0000 - val_loss: 1.9115 - val_acc: 0.8398\n",
      "Epoch 52/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.2908e-06 - acc: 1.0000 - val_loss: 1.9199 - val_acc: 0.8398\n",
      "Epoch 53/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 1.2091e-06 - acc: 1.0000 - val_loss: 1.9287 - val_acc: 0.8398\n",
      "Epoch 54/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.1314e-06 - acc: 1.0000 - val_loss: 1.9364 - val_acc: 0.8398\n",
      "Epoch 55/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.0595e-06 - acc: 1.0000 - val_loss: 1.9448 - val_acc: 0.8396\n",
      "Epoch 56/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 9.9253e-07 - acc: 1.0000 - val_loss: 1.9530 - val_acc: 0.8398\n",
      "Epoch 57/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 9.3133e-07 - acc: 1.0000 - val_loss: 1.9613 - val_acc: 0.8398\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 93us/step - loss: 8.7396e-07 - acc: 1.0000 - val_loss: 1.9695 - val_acc: 0.8398\n",
      "Epoch 59/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 8.2055e-07 - acc: 1.0000 - val_loss: 1.9776 - val_acc: 0.8398\n",
      "Epoch 60/100\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 7.7041e-07 - acc: 1.0000 - val_loss: 1.9856 - val_acc: 0.8400\n",
      "Epoch 61/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 7.2499e-07 - acc: 1.0000 - val_loss: 1.9932 - val_acc: 0.8400\n",
      "Epoch 62/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 6.8029e-07 - acc: 1.0000 - val_loss: 2.0015 - val_acc: 0.8400\n",
      "Epoch 63/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 6.3941e-07 - acc: 1.0000 - val_loss: 2.0095 - val_acc: 0.8398\n",
      "Epoch 64/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 6.0171e-07 - acc: 1.0000 - val_loss: 2.0174 - val_acc: 0.8400\n",
      "Epoch 65/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 5.6597e-07 - acc: 1.0000 - val_loss: 2.0248 - val_acc: 0.8400\n",
      "Epoch 66/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 5.3244e-07 - acc: 1.0000 - val_loss: 2.0328 - val_acc: 0.8400\n",
      "Epoch 67/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 5.0142e-07 - acc: 1.0000 - val_loss: 2.0410 - val_acc: 0.8398\n",
      "Epoch 68/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 4.7218e-07 - acc: 1.0000 - val_loss: 2.0480 - val_acc: 0.8398\n",
      "Epoch 69/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 4.4470e-07 - acc: 1.0000 - val_loss: 2.0560 - val_acc: 0.8400\n",
      "Epoch 70/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 4.1917e-07 - acc: 1.0000 - val_loss: 2.0636 - val_acc: 0.8400\n",
      "Epoch 71/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 3.9462e-07 - acc: 1.0000 - val_loss: 2.0714 - val_acc: 0.8400\n",
      "Epoch 72/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 3.7202e-07 - acc: 1.0000 - val_loss: 2.0797 - val_acc: 0.8400\n",
      "Epoch 73/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 3.5076e-07 - acc: 1.0000 - val_loss: 2.0869 - val_acc: 0.8400\n",
      "Epoch 74/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 3.3080e-07 - acc: 1.0000 - val_loss: 2.0947 - val_acc: 0.8402\n",
      "Epoch 75/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 3.1212e-07 - acc: 1.0000 - val_loss: 2.1018 - val_acc: 0.8400\n",
      "Epoch 76/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 2.9436e-07 - acc: 1.0000 - val_loss: 2.1095 - val_acc: 0.8400\n",
      "Epoch 77/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 2.7758e-07 - acc: 1.0000 - val_loss: 2.1173 - val_acc: 0.8400\n",
      "Epoch 78/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 2.6176e-07 - acc: 1.0000 - val_loss: 2.1250 - val_acc: 0.8400\n",
      "Epoch 79/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 2.4720e-07 - acc: 1.0000 - val_loss: 2.1325 - val_acc: 0.8400\n",
      "Epoch 80/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 2.3328e-07 - acc: 1.0000 - val_loss: 2.1399 - val_acc: 0.8398\n",
      "Epoch 81/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 2.2050e-07 - acc: 1.0000 - val_loss: 2.1469 - val_acc: 0.8398\n",
      "Epoch 82/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 2.0788e-07 - acc: 1.0000 - val_loss: 2.1550 - val_acc: 0.8398\n",
      "Epoch 83/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.9652e-07 - acc: 1.0000 - val_loss: 2.1627 - val_acc: 0.8400\n",
      "Epoch 84/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 1.8552e-07 - acc: 1.0000 - val_loss: 2.1704 - val_acc: 0.8400\n",
      "Epoch 85/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.7504e-07 - acc: 1.0000 - val_loss: 2.1777 - val_acc: 0.8400\n",
      "Epoch 86/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 1.6546e-07 - acc: 1.0000 - val_loss: 2.1846 - val_acc: 0.8402\n",
      "Epoch 87/100\n",
      "20000/20000 [==============================] - 2s 89us/step - loss: 1.5627e-07 - acc: 1.0000 - val_loss: 2.1924 - val_acc: 0.8402\n",
      "Epoch 88/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.4761e-07 - acc: 1.0000 - val_loss: 2.2003 - val_acc: 0.8400\n",
      "Epoch 89/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 1.3938e-07 - acc: 1.0000 - val_loss: 2.2075 - val_acc: 0.8402\n",
      "Epoch 90/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 1.3180e-07 - acc: 1.0000 - val_loss: 2.2150 - val_acc: 0.8402\n",
      "Epoch 91/100\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 1.2453e-07 - acc: 1.0000 - val_loss: 2.2221 - val_acc: 0.8398\n",
      "Epoch 92/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.1775e-07 - acc: 1.0000 - val_loss: 2.2298 - val_acc: 0.8398\n",
      "Epoch 93/100\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.1135e-07 - acc: 1.0000 - val_loss: 2.2369 - val_acc: 0.8398\n",
      "Epoch 94/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 1.0521e-07 - acc: 1.0000 - val_loss: 2.2443 - val_acc: 0.8394\n",
      "Epoch 95/100\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 9.9443e-08 - acc: 1.0000 - val_loss: 2.2512 - val_acc: 0.8400\n",
      "Epoch 96/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 9.4090e-08 - acc: 1.0000 - val_loss: 2.2592 - val_acc: 0.8398\n",
      "Epoch 97/100\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 8.8797e-08 - acc: 1.0000 - val_loss: 2.2660 - val_acc: 0.8398\n",
      "Epoch 98/100\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 8.4111e-08 - acc: 1.0000 - val_loss: 2.2735 - val_acc: 0.8396\n",
      "Epoch 99/100\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 7.9506e-08 - acc: 1.0000 - val_loss: 2.2809 - val_acc: 0.8400\n",
      "Epoch 100/100\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 7.5196e-08 - acc: 1.0000 - val_loss: 2.2878 - val_acc: 0.8398\n",
      "25000/25000 [==============================] - 4s 167us/step\n",
      "acc : 0.8382400274276733\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# model.add(Embedding(2000, 100, input_length=100)) # 종류는 2000개고 구의 길이는 100개로 끊어줌\n",
    "model.add(Embedding(2000, 100))\n",
    "model.add(Conv1D(128,5,padding='same'))\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "# 여기서 strach out 됨....ㅠ 붙여서 써야겠네\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=200, validation_split=0.2)\n",
    "acc = model.evaluate(x_test,y_test)[1]\n",
    "print('acc :',acc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) 임베딩 레이어 = 범주화 2000가지(2000번째로 나온 phrase까지) & 아웃풋 노드 100\n",
    "2) Conv1D -> lstm으로 받아서 flatten & input shape 필요없음 (그래서 주로 쓰는듯?)\n",
    "3) 2가지 binary, sigmoid , Dense(1)로 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프로 학습곡선 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_val_loss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "plt.plot(y_val_loss, marker='.', c='red', label='Test Loss')\n",
    "plt.plot(y_loss, marker='.', c='black', label='Train Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid();plt.xlabel('epoch');plt.ylabel('loss');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "쓰레기를 만들었자너 ㅠ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
