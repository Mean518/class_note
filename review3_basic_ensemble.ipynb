{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "# ensemble 데이터 일단 합쳐서 해보자 (성능 놓으면 쪼갤필요가 없잖아..?)\n",
    "x = np.array([range(1,101), range(311,411), range(100),range(101,201), range(411,511), range(100,200)]).T \n",
    "y = np.array([range(711,811), range(711,811), range(100),range(501,601), range(711,811), range(100)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 11830.6055 - mse: 11830.6035 - acc: 0.4000 - val_loss: 187.0525 - val_mse: 187.0525 - val_acc: 0.0000e+00\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 55.4564 - mse: 55.4564 - acc: 0.4375 - val_loss: 24.0208 - val_mse: 24.0208 - val_acc: 0.0000e+00\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.5646 - mse: 3.5646 - acc: 0.4500 - val_loss: 1.6712 - val_mse: 1.6712 - val_acc: 0.0000e+00\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.2066 - mse: 0.2066 - acc: 0.4625 - val_loss: 0.0725 - val_mse: 0.0725 - val_acc: 0.0000e+00\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0140 - mse: 0.0140 - acc: 0.3875 - val_loss: 0.0087 - val_mse: 0.0087 - val_acc: 0.0000e+00\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.8098e-04 - mse: 5.8098e-04 - acc: 0.2875 - val_loss: 7.2043e-06 - val_mse: 7.2043e-06 - val_acc: 0.0000e+00\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.5008e-05 - mse: 2.5008e-05 - acc: 0.3000 - val_loss: 1.0760e-06 - val_mse: 1.0760e-06 - val_acc: 0.0000e+00\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.1425e-07 - mse: 1.1425e-07 - acc: 0.2875 - val_loss: 1.4635e-07 - val_mse: 1.4635e-07 - val_acc: 0.0000e+00\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.7847e-08 - mse: 2.7847e-08 - acc: 0.3375 - val_loss: 2.3451e-08 - val_mse: 2.3451e-08 - val_acc: 0.0000e+00\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.3663e-08 - mse: 1.3663e-08 - acc: 0.4250 - val_loss: 1.9732e-08 - val_mse: 1.9732e-08 - val_acc: 0.0500\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.5276e-08 - mse: 1.5276e-08 - acc: 0.3875 - val_loss: 2.5096e-08 - val_mse: 2.5096e-08 - val_acc: 0.4500\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.4598e-08 - mse: 1.4598e-08 - acc: 0.4250 - val_loss: 1.0542e-08 - val_mse: 1.0542e-08 - val_acc: 0.1000\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.7935e-08 - mse: 1.7935e-08 - acc: 0.3625 - val_loss: 1.7720e-08 - val_mse: 1.7720e-08 - val_acc: 0.3500\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.3642e-08 - mse: 1.3642e-08 - acc: 0.4000 - val_loss: 8.6938e-09 - val_mse: 8.6938e-09 - val_acc: 0.6500\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.4025e-08 - mse: 3.4025e-08 - acc: 0.2625 - val_loss: 1.4237e-08 - val_mse: 1.4237e-08 - val_acc: 0.7000\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.5431e-07 - mse: 1.5431e-07 - acc: 0.2000 - val_loss: 6.7205e-07 - val_mse: 6.7205e-07 - val_acc: 0.0000e+00\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.2501e-05 - mse: 1.2501e-05 - acc: 0.0625 - val_loss: 1.5595e-04 - val_mse: 1.5595e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.4827e-04 - mse: 5.4827e-04 - acc: 0.0625 - val_loss: 0.0010 - val_mse: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.1994 - mse: 0.1994 - acc: 0.0375 - val_loss: 15.7904 - val_mse: 15.7904 - val_acc: 0.0000e+00\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 348.3427 - mse: 348.3427 - acc: 0.1875 - val_loss: 24.3921 - val_mse: 24.3921 - val_acc: 1.0000\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 411.7622 - mse: 411.7624 - acc: 0.3375 - val_loss: 148.8865 - val_mse: 148.8865 - val_acc: 0.0000e+00\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 22.1311 - mse: 22.1311 - acc: 0.4125 - val_loss: 0.6715 - val_mse: 0.6715 - val_acc: 0.0000e+00\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.1475 - mse: 0.1475 - acc: 0.2125 - val_loss: 0.0470 - val_mse: 0.0470 - val_acc: 0.0000e+00\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0096 - mse: 0.0096 - acc: 0.3125 - val_loss: 0.0035 - val_mse: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0018 - mse: 0.0018 - acc: 0.3875 - val_loss: 0.0013 - val_mse: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.5728e-04 - mse: 5.5728e-04 - acc: 0.4375 - val_loss: 5.4083e-04 - val_mse: 5.4083e-04 - val_acc: 1.0000\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0627e-05 - mse: 4.0627e-05 - acc: 0.4875 - val_loss: 1.0326e-05 - val_mse: 1.0326e-05 - val_acc: 0.0000e+00\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 4.6773e-06 - mse: 4.6773e-06 - acc: 0.4250 - val_loss: 2.4960e-05 - val_mse: 2.4960e-05 - val_acc: 0.0000e+00\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.9517e-06 - mse: 1.9517e-06 - acc: 0.3875 - val_loss: 1.0190e-06 - val_mse: 1.0190e-06 - val_acc: 0.0000e+00\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.5093e-07 - mse: 5.5093e-07 - acc: 0.2250 - val_loss: 4.3445e-07 - val_mse: 4.3445e-07 - val_acc: 0.0000e+00\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.3526e-07 - mse: 5.3526e-07 - acc: 0.2125 - val_loss: 1.9468e-08 - val_mse: 1.9468e-08 - val_acc: 0.0000e+00\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.4913e-07 - mse: 5.4913e-07 - acc: 0.3250 - val_loss: 1.0951e-06 - val_mse: 1.0951e-06 - val_acc: 0.0000e+00\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.7509e-06 - mse: 2.7509e-06 - acc: 0.3125 - val_loss: 2.4771e-07 - val_mse: 2.4771e-07 - val_acc: 0.0000e+00\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.4131e-05 - mse: 2.4131e-05 - acc: 0.2500 - val_loss: 2.4566e-06 - val_mse: 2.4566e-06 - val_acc: 0.0000e+00\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.0422e-05 - mse: 3.0422e-05 - acc: 0.3125 - val_loss: 2.5798e-05 - val_mse: 2.5798e-05 - val_acc: 0.0000e+00\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.0133e-06 - mse: 5.0133e-06 - acc: 0.2750 - val_loss: 7.2765e-06 - val_mse: 7.2765e-06 - val_acc: 0.0000e+00\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.2083e-05 - mse: 2.2083e-05 - acc: 0.0375 - val_loss: 2.4632e-05 - val_mse: 2.4632e-05 - val_acc: 0.0000e+00\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 4.2307e-04 - mse: 4.2307e-04 - acc: 0.0250 - val_loss: 0.0158 - val_mse: 0.0158 - val_acc: 0.0000e+00\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0669 - mse: 0.0669 - acc: 0.0875 - val_loss: 0.2234 - val_mse: 0.2234 - val_acc: 0.0000e+00\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 8.6963 - mse: 8.6963 - acc: 0.0000e+00 - val_loss: 411.5402 - val_mse: 411.5402 - val_acc: 0.0000e+00\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 82.4087 - mse: 82.4087 - acc: 0.1000 - val_loss: 82.8831 - val_mse: 82.8831 - val_acc: 1.0000\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1310.4177 - mse: 1310.4177 - acc: 0.3875 - val_loss: 1621.3860 - val_mse: 1621.3861 - val_acc: 1.0000\n",
      "Epoch 43/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 289.0722 - mse: 289.0722 - acc: 0.3375 - val_loss: 29.6589 - val_mse: 29.6589 - val_acc: 1.0000\n",
      "Epoch 44/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 4.1868 - mse: 4.1868 - acc: 0.4125 - val_loss: 0.0335 - val_mse: 0.0335 - val_acc: 0.0000e+00\n",
      "Epoch 45/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0052 - mse: 0.0052 - acc: 0.4250 - val_loss: 0.0026 - val_mse: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 46/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0368 - mse: 0.0368 - acc: 0.3000 - val_loss: 0.0536 - val_mse: 0.0536 - val_acc: 0.0000e+00\n",
      "Epoch 47/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0163 - mse: 0.0163 - acc: 0.2375 - val_loss: 2.3648e-04 - val_mse: 2.3648e-04 - val_acc: 0.1000\n",
      "Epoch 48/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0149 - mse: 0.0149 - acc: 0.2000 - val_loss: 0.3282 - val_mse: 0.3282 - val_acc: 0.0000e+00\n",
      "Epoch 49/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.7047 - mse: 0.7047 - acc: 0.1875 - val_loss: 0.2966 - val_mse: 0.2966 - val_acc: 0.0000e+00\n",
      "Epoch 50/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.1234 - mse: 0.1234 - acc: 0.2750 - val_loss: 0.0348 - val_mse: 0.0348 - val_acc: 0.0000e+00\n",
      "Epoch 51/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0840 - mse: 0.0840 - acc: 0.1875 - val_loss: 0.3368 - val_mse: 0.3368 - val_acc: 1.0000\n",
      "Epoch 52/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 4.2135 - mse: 4.2135 - acc: 0.2125 - val_loss: 4.3915 - val_mse: 4.3915 - val_acc: 0.0000e+00\n",
      "Epoch 53/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1531.9184 - mse: 1531.9183 - acc: 0.2875 - val_loss: 8273.4081 - val_mse: 8273.4082 - val_acc: 0.0000e+00\n",
      "Epoch 54/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 607.6145 - mse: 607.6147 - acc: 0.3125 - val_loss: 17.9401 - val_mse: 17.9401 - val_acc: 1.0000\n",
      "Epoch 55/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.2836 - mse: 3.2836 - acc: 0.2375 - val_loss: 0.0270 - val_mse: 0.0270 - val_acc: 1.0000\n",
      "Epoch 56/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0048 - mse: 0.0048 - acc: 0.3500 - val_loss: 4.2508e-04 - val_mse: 4.2508e-04 - val_acc: 0.9500\n",
      "Epoch 57/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0021 - mse: 0.0021 - acc: 0.2625 - val_loss: 6.4801e-05 - val_mse: 6.4801e-05 - val_acc: 0.0000e+00\n",
      "Epoch 58/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 6.2974e-06 - mse: 6.2974e-06 - acc: 0.3625 - val_loss: 1.7099e-08 - val_mse: 1.7099e-08 - val_acc: 0.6000\n",
      "Epoch 59/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 8.6378e-09 - mse: 8.6378e-09 - acc: 0.4375 - val_loss: 9.8672e-09 - val_mse: 9.8672e-09 - val_acc: 0.8000\n",
      "Epoch 60/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.3557e-08 - mse: 1.3557e-08 - acc: 0.4625 - val_loss: 1.0448e-08 - val_mse: 1.0448e-08 - val_acc: 0.5500\n",
      "Epoch 61/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7713e-08 - mse: 1.7713e-08 - acc: 0.4000 - val_loss: 1.6669e-08 - val_mse: 1.6669e-08 - val_acc: 0.7500\n",
      "Epoch 62/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.6525e-08 - mse: 1.6525e-08 - acc: 0.4625 - val_loss: 3.8422e-08 - val_mse: 3.8422e-08 - val_acc: 0.0000e+00\n",
      "Epoch 63/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8275e-08 - mse: 3.8275e-08 - acc: 0.3750 - val_loss: 1.1637e-08 - val_mse: 1.1637e-08 - val_acc: 0.9000\n",
      "Epoch 64/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.9621e-08 - mse: 1.9621e-08 - acc: 0.3875 - val_loss: 1.1127e-08 - val_mse: 1.1127e-08 - val_acc: 0.7500\n",
      "Epoch 65/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.3773e-08 - mse: 2.3773e-08 - acc: 0.4250 - val_loss: 2.0724e-08 - val_mse: 2.0724e-08 - val_acc: 0.0000e+00\n",
      "Epoch 66/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.6101e-08 - mse: 1.6101e-08 - acc: 0.5000 - val_loss: 6.9852e-08 - val_mse: 6.9852e-08 - val_acc: 0.7000\n",
      "Epoch 67/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.6841e-08 - mse: 1.6841e-08 - acc: 0.4000 - val_loss: 1.6498e-08 - val_mse: 1.6498e-08 - val_acc: 0.9000\n",
      "Epoch 68/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 6.1468e-08 - mse: 6.1468e-08 - acc: 0.4375 - val_loss: 6.0282e-07 - val_mse: 6.0282e-07 - val_acc: 0.0000e+00\n",
      "Epoch 69/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.1826e-08 - mse: 6.1826e-08 - acc: 0.3375 - val_loss: 2.6832e-08 - val_mse: 2.6832e-08 - val_acc: 0.3500\n",
      "Epoch 70/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.6495e-08 - mse: 2.6494e-08 - acc: 0.5250 - val_loss: 8.8915e-08 - val_mse: 8.8915e-08 - val_acc: 0.0000e+00\n",
      "Epoch 71/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.8209e-08 - mse: 1.8209e-08 - acc: 0.4250 - val_loss: 1.0752e-08 - val_mse: 1.0752e-08 - val_acc: 1.0000\n",
      "Epoch 72/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.7754e-08 - mse: 8.7754e-08 - acc: 0.3875 - val_loss: 3.3169e-06 - val_mse: 3.3169e-06 - val_acc: 1.0000\n",
      "Epoch 73/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 4.1695e-04 - mse: 4.1695e-04 - acc: 0.3500 - val_loss: 6.7966e-04 - val_mse: 6.7966e-04 - val_acc: 1.0000\n",
      "Epoch 74/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0016 - mse: 0.0016 - acc: 0.3000 - val_loss: 0.0032 - val_mse: 0.0032 - val_acc: 0.2000\n",
      "Epoch 75/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0049 - mse: 0.0049 - acc: 0.3500 - val_loss: 9.4842e-04 - val_mse: 9.4842e-04 - val_acc: 1.0000\n",
      "Epoch 76/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.7301 - mse: 3.7301 - acc: 0.3750 - val_loss: 23.6652 - val_mse: 23.6652 - val_acc: 1.0000\n",
      "Epoch 77/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 742.1113 - mse: 742.1112 - acc: 0.4000 - val_loss: 131.1096 - val_mse: 131.1096 - val_acc: 1.0000\n",
      "Epoch 78/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 624.7121 - mse: 624.7119 - acc: 0.2500 - val_loss: 7.6474 - val_mse: 7.6474 - val_acc: 0.0000e+00\n",
      "Epoch 79/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.6264 - mse: 1.6264 - acc: 0.4000 - val_loss: 0.5161 - val_mse: 0.5161 - val_acc: 1.0000\n",
      "Epoch 80/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0310 - mse: 0.0310 - acc: 0.4875 - val_loss: 0.0038 - val_mse: 0.0038 - val_acc: 1.0000\n",
      "Epoch 81/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.0954e-04 - mse: 1.0954e-04 - acc: 0.3750 - val_loss: 5.6714e-06 - val_mse: 5.6714e-06 - val_acc: 1.0000\n",
      "Epoch 82/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.3396e-07 - mse: 6.3396e-07 - acc: 0.4250 - val_loss: 1.6425e-07 - val_mse: 1.6425e-07 - val_acc: 1.0000\n",
      "Epoch 83/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 4.4238e-08 - mse: 4.4238e-08 - acc: 0.3875 - val_loss: 2.2859e-08 - val_mse: 2.2859e-08 - val_acc: 0.0000e+00\n",
      "Epoch 84/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6253e-08 - mse: 1.6253e-08 - acc: 0.5000 - val_loss: 1.1045e-08 - val_mse: 1.1045e-08 - val_acc: 0.0500\n",
      "Epoch 85/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 9.3839e-09 - mse: 9.3839e-09 - acc: 0.3625 - val_loss: 2.1989e-08 - val_mse: 2.1989e-08 - val_acc: 0.7000\n",
      "Epoch 86/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.0421e-08 - mse: 2.0421e-08 - acc: 0.3500 - val_loss: 2.4230e-08 - val_mse: 2.4230e-08 - val_acc: 0.5500\n",
      "Epoch 87/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.9309e-08 - mse: 1.9309e-08 - acc: 0.3875 - val_loss: 6.0080e-09 - val_mse: 6.0080e-09 - val_acc: 0.1000\n",
      "Epoch 88/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.2656e-08 - mse: 1.2656e-08 - acc: 0.4000 - val_loss: 2.4661e-08 - val_mse: 2.4661e-08 - val_acc: 0.0000e+00\n",
      "Epoch 89/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.7653e-08 - mse: 3.7653e-08 - acc: 0.3625 - val_loss: 3.8575e-08 - val_mse: 3.8575e-08 - val_acc: 0.1500\n",
      "Epoch 90/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 7.9938e-07 - mse: 7.9938e-07 - acc: 0.3500 - val_loss: 7.5875e-08 - val_mse: 7.5875e-08 - val_acc: 0.1500\n",
      "Epoch 91/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.1933e-07 - mse: 1.1933e-07 - acc: 0.4750 - val_loss: 4.8031e-08 - val_mse: 4.8031e-08 - val_acc: 0.0500\n",
      "Epoch 92/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.4745e-07 - mse: 1.4745e-07 - acc: 0.3750 - val_loss: 2.6204e-07 - val_mse: 2.6204e-07 - val_acc: 1.0000\n",
      "Epoch 93/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 3ms/step - loss: 1.3676e-06 - mse: 1.3676e-06 - acc: 0.4875 - val_loss: 3.4057e-07 - val_mse: 3.4057e-07 - val_acc: 1.0000\n",
      "Epoch 94/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.5999e-06 - mse: 1.5999e-06 - acc: 0.4625 - val_loss: 1.0961e-06 - val_mse: 1.0961e-06 - val_acc: 0.4500\n",
      "Epoch 95/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 4.8066e-05 - mse: 4.8066e-05 - acc: 0.4250 - val_loss: 0.0016 - val_mse: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 96/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0752 - mse: 0.0752 - acc: 0.4875 - val_loss: 0.2174 - val_mse: 0.2174 - val_acc: 0.0000e+00\n",
      "Epoch 97/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 54.2788 - mse: 54.2788 - acc: 0.4875 - val_loss: 33.6523 - val_mse: 33.6523 - val_acc: 1.0000\n",
      "Epoch 98/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 46.9270 - mse: 46.9270 - acc: 0.3250 - val_loss: 1.8209 - val_mse: 1.8209 - val_acc: 0.0000e+00\n",
      "Epoch 99/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 19.7202 - mse: 19.7202 - acc: 0.4125 - val_loss: 6.0196 - val_mse: 6.0196 - val_acc: 0.0000e+00\n",
      "Epoch 100/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 49.8405 - mse: 49.8405 - acc: 0.3500 - val_loss: 187.7516 - val_mse: 187.7516 - val_acc: 0.0000e+00\n",
      "Epoch 101/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 66.2314 - mse: 66.2313 - acc: 0.3375 - val_loss: 3.5565 - val_mse: 3.5565 - val_acc: 0.0000e+00\n",
      "Epoch 102/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 671.8492 - mse: 671.8492 - acc: 0.3500 - val_loss: 278.3591 - val_mse: 278.3591 - val_acc: 0.0000e+00\n",
      "Epoch 103/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 69.1257 - mse: 69.1257 - acc: 0.3250 - val_loss: 2.9742 - val_mse: 2.9742 - val_acc: 0.0000e+00\n",
      "Epoch 104/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.5924 - mse: 2.5924 - acc: 0.3375 - val_loss: 0.0087 - val_mse: 0.0087 - val_acc: 0.0000e+00\n",
      "Epoch 105/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0221 - mse: 0.0221 - acc: 0.4250 - val_loss: 0.0488 - val_mse: 0.0488 - val_acc: 1.0000\n",
      "Epoch 106/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0128 - mse: 0.0128 - acc: 0.4875 - val_loss: 3.2483e-04 - val_mse: 3.2483e-04 - val_acc: 0.3000\n",
      "Epoch 107/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0509 - mse: 0.0509 - acc: 0.3750 - val_loss: 0.0049 - val_mse: 0.0049 - val_acc: 0.0000e+00\n",
      "Epoch 108/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0028 - mse: 0.0028 - acc: 0.4000 - val_loss: 7.4775e-05 - val_mse: 7.4775e-05 - val_acc: 0.0000e+00\n",
      "Epoch 109/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0020 - mse: 0.0020 - acc: 0.4125 - val_loss: 0.0013 - val_mse: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 110/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0743 - mse: 0.0743 - acc: 0.4375 - val_loss: 0.4529 - val_mse: 0.4529 - val_acc: 0.0000e+00\n",
      "Epoch 111/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3291 - mse: 0.3291 - acc: 0.4375 - val_loss: 0.2125 - val_mse: 0.2125 - val_acc: 1.0000\n",
      "Epoch 112/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.2146 - mse: 0.2146 - acc: 0.4125 - val_loss: 0.4812 - val_mse: 0.4812 - val_acc: 0.0000e+00\n",
      "Epoch 113/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 4.5057 - mse: 4.5057 - acc: 0.3625 - val_loss: 5.5284 - val_mse: 5.5284 - val_acc: 0.0000e+00\n",
      "Epoch 114/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 369.0228 - mse: 369.0229 - acc: 0.3625 - val_loss: 109.8409 - val_mse: 109.8409 - val_acc: 1.0000\n",
      "Epoch 115/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 28.6774 - mse: 28.6774 - acc: 0.4000 - val_loss: 14.6157 - val_mse: 14.6157 - val_acc: 0.0000e+00\n",
      "Epoch 116/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 22.1932 - mse: 22.1932 - acc: 0.4375 - val_loss: 6.1767 - val_mse: 6.1767 - val_acc: 1.0000\n",
      "Epoch 117/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.6452 - mse: 1.6452 - acc: 0.4625 - val_loss: 0.0206 - val_mse: 0.0206 - val_acc: 0.0000e+00\n",
      "Epoch 118/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0073 - mse: 0.0073 - acc: 0.4250 - val_loss: 0.0234 - val_mse: 0.0234 - val_acc: 0.0000e+00\n",
      "Epoch 119/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0051 - mse: 0.0051 - acc: 0.4500 - val_loss: 3.9525e-05 - val_mse: 3.9525e-05 - val_acc: 0.0000e+00\n",
      "Epoch 120/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.6179e-05 - mse: 3.6179e-05 - acc: 0.4250 - val_loss: 2.8926e-06 - val_mse: 2.8926e-06 - val_acc: 0.0000e+00\n",
      "Epoch 121/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.6735e-06 - mse: 1.6735e-06 - acc: 0.4250 - val_loss: 1.1460e-05 - val_mse: 1.1460e-05 - val_acc: 0.0000e+00\n",
      "Epoch 122/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.5280e-05 - mse: 2.5280e-05 - acc: 0.3500 - val_loss: 4.5369e-04 - val_mse: 4.5369e-04 - val_acc: 1.0000\n",
      "Epoch 123/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0042 - mse: 0.0042 - acc: 0.4250 - val_loss: 0.0233 - val_mse: 0.0233 - val_acc: 1.0000\n",
      "Epoch 124/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.3296 - mse: 0.3296 - acc: 0.3875 - val_loss: 3.6548 - val_mse: 3.6548 - val_acc: 1.0000\n",
      "Epoch 125/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.7546 - mse: 2.7546 - acc: 0.3875 - val_loss: 19.7146 - val_mse: 19.7146 - val_acc: 1.0000\n",
      "Epoch 126/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 505.0627 - mse: 505.0627 - acc: 0.3125 - val_loss: 65.8464 - val_mse: 65.8464 - val_acc: 0.0000e+00\n",
      "Epoch 127/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 11.3131 - mse: 11.3131 - acc: 0.2875 - val_loss: 2.9563 - val_mse: 2.9563 - val_acc: 1.0000\n",
      "Epoch 128/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4312 - mse: 0.4312 - acc: 0.4000 - val_loss: 0.3158 - val_mse: 0.3158 - val_acc: 1.0000\n",
      "Epoch 129/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5378 - mse: 0.5378 - acc: 0.4000 - val_loss: 1.0041 - val_mse: 1.0041 - val_acc: 1.0000\n",
      "Epoch 130/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.8648 - mse: 5.8648 - acc: 0.4875 - val_loss: 9.2373 - val_mse: 9.2373 - val_acc: 1.0000\n",
      "Epoch 131/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 29.2508 - mse: 29.2508 - acc: 0.4875 - val_loss: 0.5321 - val_mse: 0.5321 - val_acc: 0.1500\n",
      "Epoch 132/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.7083 - mse: 1.7083 - acc: 0.4500 - val_loss: 0.1393 - val_mse: 0.1393 - val_acc: 0.8500\n",
      "Epoch 133/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.7051 - mse: 0.7051 - acc: 0.4500 - val_loss: 1.4811 - val_mse: 1.4811 - val_acc: 1.0000\n",
      "Epoch 134/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.3425 - mse: 0.3425 - acc: 0.4625 - val_loss: 0.0080 - val_mse: 0.0080 - val_acc: 0.0000e+00\n",
      "Epoch 135/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.1869 - mse: 3.1869 - acc: 0.4375 - val_loss: 3.5114 - val_mse: 3.5114 - val_acc: 0.0000e+00\n",
      "Epoch 136/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.5400 - mse: 1.5400 - acc: 0.3625 - val_loss: 8.8759 - val_mse: 8.8759 - val_acc: 1.0000\n",
      "Epoch 137/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 503.9925 - mse: 503.9925 - acc: 0.4250 - val_loss: 741.0660 - val_mse: 741.0660 - val_acc: 1.0000\n",
      "Epoch 138/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 624.4535 - mse: 624.4533 - acc: 0.2250 - val_loss: 109.4765 - val_mse: 109.4765 - val_acc: 0.0000e+00\n",
      "Epoch 139/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 21.2899 - mse: 21.2899 - acc: 0.2875 - val_loss: 3.9422 - val_mse: 3.9422 - val_acc: 0.0000e+00\n",
      "Epoch 140/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3970 - mse: 0.3970 - acc: 0.3875 - val_loss: 0.0211 - val_mse: 0.0211 - val_acc: 1.0000\n",
      "Epoch 141/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0019 - mse: 0.0019 - acc: 0.3500 - val_loss: 4.7233e-05 - val_mse: 4.7233e-05 - val_acc: 0.0000e+00\n",
      "Epoch 142/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 8.7879e-05 - mse: 8.7879e-05 - acc: 0.3625 - val_loss: 9.9187e-04 - val_mse: 9.9187e-04 - val_acc: 1.0000\n",
      "Epoch 143/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.6049e-04 - mse: 3.6049e-04 - acc: 0.4250 - val_loss: 1.5732e-06 - val_mse: 1.5732e-06 - val_acc: 0.0000e+00\n",
      "Epoch 144/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7623e-05 - mse: 3.7623e-05 - acc: 0.4000 - val_loss: 5.0111e-05 - val_mse: 5.0111e-05 - val_acc: 1.0000\n",
      "Epoch 145/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 6.6878e-06 - mse: 6.6878e-06 - acc: 0.5000 - val_loss: 1.8261e-06 - val_mse: 1.8261e-06 - val_acc: 0.0500\n",
      "Epoch 146/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.5042e-06 - mse: 1.5042e-06 - acc: 0.2375 - val_loss: 1.1885e-06 - val_mse: 1.1885e-06 - val_acc: 0.0000e+00\n",
      "Epoch 147/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 3.0392e-07 - mse: 3.0392e-07 - acc: 0.2750 - val_loss: 8.2266e-08 - val_mse: 8.2266e-08 - val_acc: 0.3500\n",
      "Epoch 148/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.6407e-08 - mse: 5.6407e-08 - acc: 0.3500 - val_loss: 3.5696e-08 - val_mse: 3.5696e-08 - val_acc: 0.9000\n",
      "Epoch 149/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9955e-08 - mse: 2.9955e-08 - acc: 0.4125 - val_loss: 9.7207e-09 - val_mse: 9.7207e-09 - val_acc: 0.6000\n",
      "Epoch 150/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 6.9620e-08 - mse: 6.9620e-08 - acc: 0.3750 - val_loss: 4.4953e-08 - val_mse: 4.4953e-08 - val_acc: 0.4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2ca79407f08>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델구성\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=6)) \n",
    "model.add(Dense(48)) \n",
    "model.add(Dense(100)) \n",
    "model.add(Dense(500))\n",
    "model.add(Dense(100)) \n",
    "model.add(Dense(30))\n",
    "model.add(Dense(6)) \n",
    "# 훈련\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['mse','acc'])\n",
    "model.fit(x,y, epochs=150, batch_size=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "nodes = [30,48,100,500,100,30,6]\n",
    "plt.plot(nodes,list(range(len(nodes))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n",
      "y_predict 값은\n",
      " [[711. 711.  -0. 501. 711.   0.]\n",
      " [712. 712.   1. 502. 712.   1.]\n",
      " [713. 713.   2. 503. 713.   2.]\n",
      " [714. 714.   3. 504. 714.   3.]\n",
      " [715. 715.   4. 505. 715.   4.]\n",
      " [716. 716.   5. 506. 716.   5.]\n",
      " [717. 717.   6. 507. 717.   6.]\n",
      " [718. 718.   7. 508. 718.   7.]\n",
      " [719. 719.   8. 509. 719.   8.]\n",
      " [720. 720.   9. 510. 720.   9.]\n",
      " [721. 721.  10. 511. 721.  10.]\n",
      " [722. 722.  11. 512. 722.  11.]\n",
      " [723. 723.  12. 513. 723.  12.]\n",
      " [724. 724.  13. 514. 724.  13.]\n",
      " [725. 725.  14. 515. 725.  14.]\n",
      " [726. 726.  15. 516. 726.  15.]\n",
      " [727. 727.  16. 517. 727.  16.]\n",
      " [728. 728.  17. 518. 728.  17.]\n",
      " [729. 729.  18. 519. 729.  18.]\n",
      " [730. 730.  19. 520. 730.  19.]\n",
      " [731. 731.  20. 521. 731.  20.]\n",
      " [732. 732.  21. 522. 732.  21.]\n",
      " [733. 733.  22. 523. 733.  22.]\n",
      " [734. 734.  23. 524. 734.  23.]\n",
      " [735. 735.  24. 525. 735.  24.]\n",
      " [736. 736.  25. 526. 736.  25.]\n",
      " [737. 737.  26. 527. 737.  26.]\n",
      " [738. 738.  27. 528. 738.  27.]\n",
      " [739. 739.  28. 529. 739.  28.]\n",
      " [740. 740.  29. 530. 740.  29.]\n",
      " [741. 741.  30. 531. 741.  30.]\n",
      " [742. 742.  31. 532. 742.  31.]\n",
      " [743. 743.  32. 533. 743.  32.]\n",
      " [744. 744.  33. 534. 744.  33.]\n",
      " [745. 745.  34. 535. 745.  34.]\n",
      " [746. 746.  35. 536. 746.  35.]\n",
      " [747. 747.  36. 537. 747.  36.]\n",
      " [748. 748.  37. 538. 748.  37.]\n",
      " [749. 749.  38. 539. 749.  38.]\n",
      " [750. 750.  39. 540. 750.  39.]\n",
      " [751. 751.  40. 541. 751.  40.]\n",
      " [752. 752.  41. 542. 752.  41.]\n",
      " [753. 753.  42. 543. 753.  42.]\n",
      " [754. 754.  43. 544. 754.  43.]\n",
      " [755. 755.  44. 545. 755.  44.]\n",
      " [756. 756.  45. 546. 756.  45.]\n",
      " [757. 757.  46. 547. 757.  46.]\n",
      " [758. 758.  47. 548. 758.  47.]\n",
      " [759. 759.  48. 549. 759.  48.]\n",
      " [760. 760.  49. 550. 760.  49.]\n",
      " [761. 761.  50. 551. 761.  50.]\n",
      " [762. 762.  51. 552. 762.  51.]\n",
      " [763. 763.  52. 553. 763.  52.]\n",
      " [764. 764.  53. 554. 764.  53.]\n",
      " [765. 765.  54. 555. 765.  54.]\n",
      " [766. 766.  55. 556. 766.  55.]\n",
      " [767. 767.  56. 557. 767.  56.]\n",
      " [768. 768.  57. 558. 768.  57.]\n",
      " [769. 769.  58. 559. 769.  58.]\n",
      " [770. 770.  59. 560. 770.  59.]\n",
      " [771. 771.  60. 561. 771.  60.]\n",
      " [772. 772.  61. 562. 772.  61.]\n",
      " [773. 773.  62. 563. 773.  62.]\n",
      " [774. 774.  63. 564. 774.  63.]\n",
      " [775. 775.  64. 565. 775.  64.]\n",
      " [776. 776.  65. 566. 776.  65.]\n",
      " [777. 777.  66. 567. 777.  66.]\n",
      " [778. 778.  67. 568. 778.  67.]\n",
      " [779. 779.  68. 569. 779.  68.]\n",
      " [780. 780.  69. 570. 780.  69.]\n",
      " [781. 781.  70. 571. 781.  70.]\n",
      " [782. 782.  71. 572. 782.  71.]\n",
      " [783. 783.  72. 573. 783.  72.]\n",
      " [784. 784.  73. 574. 784.  73.]\n",
      " [785. 785.  74. 575. 785.  74.]\n",
      " [786. 786.  75. 576. 786.  75.]\n",
      " [787. 787.  76. 577. 787.  76.]\n",
      " [788. 788.  77. 578. 788.  77.]\n",
      " [789. 789.  78. 579. 789.  78.]\n",
      " [790. 790.  79. 580. 790.  79.]\n",
      " [791. 791.  80. 581. 791.  80.]\n",
      " [792. 792.  81. 582. 792.  81.]\n",
      " [793. 793.  82. 583. 793.  82.]\n",
      " [794. 794.  83. 584. 794.  83.]\n",
      " [795. 795.  84. 585. 795.  84.]\n",
      " [796. 796.  85. 586. 796.  85.]\n",
      " [797. 797.  86. 587. 797.  86.]\n",
      " [798. 798.  87. 588. 798.  87.]\n",
      " [799. 799.  88. 589. 799.  88.]\n",
      " [800. 800.  89. 590. 800.  89.]\n",
      " [801. 801.  90. 591. 801.  90.]\n",
      " [802. 802.  91. 592. 802.  91.]\n",
      " [803. 803.  92. 593. 803.  92.]\n",
      " [804. 804.  93. 594. 804.  93.]\n",
      " [805. 805.  94. 595. 805.  94.]\n",
      " [806. 806.  95. 596. 806.  95.]\n",
      " [807. 807.  96. 597. 807.  96.]\n",
      " [808. 808.  97. 598. 808.  97.]\n",
      " [809. 809.  98. 599. 809.  98.]\n",
      " [810. 810.  99. 600. 810.  99.]]\n",
      "loss : 3.631234926082527e-08\n",
      "mse : 3.631235045986614e-08\n",
      "acc : 0.1599999964237213\n"
     ]
    }
   ],
   "source": [
    "loss, mse, acc = model.evaluate(x, y, batch_size=1)\n",
    "y_predict = model.predict(x)\n",
    "print('y_predict 값은\\n', np.round(y_predict))\n",
    "print(\"loss :\", loss)\n",
    "print(\"mse :\", mse)\n",
    "print(\"acc :\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc : 1.0\n"
     ]
    }
   ],
   "source": [
    "# accuracy_score 1차원만 지원하는거 실화냐....\n",
    "print(\"acc :\",accuracy_score(y.reshape(-1), np.round(y_predict).reshape(-1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 43.6325 - mse: 43.6325 - acc: 0.4125 - val_loss: 4.6574 - val_mse: 4.6574 - val_acc: 0.0000e+00\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.7133 - mse: 1.7133 - acc: 0.3125 - val_loss: 0.0435 - val_mse: 0.0435 - val_acc: 1.0000\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0124 - mse: 0.0124 - acc: 0.3375 - val_loss: 1.8179e-04 - val_mse: 1.8179e-04 - val_acc: 1.0000\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.1357e-04 - mse: 1.1357e-04 - acc: 0.4500 - val_loss: 5.4474e-06 - val_mse: 5.4474e-06 - val_acc: 0.0000e+00\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.5233e-07 - mse: 2.5233e-07 - acc: 0.3875 - val_loss: 2.2404e-08 - val_mse: 2.2404e-08 - val_acc: 0.0500\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.2184e-08 - mse: 2.2184e-08 - acc: 0.4875 - val_loss: 7.5825e-09 - val_mse: 7.5825e-09 - val_acc: 0.1500\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 6.7474e-08 - mse: 6.7474e-08 - acc: 0.3625 - val_loss: 2.0123e-08 - val_mse: 2.0123e-08 - val_acc: 0.3500\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.0112e-08 - mse: 2.0112e-08 - acc: 0.3875 - val_loss: 4.3361e-08 - val_mse: 4.3361e-08 - val_acc: 0.1000\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.8763e-07 - mse: 1.8763e-07 - acc: 0.3375 - val_loss: 1.8074e-08 - val_mse: 1.8074e-08 - val_acc: 0.5500\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.8851e-07 - mse: 1.8851e-07 - acc: 0.3375 - val_loss: 4.2817e-07 - val_mse: 4.2817e-07 - val_acc: 0.7000\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.5322e-07 - mse: 6.5322e-07 - acc: 0.3250 - val_loss: 6.3460e-08 - val_mse: 6.3460e-08 - val_acc: 0.9000\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.2853e-06 - mse: 2.2853e-06 - acc: 0.4000 - val_loss: 3.0313e-06 - val_mse: 3.0313e-06 - val_acc: 0.6000\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.5958e-05 - mse: 1.5958e-05 - acc: 0.4125 - val_loss: 1.4783e-06 - val_mse: 1.4783e-06 - val_acc: 0.3000\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2444e-06 - mse: 5.2444e-06 - acc: 0.3500 - val_loss: 6.3224e-07 - val_mse: 6.3224e-07 - val_acc: 0.0000e+00\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.1326e-06 - mse: 1.1326e-06 - acc: 0.2875 - val_loss: 6.9655e-07 - val_mse: 6.9655e-07 - val_acc: 0.7000\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.0024e-06 - mse: 1.0024e-06 - acc: 0.3375 - val_loss: 1.7311e-05 - val_mse: 1.7311e-05 - val_acc: 1.0000\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.2759e-06 - mse: 8.2759e-06 - acc: 0.3000 - val_loss: 2.8306e-05 - val_mse: 2.8306e-05 - val_acc: 0.0000e+00\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0012 - mse: 0.0012 - acc: 0.3875 - val_loss: 0.0534 - val_mse: 0.0534 - val_acc: 1.0000\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 124.2948 - mse: 124.2948 - acc: 0.4875 - val_loss: 2310.4756 - val_mse: 2310.4758 - val_acc: 0.0000e+00\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1105.5804 - mse: 1105.5803 - acc: 0.2750 - val_loss: 98.2463 - val_mse: 98.2463 - val_acc: 0.0000e+00\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.2052 - mse: 5.2052 - acc: 0.2875 - val_loss: 0.7498 - val_mse: 0.7498 - val_acc: 0.0000e+00\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.2212 - mse: 0.2212 - acc: 0.3500 - val_loss: 0.3344 - val_mse: 0.3344 - val_acc: 0.0000e+00\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0839 - mse: 0.0839 - acc: 0.1125 - val_loss: 0.0335 - val_mse: 0.0335 - val_acc: 1.0000\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0176 - mse: 0.0176 - acc: 0.1750 - val_loss: 0.0041 - val_mse: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.7985e-04 - mse: 2.7985e-04 - acc: 0.3625 - val_loss: 1.0025e-05 - val_mse: 1.0025e-05 - val_acc: 0.0000e+00\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 5.3490e-06 - mse: 5.3490e-06 - acc: 0.3625 - val_loss: 3.2059e-07 - val_mse: 3.2059e-07 - val_acc: 0.0500\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.7147e-08 - mse: 2.7147e-08 - acc: 0.5000 - val_loss: 8.9766e-09 - val_mse: 8.9766e-09 - val_acc: 0.3000\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 6.9693e-08 - mse: 6.9693e-08 - acc: 0.4000 - val_loss: 1.5835e-07 - val_mse: 1.5835e-07 - val_acc: 0.0000e+00\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9340e-08 - mse: 2.9340e-08 - acc: 0.3875 - val_loss: 1.6911e-07 - val_mse: 1.6911e-07 - val_acc: 0.3000\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.9360e-07 - mse: 1.9360e-07 - acc: 0.4625 - val_loss: 7.5397e-08 - val_mse: 7.5397e-08 - val_acc: 1.0000\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 4.0350e-08 - mse: 4.0350e-08 - acc: 0.4500 - val_loss: 9.8556e-08 - val_mse: 9.8556e-08 - val_acc: 0.0500\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.7367e-08 - mse: 2.7367e-08 - acc: 0.4000 - val_loss: 2.9028e-08 - val_mse: 2.9028e-08 - val_acc: 0.0000e+00\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.0470e-07 - mse: 2.0470e-07 - acc: 0.3375 - val_loss: 4.4692e-07 - val_mse: 4.4692e-07 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2ca76e12b08>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping \n",
    "early_stopping = EarlyStopping(monitor='loss', patience=25, mode='auto')\n",
    "model.fit(x,y, epochs=150, batch_size=1,validation_split = 0.2,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n",
      "loss : 7.574531807108542e-07\n",
      "mse : 7.574532219223329e-07\n",
      "model acc : 0.0\n",
      "acc : 1.0\n"
     ]
    }
   ],
   "source": [
    "loss, mse, acc = model.evaluate(x, y, batch_size=1)\n",
    "y_predict = model.predict(x)\n",
    "# print('y_predict 값은\\n', np.round(y_predict))\n",
    "print(\"loss :\", loss)\n",
    "print(\"mse :\", mse)\n",
    "print(\"model acc :\", acc)\n",
    "print(\"acc :\",accuracy_score(y.reshape(-1), np.round(y_predict).reshape(-1)) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit에 callbacks=[early_stopping] 적용 전,\n",
    "loss : 3.631234926082527e-08\n",
    "mse : 3.631235045986614e-08\n",
    "model acc : 0.1599999964237213\n",
    "acc : 1.0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ㅇㅁㅇ...! early_stopping 적용해서 더 안좋을 수도 있다.. 그러니 early_stopping도 튜닝을 별도로 진행해줘야 한다 ㅠㅠ!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 합쳐서 해도 1.0인데 무엇을 위해.. 앙상블을 하나....\n",
    "# 각 열별 w는 어떻게 뜯어볼 수 있는거지 ㅇㅅㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 (2열 -> 2열)\n",
    "x1 = np.array([range(1,101), range(311,411), range(100)]).T \n",
    "y1 = np.array([range(711,811), range(711,811), range(100)]).T\n",
    "                # + 700         + 400           + 0\n",
    "x2 = np.array([range(101,201), range(411,511), range(100,200)]).T \n",
    "y2 = np.array([range(501,601), range(711,811), range(100)]).T\n",
    "                # + 400         + 300           -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 함수형 모델 1\n",
    "input1 = Input(shape=(3, ), name='input1')\n",
    "dense1 = Dense(5, activation='relu', name='aaa')(input1) # input 레이어를 알려줘야 함/ 노드를 5개로 가지고 input1을 인풋레이어로 받는 댄스층\n",
    "dense1 = Dense(30, activation='relu', name='bbb')(dense1) \n",
    "dense1 = Dense(150, activation='relu', name='ccc')(dense1)\n",
    "\n",
    "# 함수형 모델 2\n",
    "input2 = Input(shape=(3, ), name='input2')\n",
    "dense2 = Dense(5, activation='relu', name='aaa1')(input2) # input 레이어를 알려줘야 함/ 노드를 5개로 가지고 input1을 인풋레이어로 받는 댄스층\n",
    "dense2 = Dense(30, activation='relu', name='bbb1')(dense2)\n",
    "dense2 = Dense(150, activation='relu', name='ccc1')(dense2)\n",
    "\n",
    "# 엮어주쟈~~~ ***************************************************\n",
    "from keras.layers.merge import concatenate\n",
    "merge = concatenate([dense1,dense2], name='111') #output 자체를 묶어준다\n",
    "middle = Dense(30, name='222')(merge)\n",
    "middle = Dense(8, name='333')(middle)\n",
    "################ output 모델 구성 #################\n",
    "\n",
    "# 함수형 모델 1\n",
    "output1 = Dense(3, name='a')(middle)\n",
    "\n",
    "# 함수형 모델 2\n",
    "output2 = Dense(3, name='c1')(middle)\n",
    "\n",
    "model = Model(inputs=[input1, input2],\n",
    "                outputs=[output1,output2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "다이아몬드 모형이 아니라 행성 모형이라고 해야 될 판이다..\n",
    "+ 노드 양상을 시각화 해서 감을 잡는게 좋을 듯 (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhc1Znn8e9bVZJKpV3Wai22sY3BhrAJh71Jhz2EkGUmZM8kPU5nEgLdnc5GPz2dTmcmmUkzgUlC2iFrh6HTgUCIw2JCQ9gCxAaMjTcMGEteZS22bO3SmT+qJNcmVIpVqltVv8/z6LGqbkl6uVb9fDj3nPeacw4REfEuX6YLEBGRN6egFhHxOAW1iIjHKahFRDxOQS0i4nGBdHzTmpoat3DhwnR8axGRnLR+/fqDzrnaZMfSEtQLFy5k3bp16fjWIiI5yczemOqYpj5ERDxOQS0i4nEKahERj1NQi4h4nIJaRMTjUgpqM6s0s7vMbKuZbTGzc9NdmIiIhKW6PO8W4EHn3PvMrBAIpbEmERGJMu2I2szKgYuAHwI454adc72zXcjgyBirH3+Vp3YcnO1vLSKS1VKZ+jgB6AR+bGYvmNntZlYS/yIzW2Vm68xsXWdn54wLKfD7WP34a/y/53bN+GtFRHJZKkEdAM4EbnPOnQEcBb4U/yLn3GrnXJtzrq22NukuyDfl9xmXLq/nsa0HGBodm/HXi4jkqlSCugPocM49G3l8F+HgnnWXLW/g6PAYT+/oSse3FxHJStMGtXNuH9BuZssiT70d2JyOYs5bMo+SQj9rN+9Lx7cXEclKqa6jvh64w8xeAk4H/kc6iikK+Ln4pDoe3ryfsXHdy1FEBFIMaufci5H557c45651zvWkq6DLVzRw8MgwL+xK248QEckqntuZePGyWgr8xtrN+zNdioiIJ3guqMuDBZy7uIaHXt6Hc5r+EBHxXFADXL6inje6+tm+/0imSxERyThPBvWlJ9djBg+9rNUfIiKeDOq68iBntFRqmZ6ICB4NaoDLVjSwafdhOnr6M12KiEhGeTaoL1/RAMDDWv0hInnOs0G9qKaEpXWlrH1ZQS0i+c2zQQ3hUfVzO7vpOTqc6VJERDLG00F92Yp6xsYdj2w9kOlSREQyxtNBfWpTBY0VQS3TE5G85umgNjMuW17PE690MjCsHtUikp88HdQQXqY3ODLO77fP/K4xIiK5wPNBvXJRNRXFBdr8IiJ5y/NBXeD38faT6nhkywFGx8YzXY6IyJzzfFBDePrj0MAIz73enelSRETmXFYE9UUn1lAU8KlHtYjkpawI6lBhgItOrGWtelSLSB7KiqAGuGx5PXsODbJp9+FMlyIiMqeyJqgvObken3pUi0geypqgriopZOWiai3TE5G8kzVBDXDZ8ga27z/C6wePZroUEZE5k11BvaIegLWa/hCRPJJVQd1cFWLF/HIt0xORvJJSUJvZTjPbaGYvmtm6dBf1Zi5f0cDzu3o40DeYyTJERObMTEbUb3POne6ca0tbNSm4bEU9zsHvNqtHtYjkh6ya+gBYVl/Ggnkh7tuwm7FxbX4RkdyXalA7YK2ZrTezVcleYGarzGydma3r7ExfS1Iz4wMrW3nmtW4+dPsz7D00kLafJSLiBakG9fnOuTOBK4HPmNlF8S9wzq12zrU559pqa2tntch4n7roBL71n07jpY5DXHnLE9oEIyI5LaWgds7tifx5ALgHWJnOoqZjZrzvrGbWXH8BLVUhPvWv67npno0MjuguMCKSe6YNajMrMbOyic+By4BN6S4sFSfUlnL3p89j1UUncMezu7jmO0+ydZ96gYhIbkllRF0PPGlmG4DngN865x5Mb1mpKwz4+MpVJ/OzT6yk++gI13znKX72h53qsiciOcPSEWhtbW1u3bq5X2598MgQf/vLDTy6rZNLTq7nf73vLVSXFM55HSIiM2Vm66da/px1y/PeTE1pET/6+Nn8/dXLeXx7J1fe8jhP7ziY6bJERI5LTgU1hC80fuKCRdzzmfMoKQrwoR8+yzcf3MqI7rcoIlkq54J6wor5Fay5/gLe39bCbY+9yvu+/wfe6FLXPRHJPjkb1BC+hdc33vsWvvehM3m98wjvuPVJ7n1hd6bLEhGZkZwO6glXndrIAzdexMmNZdz4ixf561+8yJGh0UyXJSKSkrwIaoCmymLu/K/ncOMlS7n3xd2849Yn2NDem+myRESmlTdBDRDw+7jxkhP5xafOZXTM8d7bnua2x15lXM2dRMTD8iqoJ5y9sJr7P3chl62o55sPbuUjP3qW/YfV31pEvCkvgxqgIlTAdz94Jt94z6k8/0YvV97yBI9s0Z1jRMR78jaoIbzm+rqVrfzm+gtoKA/yyZ+u4x/ue1nNnUTEU/I6qCcsqSvlns+cxyfOX8RPnt7Jtd99ilf292W6LBERQEE9qSjg5+/fuZwff/xsOvuGeOd3nuSOZ99QcycRyTgFdZy3nVTHAzdeyNkLq7npnk18+ufP09s/nOmyRCSPKaiTqCsL8tP/spKvXHUSj2zdz5W3PMEzr3VluiwRyVMK6in4fMaqixZz96fPoyjg44M/eIab125jVM2dRGSOKain8ZbmStZ87kLefUYzt/7HDt6/+hnau/szXZaI5BEFdQpKiwL8838+jVuuO53t+/q46tYn+M2GPZkuS0TyhIJ6Bt51ehP333AhS+pKuf7OF/jbX27gqJo7iUiaKahnqKU6xL9/6lw++7Yl3PV8B+/8v0+yafehTJclIjlMQf0nKPD7+Pzly7jjL95K//AY7/7eU9z+xGtq7iQiaaGgPg7nLa7hgRsu5OJldfzTb7fw8Z/8kc6+oUyXJSI5RkF9nKpKCln9kbP42rWn8OxrXVx5y+M8tu1ApssSkRyioJ4FZsZHzlnAfZ+9gHklRXz8x3/ka2s2MzSq5k4icvwU1LNoWUMZv/7s+Xz03AX88MnXec/3nubVziOZLktEslzKQW1mfjN7wczWpLOgbBcs8POP7zqFH3y0jT29A1x965P84o+71NxJRP5kMxlR3wBsSVchuebS5fU8cMNFnN5SyRfv3shn73yBQwMjmS5LRLJQIJUXmVkz8A7g68Bfp7WiHNJQEeTnf/FWvv/7V7n54e28uKuXa8+YT2t1iJbqEK3VIRorivH7LNOlioiHpRTUwLeBLwBlU73AzFYBqwBaW1uPv7Ic4fcZn3nbEs5bPI+b7tnE93//GmNR660DPqO5qngyuCc+WiIfFcUFGaxeRLzApps7NbOrgaucc//NzC4GPu+cu/rNvqatrc2tW7du9qrMIaNj4+w9NMiu7v6Yj/bIn739sdMjFcUFMeEdHeaNlUEK/LoeLJILzGy9c64t2bFURtTnA9eY2VVAECg3s5875z48m0Xmi4DfNzlaPj/J8UMDI7RHBfeu7n7aewbYvPcwazfvY2Ts2D+sfp8xvzI4ZZBXFBdgpmkVkWw37Yg65sUaUWfU2Lhj3+FBdnXFBvnEiLzraOydaMqCgYSplInHTZXFFAY0GhfxiuMdUYtH+H1GU2UxTZXFnLt4XsLxI0OjkwEeHeTb9vfxyJYDDEfd9MBn0FhRTEt1cdIReXVJoUbjIh4xo6B2zj0GPJaWSuS4lRYFOLmxnJMbyxOOjY879vcN0t49kDAv/ui2zoQeJSWF/tiplHmRUXlViOaqYoIF/rn6zxLJexpR5wmfz2isKKaxopiVi6oTjvcPj9LRM8CurtjplNcPHuX32zsZGo29BVlDeTB2FD6vePJxbWmRRuMis0hBLQCECgOcWF/GifWJKzCdc3T2DUUubPazq2tgMsif2nGQuw8Pxrw+WOCb8gJnc1WI4kKNxkVmQkEt0zIz6sqD1JUHaVuYOBofHBmjo2cg6QXOp1/ton84tjlVXVlR0gucrdUh6sqK8GkDkEgMBbUct2CBnyV1pSypK0045pyj6+jwsTnxrsiovLuf517v5t4XdxO98Kgw4KOlKskFznnh+fGSIv3KSv7Rb72klZlRU1pETWkRZ7ZWJRwfGh1jT+9g7AXOyDz5H3f2cCTunpQ1pYUxo/CJC5yt80I0lAe1HV9ykoJaMqoo4GdRTQmLakoSjjnn6O0fSbqDc/0bPax5aW/MdvwCv9FcNTEKTxyVlwW1HV+yk4JaPMvMqCoppKqkkNNaKhOOj4yNs7d3MGmQb2jvTehWWBUqSHqBs6U6RGNFkIC244tHKaglaxX4fbTOC097JHOof2RyPjw6yDfuPsSDm/YxGtccqykyN95cFRvkrdUhKkIajUvmKKglZ1WECqgIVXBKU0XCsdGx8fB2/JhdnOFlhw+9vI/uuO345cFA+B+FJCPy+ZXFao4laaWglrwU8PtorgqPnlmceLxvcGRyF2f0ssOte/v43ebE7fjzK5NvxW+tDlEZUnMsOT4KapEkyoIFLJ9fwPL5idvxx8Yd+w/HrlSZCPPfbTnAwSOx2/FLiwJTXuBsqiqmKKANQPLmFNQiMxRuL1vM/MpizjkhsTnW0aHIdvy4C5yvdh7lsW2x2/HNoLE8mLSvSmt1iHlqjiUoqEVmXUlRgGUNZSxrSNyOPz7u6DwS3o4/sV68PbI1//FXOtl/OHY0Hir0x64Xry6enCtvrgqpOVaeUFCLzCGfz6gvD1JfHuTsKbfj90cF+cDk50++cpCBkdjt+PXlRUnnxVurQ9SWqTlWrlBQi3hIeDt+GUvqkjfHOnhkOOEC567ufv7wahf3vBC7HT9Y4IuMwkOJUytqjpVVFNQiWcLMqC0rorasiLMWJG7HHxwZY3fvQMKt3HZ1D/DMa10cjWuOVVNalPQCZ+u8EPVlQTXH8hAFtUiOCBb4WVxbyuLa5M2xeqK248f3VLlvwx7Go5tj+X00R4V4dJC3VIcoVXOsOaWzLZIHzIzqkkKqSwo5Pcl2/OHRcfb0DhzrOR7XV6VvMLY51rySwqg2tcUxQd5YUazmWLNMQS0iFAZ8LKwpYWGS5lgQ3o6/qztxO/6G9l7u35jYHKupsjhpT5XWeSHK1RxrxhTUIjKtilABp4YqOLU5+Xb8vYcGkwb5/Rv30tMf2xyrcqrmWFUhGiuD2o6fhIJaRI5LwO+bnAY5P8nxw4MjSS9wbt5zmLUv72Nk7NhoPLyZKDjldvyK4vzcjq+gFpG0Kg8WsGJ+BSvmJ47Gx8ZduDlWV+KSw7Uv76crrjlWWTCQ9ALnRHOswkBujsYV1CKSMX5feD67qbKYcxcn344fvqFy7AXO7fv7eGTrAYZHY5tjNVZMXNgsTgjy6izejq+gFhHPKikKcFJDOSc1JDbHGh93HOgbSnrjiEe3ddLZF7sdv6TQP2VPlabKYk9vx582qM0sCDwOFEVef5dz7r+nuzARkTfj8xkNFUEaKoKsXJS4Hb9/ONIcqyt2yeHOrqM8/kongyOxzbEa4ptjRY3Ia0ozOxpPZUQ9BPy5c+6ImRUAT5rZA865Z9Jcm4jInyxUGODE+jJOrE++Hb/zyNCxefGuY73Hn3zlIPsOD8a8vrjAn3Q6ZeJxukfj0wa1c84BRyIPCyIfbuqvEBHxNjOjrixIXVmQsxZM1RxrIOYC58TnT7/aRX/cdvy6snBzrCtPbeSTFyya9XpTmqM2Mz+wHlgCfNc592yS16wCVgG0trbOZo0iInMqWOBnwbwQBX4j4DcK/L7Jj4Df2LavL2ZZ4YG+IUbGxmnv7k9LPeZc6oNjM6sE7gGud85tmup1bW1tbt26dbNQnohIejjn6I3bcRk9gt7TOxDT/6TAbzRXhZLerael+vh3XJrZeudcW7JjM1r14ZzrNbPHgCuAKYNaRMQLhkfH2d0bd7edqKV+fUOxPUxqSsM9TM5aUMW7z2iKmY+uLw9mrIdJKqs+aoGRSEgXA5cA30x7ZSIi03DO0XV0OCGEJx7vPTwY06O7MOCjpSo8Gj57YVVMa9eWqhAlHu0KmEpVjcBPI/PUPuDfnXNr0luWiEhYsgt70dMUU13YO+eEeTTHrdCoKyvKyj7bqaz6eAk4Yw5qEZE85JyjM2rjSnv3QEwQxy+VCxb4JoP33MXzYoK4OUfvXOPNcb6I5JSB4bGYreDRQdze0x+z+QTCm09aq0Ocv6QmMjVxbA1zbWn+3QtSQS0ix+3NtnPv6u6fcjv3opoS/uzE2qzazp0JCmoRScmRodFjo+C4+eKOnoGYBklmML+imJbqYt62rDanGiRlgoJaRIA3bzna3t2f2HK0KEDrvBDL6su49OT6mHsqNuVwy9FMUFCL5JHDgyMxQRxuVhReUdHR0z9lE//LVtSriX8GKahFcsib3RZrV3c/vVPcFmv5/HIuX9EQE8S6LZZ3KKhFssxUN5rd1d3P7t6BmBvNBnxGc1X4RrPvOLUxYdtzRbFuNJsNFNQiHjM8Os6e+G3PUR99g7HbnqtLwtueT2up5J2nNcZcuGusKM7YtmeZPQpqkTnmnKMnalQcv/V576HYZkCFfh/NkSZAZy2oigniluoQpR7d9iyzR3/DImkwNBre9hwdxNEX7o4kNAMqorW6mLMXVtFa3RTTg6K+LJiV255l9iioRf4EzjkOHhlOOjXRHtn2HN0MqChwbNvzWxdVx42KiwkV6q0oU9Nvh8gUws2AJm7VFB4NRwfzwEhsM6D68qKk/SfC99zLzmZA4g0Kaslb4+Ph++btipojjh4dH4jb9hwq9E+OhMM9KIppnXesGZC2PUu6KKglp/UPj052Y4sP4vbufobitj03Ru5E/Wcn1h7rUxwZFc/TtmfJEAW1ZLXxiW3PUeF7LIwHOHgkdlRcWhSgpTrE4tqShB4UTVXFFAU0KhbvUVCL5/UNjiT0KJ74vKNngOGxY6Nin8H8yvBStktOrovZ9txSHaIqpG3Pkn0U1JJxE9ue23tiR8MTYdwd3wwoGGDBvBAnNZZx6Yr6mIt28yuLte1Zco6CWubEoYGRKW+ltLtngNG4bc9NkfvaXXFKAy1Vcc2AQtr2LPlFQS2zYmRsnL29g0mDeFd3P4cGYpsBVUWaAZ3aVJHQg6KxIkhAo2KRSQpqSYlzjt64ZkDtk20y+9nTOxjTDKjAb7RUhWiuDnFaS0VCM6DyoEbFIqlSUMuk4dFxdsc3A4paX9yXsO053AzozNYqrj099g4e9eVBNQMSmSUK6jzinKP76PAUa4oH2HNoIGbbc2HAR0tkrvjshVUx/SdaqkKUqBmQyJzQOy3HhLc9D0x54a5/OHbbc11Z0WT/ieiObK3VIerKtO1ZxAsU1FnGufC258kg7opdX7zv8GDM64MFx5oBxfegaK4KUVyoDR4iXjdtUJtZC/AzoAEYB1Y7525Jd2H5bGB4LHyRLqpH8WRzoO5+BkfGY17fWBHe9nzB0prJbmwTI+Pa0iJt8BDJcqmMqEeBv3HOPW9mZcB6M3vYObc5zbXlrPFxx4G+oSmXsnXGNQMqKfTTUh1i4bwSLlpaG9N/oqmyWM2ARHLctEHtnNsL7I183mdmW4AmQEE9AzsPHuVba7exZe9h2nsGGB6N3fbcWFFMS3VxQv+J1uoQ1WoGJJLXZjRHbWYLgTOAZ5McWwWsAmhtbZ2F0nLDyNg4P3jiNW753SsU+n1csLSGS06ujwni+ZXFFAa0wUNEkks5qM2sFLgbuNE5dzj+uHNuNbAaoK2tzcUfz0cb2nv50q82smXvYa5Y0cBX37WC+vJgpssSkSyTUlCbWQHhkL7DOfer9JaU/Y4OjfLPa7fzk6dfp7asiH/5yFlcvqIh02WJSJZKZdWHAT8Etjjnbk5/Sdnt0a0H+Lt7N7G7d4APn9PKF644SdulReS4pDKiPh/4CLDRzF6MPPcV59z96Ssr+xw8MsRXf7OZ32zYw9K6Uu76y3NpW1id6bJEJAeksurjSUBLDqbgnOOX6zv4+m+3MDA8xl9dciJ/efEJulOIiMwa7Uw8DjsPHuUr92zk6Ve7OHthFf/zPaeypK4s02WJSI5RUP8J4pfcff3dp/CBs1vVF0NE0kJBPUNacicic01BnSItuRORTFFQp+DRbQf4u3u05E5EMkNB/SYOHhniH3+zmfs27GGJltyJSIYoqJOIX3J34yVL+fTFi7XkTkQyQkEdZ+fBo9x070ae2qEldyLiDQrqiJGxcW5/4nW+/bvtWnInIp6ioAZe6ujli3dryZ2IeFNeB/XRoVFufng7P35KS+5ExLvyNqi15E5EskXeBbWW3IlItsmboHbOcdf6Dr5+/xb6h7TkTkSyR14E9Rtd4S53T+3oom1BFd94r5bciUj2yOmgjl9y90/XnsIHV2rJnYhkl5wN6vgld/9wzQoaKrTkTkSyT84FtZbciUiuyamgfmzbAW7SkjsRyTE5EdQHjwzxtTWb+fWLWnInIrknq4NaS+5EJB9kbVBryZ2I5IusC2otuRORfJNVQR295O7yFfV89ZpTtORORHLetEFtZj8CrgYOOOdOSX9JiaKX3NWUFvH9D5/FFadoyZ2I5IdURtQ/Ab4D/Cy9pSSnJXciku+mDWrn3ONmtjD9pST61kPb+M6jO7TkTkTy2qzNUZvZKmAVQGtr66x8z7vWd3Dh0hpu/1ibltyJSN7yzdY3cs6tds61Oefaamtrj/v7DY2Osb9vkLMWVCmkRSSvzVpQz7a9vYM4B81VoUyXIiKSUZ4N6o6eAQBaqoozXImISGZNG9RmdifwB2CZmXWY2SfTXxZ09PQD0FytEbWI5LdUVn18YC4KidfRM0DAZ9SXFWXix4uIeIZnpz7ae/pprAwS8Hu2RBGROeHZFOzoGaC5UtMeIiIeDup+mnUhUUTEm0E9NDrG/sNDtOhCooiIN4N6T+8ggEbUIiJ4NKjbuyNL87TZRUTEm0E9sdlFI2oREc8GdX94DXW5bgogIuLRoB5gfmUxft1eS0TEq0GtpXkiIhM8GtQDtOhCoogI4MGgHhwZ40DfkEbUIiIRngvq3b2RFR/VCmoREfBgUB9bmqepDxER8GRQT2x20YhaRAQ8GdQDFPiNujKtoRYRAY8GdZPWUIuITPJcULd392t+WkQkiueCuqNnQPPTIiJRPBXUgyNjHDyiNdQiItE8FdRamicikshjQa2leSIi8TwV1O0aUYuIJPBUUHf09FPo91FXVpTpUkREPCOloDazK8xsm5ntMLMvpauYjp4BmqqK8WkNtYjIpGmD2sz8wHeBK4HlwAfMbHk6itHSPBGRRKmMqFcCO5xzrznnhoF/A96VjmJ264YBIiIJUgnqJqA96nFH5LkYZrbKzNaZ2brOzs4ZFzI+7rhoaS0rF1XP+GtFRHJZIIXXJJswdglPOLcaWA3Q1taWcHw6Pp9x8/tPn+mXiYjkvFRG1B1AS9TjZmBPesoREZF4qQT1H4GlZrbIzAqB64D70luWiIhMmHbqwzk3amafBR4C/MCPnHMvp70yEREBUpujxjl3P3B/mmsREZEkPLUzUUREEimoRUQ8TkEtIuJxCmoREY8z52a8N2X6b2rWCbwxzctqgIOz/sNnl2qcPdlQZzbUCNlRp2qcuQXOudpkB9IS1Kkws3XOubaM/PAUqcbZkw11ZkONkB11qsbZpakPERGPU1CLiHhcJoN6dQZ/dqpU4+zJhjqzoUbIjjpV4yzK2By1iIikRlMfIiIep6AWEfG4OQ/qubpR7kyYWYuZPWpmW8zsZTO7IfJ8tZk9bGavRP6synStEL6PpZm9YGZrIo8XmdmzkTp/EWlHm8n6Ks3sLjPbGjmn53rxXJrZX0X+vjeZ2Z1mFsz0uTSzH5nZATPbFPVc0nNnYbdG3ksvmdmZGa7zf0f+zl8ys3vMrDLq2JcjdW4zs8szVWPUsc+bmTOzmsjjjJ3LVMxpUM/ljXJnaBT4G+fcycA5wGcidX0JeMQ5txR4JPLYC24AtkQ9/ibwfyJ19gCfzEhVx9wCPOicOwk4jXCtnjqXZtYEfA5oc86dQriF73Vk/lz+BLgi7rmpzt2VwNLIxyrgtjmqEZLX+TBwinPuLcB24MsAkffSdcCKyNd8L5IFmagRM2sBLgV2RT2dyXM5PefcnH0A5wIPRT3+MvDluawhxTp/TfgvchvQGHmuEdjmgdqaCb9Z/xxYQ/hWaQeBQLJznIH6yoHXiVyojnreU+eSY/cCrSbc7ncNcLkXziWwENg03bkD/gX4QLLXZaLOuGPvBu6IfB7zPifc2/7cTNUI3EV4ALETqPHCuZzuY66nPlK6UW4mmdlC4AzgWaDeObcXIPJnXeYqm/Rt4AvAeOTxPKDXOTcaeZzpc3oC0An8ODI9c7uZleCxc+mc2w18i/Coai9wCFiPt87lhKnOnZffT58AHoh87pk6zewaYLdzbkPcIc/UmMxcB3VKN8rNFDMrBe4GbnTOHc50PfHM7GrggHNuffTTSV6ayXMaAM4EbnPOnQEcxTtTRpMi87zvAhYB84ESwv/7G88zv59JeO3vHgAzu4nwdOIdE08ledmc12lmIeAm4O+THU7yXMbP5YS5DmrP3ijXzAoIh/QdzrlfRZ7eb2aNkeONwIFM1RdxPnCNme0E/o3w9Me3gUozm7hbT6bPaQfQ4Zx7NvL4LsLB7bVzeQnwunOu0zk3AvwKOA9vncsJU507z72fzOxjwNXAh1xkDgHv1LmY8D/MGyLvoWbgeTNrwDs1JjXXQe3JG+WamQE/BLY4526OOnQf8LHI5x8jPHedMc65Lzvnmp1zCwmfu/9wzn0IeBR4X+RlGa3TObcPaDezZZGn3g5sxmPnkvCUxzlmFor8/U/U6ZlzGWWqc3cf8NHIioVzgEMTUySZYGZXAF8ErnHO9Ucdug+4zsyKzGwR4Qt2z811fc65jc65Oufcwsh7qAM4M/I766lzmWCuJ8WBqwhfEX4VuCnTk/SRmi4g/L85LwEvRj6uIjz/+wjwSuTP6kzXGlXzxcCayOcnEP7F3wH8EijKcG2nA+si5/NeoMqL5xL4KrAV2AT8K1CU6XMJ3El4znyEcJB8cqpzR/h/178beS9tJLyCJZN17iA8zzvxHvp+1OtvitS5DbgyUzXGHd/JsYuJGTuXqXxoC7mIiMdpZ6KIiMcpqEVEPE5BLSLicQpqERGPU1CLiHicgugYZosAAAAQSURBVFpExOMU1CIiHvf/ARAYldeuv0+EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "nodes = [3,5,30,150,30,8,3]\n",
    "plt.plot(nodes,list(range(len(nodes))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input1 (InputLayer)             (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input2 (InputLayer)             (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "aaa (Dense)                     (None, 5)            20          input1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "aaa1 (Dense)                    (None, 5)            20          input2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bbb (Dense)                     (None, 30)           180         aaa[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "bbb1 (Dense)                    (None, 30)           180         aaa1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "ccc (Dense)                     (None, 150)          4650        bbb[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "ccc1 (Dense)                    (None, 150)          4650        bbb1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "111 (Concatenate)               (None, 300)          0           ccc[0][0]                        \n",
      "                                                                 ccc1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "222 (Dense)                     (None, 30)           9030        111[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "333 (Dense)                     (None, 8)            248         222[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "a (Dense)                       (None, 3)            27          333[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "c1 (Dense)                      (None, 3)            27          333[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 19,032\n",
      "Trainable params: 19,032\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/150\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 104573.8996 - a_loss: 69294.8906 - c1_loss: 35279.0508 - a_mse: 69294.8906 - c1_mse: 35279.0508 - val_loss: 2249.4094 - val_a_loss: 874.8181 - val_c1_loss: 1374.5914 - val_a_mse: 874.8181 - val_c1_mse: 1374.5914\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 477.0048 - a_loss: 216.8335 - c1_loss: 260.1712 - a_mse: 216.8335 - c1_mse: 260.1712 - val_loss: 1895.6222 - val_a_loss: 845.8070 - val_c1_loss: 1049.8152 - val_a_mse: 845.8070 - val_c1_mse: 1049.8152\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 392.2995 - a_loss: 181.9997 - c1_loss: 210.2999 - a_mse: 181.9997 - c1_mse: 210.2999 - val_loss: 1053.9943 - val_a_loss: 444.4056 - val_c1_loss: 609.5887 - val_a_mse: 444.4056 - val_c1_mse: 609.5887\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 277.4398 - a_loss: 120.0334 - c1_loss: 157.4063 - a_mse: 120.0334 - c1_mse: 157.4063 - val_loss: 857.6243 - val_a_loss: 355.2856 - val_c1_loss: 502.3387 - val_a_mse: 355.2856 - val_c1_mse: 502.3387\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 228.0689 - a_loss: 96.0954 - c1_loss: 131.9735 - a_mse: 96.0954 - c1_mse: 131.9735 - val_loss: 457.4572 - val_a_loss: 146.8034 - val_c1_loss: 310.6538 - val_a_mse: 146.8034 - val_c1_mse: 310.6538\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 163.7705 - a_loss: 67.2774 - c1_loss: 96.4931 - a_mse: 67.2774 - c1_mse: 96.4931 - val_loss: 819.9966 - val_a_loss: 251.3418 - val_c1_loss: 568.6548 - val_a_mse: 251.3418 - val_c1_mse: 568.6548\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 111.8503 - a_loss: 44.5392 - c1_loss: 67.3111 - a_mse: 44.5392 - c1_mse: 67.3111 - val_loss: 541.1735 - val_a_loss: 175.2375 - val_c1_loss: 365.9361 - val_a_mse: 175.2375 - val_c1_mse: 365.9361\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 71.0243 - a_loss: 23.9588 - c1_loss: 47.0655 - a_mse: 23.9588 - c1_mse: 47.0655 - val_loss: 489.1413 - val_a_loss: 118.4218 - val_c1_loss: 370.7195 - val_a_mse: 118.4218 - val_c1_mse: 370.7195\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 56.9687 - a_loss: 19.3765 - c1_loss: 37.5921 - a_mse: 19.3765 - c1_mse: 37.5921 - val_loss: 205.0214 - val_a_loss: 61.0795 - val_c1_loss: 143.9419 - val_a_mse: 61.0795 - val_c1_mse: 143.9419\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 33.1337 - a_loss: 12.1831 - c1_loss: 20.9506 - a_mse: 12.1831 - c1_mse: 20.9506 - val_loss: 193.9888 - val_a_loss: 49.4518 - val_c1_loss: 144.5370 - val_a_mse: 49.4518 - val_c1_mse: 144.5370\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 18.5709 - a_loss: 7.1002 - c1_loss: 11.4707 - a_mse: 7.1002 - c1_mse: 11.4707 - val_loss: 187.0282 - val_a_loss: 47.9604 - val_c1_loss: 139.0679 - val_a_mse: 47.9604 - val_c1_mse: 139.0679\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 10.4161 - a_loss: 4.3829 - c1_loss: 6.0332 - a_mse: 4.3829 - c1_mse: 6.0332 - val_loss: 119.3982 - val_a_loss: 37.9569 - val_c1_loss: 81.4413 - val_a_mse: 37.9569 - val_c1_mse: 81.4413\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 5.1299 - a_loss: 2.3767 - c1_loss: 2.7532 - a_mse: 2.3767 - c1_mse: 2.7532 - val_loss: 88.9845 - val_a_loss: 35.0829 - val_c1_loss: 53.9016 - val_a_mse: 35.0829 - val_c1_mse: 53.9016\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.3960 - a_loss: 1.0714 - c1_loss: 1.3246 - a_mse: 1.0714 - c1_mse: 1.3246 - val_loss: 66.4329 - val_a_loss: 26.3003 - val_c1_loss: 40.1326 - val_a_mse: 26.3003 - val_c1_mse: 40.1326\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.3568 - a_loss: 0.5657 - c1_loss: 0.7912 - a_mse: 0.5657 - c1_mse: 0.7912 - val_loss: 46.0101 - val_a_loss: 19.2039 - val_c1_loss: 26.8063 - val_a_mse: 19.2039 - val_c1_mse: 26.8063\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.8971 - a_loss: 0.4221 - c1_loss: 0.4750 - a_mse: 0.4221 - c1_mse: 0.4750 - val_loss: 40.3691 - val_a_loss: 15.1355 - val_c1_loss: 25.2336 - val_a_mse: 15.1355 - val_c1_mse: 25.2336\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5841 - a_loss: 0.2895 - c1_loss: 0.2946 - a_mse: 0.2895 - c1_mse: 0.2946 - val_loss: 38.2223 - val_a_loss: 14.7625 - val_c1_loss: 23.4599 - val_a_mse: 14.7625 - val_c1_mse: 23.4599\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3801 - a_loss: 0.1835 - c1_loss: 0.1967 - a_mse: 0.1835 - c1_mse: 0.1967 - val_loss: 40.2025 - val_a_loss: 17.0275 - val_c1_loss: 23.1750 - val_a_mse: 17.0275 - val_c1_mse: 23.1750\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1631 - a_loss: 0.0761 - c1_loss: 0.0870 - a_mse: 0.0761 - c1_mse: 0.0870 - val_loss: 37.8205 - val_a_loss: 15.4117 - val_c1_loss: 22.4087 - val_a_mse: 15.4117 - val_c1_mse: 22.4087\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1734 - a_loss: 0.0923 - c1_loss: 0.0811 - a_mse: 0.0923 - c1_mse: 0.0811 - val_loss: 35.6751 - val_a_loss: 14.7006 - val_c1_loss: 20.9745 - val_a_mse: 14.7006 - val_c1_mse: 20.9745\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1077 - a_loss: 0.0481 - c1_loss: 0.0596 - a_mse: 0.0481 - c1_mse: 0.0596 - val_loss: 35.5028 - val_a_loss: 14.5606 - val_c1_loss: 20.9422 - val_a_mse: 14.5606 - val_c1_mse: 20.9422\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0573 - a_loss: 0.0246 - c1_loss: 0.0327 - a_mse: 0.0246 - c1_mse: 0.0327 - val_loss: 33.8617 - val_a_loss: 13.4532 - val_c1_loss: 20.4085 - val_a_mse: 13.4532 - val_c1_mse: 20.4085\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0559 - a_loss: 0.0219 - c1_loss: 0.0339 - a_mse: 0.0219 - c1_mse: 0.0339 - val_loss: 33.6122 - val_a_loss: 13.7673 - val_c1_loss: 19.8449 - val_a_mse: 13.7673 - val_c1_mse: 19.8449\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0706 - a_loss: 0.0306 - c1_loss: 0.0399 - a_mse: 0.0306 - c1_mse: 0.0399 - val_loss: 31.7098 - val_a_loss: 12.7899 - val_c1_loss: 18.9199 - val_a_mse: 12.7899 - val_c1_mse: 18.9199\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0613 - a_loss: 0.0249 - c1_loss: 0.0364 - a_mse: 0.0249 - c1_mse: 0.0364 - val_loss: 32.5688 - val_a_loss: 13.4422 - val_c1_loss: 19.1266 - val_a_mse: 13.4422 - val_c1_mse: 19.1266\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0642 - a_loss: 0.0178 - c1_loss: 0.0464 - a_mse: 0.0178 - c1_mse: 0.0464 - val_loss: 30.7028 - val_a_loss: 12.2495 - val_c1_loss: 18.4532 - val_a_mse: 12.2495 - val_c1_mse: 18.4532\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0992 - a_loss: 0.0385 - c1_loss: 0.0607 - a_mse: 0.0385 - c1_mse: 0.0607 - val_loss: 31.1970 - val_a_loss: 12.6957 - val_c1_loss: 18.5013 - val_a_mse: 12.6957 - val_c1_mse: 18.5013\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4857 - a_loss: 0.0886 - c1_loss: 0.3972 - a_mse: 0.0886 - c1_mse: 0.3972 - val_loss: 32.5100 - val_a_loss: 14.0535 - val_c1_loss: 18.4566 - val_a_mse: 14.0535 - val_c1_mse: 18.4566\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1080 - a_loss: 0.0434 - c1_loss: 0.0645 - a_mse: 0.0434 - c1_mse: 0.0645 - val_loss: 30.2273 - val_a_loss: 12.2930 - val_c1_loss: 17.9343 - val_a_mse: 12.2930 - val_c1_mse: 17.9343\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0633 - a_loss: 0.0326 - c1_loss: 0.0306 - a_mse: 0.0326 - c1_mse: 0.0306 - val_loss: 30.9164 - val_a_loss: 12.7726 - val_c1_loss: 18.1439 - val_a_mse: 12.7726 - val_c1_mse: 18.1439\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1255 - a_loss: 0.0348 - c1_loss: 0.0907 - a_mse: 0.0348 - c1_mse: 0.0907 - val_loss: 30.1821 - val_a_loss: 12.2619 - val_c1_loss: 17.9202 - val_a_mse: 12.2619 - val_c1_mse: 17.9202\n",
      "Epoch 32/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2338 - a_loss: 0.0459 - c1_loss: 0.1879 - a_mse: 0.0459 - c1_mse: 0.1879 - val_loss: 29.3687 - val_a_loss: 11.2461 - val_c1_loss: 18.1226 - val_a_mse: 11.2461 - val_c1_mse: 18.1226\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.2937 - a_loss: 0.4469 - c1_loss: 1.8468 - a_mse: 0.4469 - c1_mse: 1.8468 - val_loss: 29.6177 - val_a_loss: 12.3734 - val_c1_loss: 17.2443 - val_a_mse: 12.3734 - val_c1_mse: 17.2443\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1287 - a_loss: 0.0333 - c1_loss: 0.0954 - a_mse: 0.0333 - c1_mse: 0.0954 - val_loss: 30.6923 - val_a_loss: 12.6952 - val_c1_loss: 17.9971 - val_a_mse: 12.6952 - val_c1_mse: 17.9971\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0392 - a_loss: 0.0145 - c1_loss: 0.0247 - a_mse: 0.0145 - c1_mse: 0.0247 - val_loss: 30.5311 - val_a_loss: 12.7532 - val_c1_loss: 17.7779 - val_a_mse: 12.7532 - val_c1_mse: 17.7779\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0160 - a_loss: 0.0057 - c1_loss: 0.0103 - a_mse: 0.0057 - c1_mse: 0.0103 - val_loss: 30.2889 - val_a_loss: 12.4362 - val_c1_loss: 17.8526 - val_a_mse: 12.4362 - val_c1_mse: 17.8526\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0168 - a_loss: 0.0062 - c1_loss: 0.0106 - a_mse: 0.0062 - c1_mse: 0.0106 - val_loss: 30.3476 - val_a_loss: 12.4857 - val_c1_loss: 17.8619 - val_a_mse: 12.4857 - val_c1_mse: 17.8619\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0345 - a_loss: 0.0104 - c1_loss: 0.0241 - a_mse: 0.0104 - c1_mse: 0.0241 - val_loss: 30.9641 - val_a_loss: 12.2495 - val_c1_loss: 18.7146 - val_a_mse: 12.2495 - val_c1_mse: 18.7146\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1130 - a_loss: 0.0255 - c1_loss: 0.0875 - a_mse: 0.0255 - c1_mse: 0.0875 - val_loss: 30.3675 - val_a_loss: 12.4383 - val_c1_loss: 17.9291 - val_a_mse: 12.4383 - val_c1_mse: 17.9291\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0249 - a_loss: 0.0086 - c1_loss: 0.0163 - a_mse: 0.0086 - c1_mse: 0.0163 - val_loss: 30.6069 - val_a_loss: 12.1731 - val_c1_loss: 18.4338 - val_a_mse: 12.1731 - val_c1_mse: 18.4338\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0352 - a_loss: 0.0155 - c1_loss: 0.0197 - a_mse: 0.0155 - c1_mse: 0.0197 - val_loss: 30.3781 - val_a_loss: 12.3898 - val_c1_loss: 17.9882 - val_a_mse: 12.3898 - val_c1_mse: 17.9882\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0221 - a_loss: 0.0065 - c1_loss: 0.0156 - a_mse: 0.0065 - c1_mse: 0.0156 - val_loss: 30.4124 - val_a_loss: 12.3863 - val_c1_loss: 18.0261 - val_a_mse: 12.3863 - val_c1_mse: 18.0261\n",
      "Epoch 43/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0194 - a_loss: 0.0074 - c1_loss: 0.0120 - a_mse: 0.0074 - c1_mse: 0.0120 - val_loss: 31.1058 - val_a_loss: 12.9911 - val_c1_loss: 18.1147 - val_a_mse: 12.9911 - val_c1_mse: 18.1147\n",
      "Epoch 44/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1242 - a_loss: 0.0470 - c1_loss: 0.0772 - a_mse: 0.0470 - c1_mse: 0.0772 - val_loss: 33.0167 - val_a_loss: 14.0735 - val_c1_loss: 18.9432 - val_a_mse: 14.0735 - val_c1_mse: 18.9432\n",
      "Epoch 45/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3005 - a_loss: 0.1014 - c1_loss: 0.1992 - a_mse: 0.1014 - c1_mse: 0.1992 - val_loss: 33.5941 - val_a_loss: 13.0755 - val_c1_loss: 20.5186 - val_a_mse: 13.0755 - val_c1_mse: 20.5186\n",
      "Epoch 46/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8087 - a_loss: 1.1822 - c1_loss: 0.6266 - a_mse: 1.1822 - c1_mse: 0.6266 - val_loss: 32.4910 - val_a_loss: 12.7772 - val_c1_loss: 19.7137 - val_a_mse: 12.7772 - val_c1_mse: 19.7137\n",
      "Epoch 47/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 6.2429 - a_loss: 1.2768 - c1_loss: 4.9662 - a_mse: 1.2768 - c1_mse: 4.9662 - val_loss: 135.3837 - val_a_loss: 44.4174 - val_c1_loss: 90.9662 - val_a_mse: 44.4174 - val_c1_mse: 90.9662\n",
      "Epoch 48/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 17.1004 - a_loss: 4.3187 - c1_loss: 12.7816 - a_mse: 4.3187 - c1_mse: 12.7816 - val_loss: 35.7624 - val_a_loss: 16.1339 - val_c1_loss: 19.6285 - val_a_mse: 16.1339 - val_c1_mse: 19.6285\n",
      "Epoch 49/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3555 - a_loss: 0.1884 - c1_loss: 0.1671 - a_mse: 0.1884 - c1_mse: 0.1671 - val_loss: 30.4422 - val_a_loss: 12.4269 - val_c1_loss: 18.0153 - val_a_mse: 12.4269 - val_c1_mse: 18.0153\n",
      "Epoch 50/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1471 - a_loss: 0.0529 - c1_loss: 0.0942 - a_mse: 0.0529 - c1_mse: 0.0942 - val_loss: 29.7146 - val_a_loss: 11.9861 - val_c1_loss: 17.7285 - val_a_mse: 11.9861 - val_c1_mse: 17.7285\n",
      "Epoch 51/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5906 - a_loss: 0.2226 - c1_loss: 0.3680 - a_mse: 0.2226 - c1_mse: 0.3680 - val_loss: 31.4583 - val_a_loss: 12.7796 - val_c1_loss: 18.6787 - val_a_mse: 12.7796 - val_c1_mse: 18.6787\n",
      "Epoch 52/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1600 - a_loss: 0.0896 - c1_loss: 0.0704 - a_mse: 0.0896 - c1_mse: 0.0704 - val_loss: 28.7951 - val_a_loss: 12.2039 - val_c1_loss: 16.5912 - val_a_mse: 12.2039 - val_c1_mse: 16.5912\n",
      "Epoch 53/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1383 - a_loss: 0.0704 - c1_loss: 0.0679 - a_mse: 0.0704 - c1_mse: 0.0679 - val_loss: 30.4560 - val_a_loss: 12.1992 - val_c1_loss: 18.2568 - val_a_mse: 12.1992 - val_c1_mse: 18.2568\n",
      "Epoch 54/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1041 - a_loss: 0.0264 - c1_loss: 0.0777 - a_mse: 0.0264 - c1_mse: 0.0777 - val_loss: 31.2746 - val_a_loss: 11.2065 - val_c1_loss: 20.0681 - val_a_mse: 11.2065 - val_c1_mse: 20.0681\n",
      "Epoch 55/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 32.2360 - a_loss: 8.3604 - c1_loss: 23.8756 - a_mse: 8.3604 - c1_mse: 23.8756 - val_loss: 123.4851 - val_a_loss: 42.0375 - val_c1_loss: 81.4476 - val_a_mse: 42.0375 - val_c1_mse: 81.4476\n",
      "Epoch 56/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 27.7930 - a_loss: 15.2424 - c1_loss: 12.5505 - a_mse: 15.2424 - c1_mse: 12.5505 - val_loss: 63.7043 - val_a_loss: 34.9715 - val_c1_loss: 28.7328 - val_a_mse: 34.9715 - val_c1_mse: 28.7328\n",
      "Epoch 57/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 46.3207 - a_loss: 31.7506 - c1_loss: 14.5701 - a_mse: 31.7506 - c1_mse: 14.5701 - val_loss: 65.3811 - val_a_loss: 24.0203 - val_c1_loss: 41.3608 - val_a_mse: 24.0203 - val_c1_mse: 41.3608\n",
      "Epoch 58/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 20.5613 - a_loss: 12.4120 - c1_loss: 8.1492 - a_mse: 12.4120 - c1_mse: 8.1492 - val_loss: 44.0763 - val_a_loss: 16.2367 - val_c1_loss: 27.8396 - val_a_mse: 16.2367 - val_c1_mse: 27.8396\n",
      "Epoch 59/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2755 - a_loss: 0.1484 - c1_loss: 0.1270 - a_mse: 0.1484 - c1_mse: 0.1270 - val_loss: 36.9594 - val_a_loss: 14.9008 - val_c1_loss: 22.0585 - val_a_mse: 14.9008 - val_c1_mse: 22.0585\n",
      "Epoch 60/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0510 - a_loss: 0.0228 - c1_loss: 0.0282 - a_mse: 0.0228 - c1_mse: 0.0282 - val_loss: 33.5386 - val_a_loss: 13.7107 - val_c1_loss: 19.8279 - val_a_mse: 13.7107 - val_c1_mse: 19.8279\n",
      "Epoch 61/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0592 - a_loss: 0.0246 - c1_loss: 0.0346 - a_mse: 0.0246 - c1_mse: 0.0346 - val_loss: 32.5593 - val_a_loss: 13.1625 - val_c1_loss: 19.3967 - val_a_mse: 13.1625 - val_c1_mse: 19.3967\n",
      "Epoch 62/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0274 - a_loss: 0.0111 - c1_loss: 0.0163 - a_mse: 0.0111 - c1_mse: 0.0163 - val_loss: 31.2958 - val_a_loss: 12.6769 - val_c1_loss: 18.6188 - val_a_mse: 12.6769 - val_c1_mse: 18.6188\n",
      "Epoch 63/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0062 - a_loss: 0.0026 - c1_loss: 0.0036 - a_mse: 0.0026 - c1_mse: 0.0036 - val_loss: 30.7946 - val_a_loss: 12.2856 - val_c1_loss: 18.5089 - val_a_mse: 12.2856 - val_c1_mse: 18.5089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1268 - a_loss: 0.0363 - c1_loss: 0.0905 - a_mse: 0.0363 - c1_mse: 0.0905 - val_loss: 30.0958 - val_a_loss: 12.4173 - val_c1_loss: 17.6784 - val_a_mse: 12.4173 - val_c1_mse: 17.6784\n",
      "Epoch 65/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 11.0892 - a_loss: 4.1080 - c1_loss: 6.9812 - a_mse: 4.1080 - c1_mse: 6.9812 - val_loss: 252.8117 - val_a_loss: 72.9922 - val_c1_loss: 179.8195 - val_a_mse: 72.9922 - val_c1_mse: 179.8195\n",
      "Epoch 66/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 87.9588 - a_loss: 35.3760 - c1_loss: 52.5828 - a_mse: 35.3760 - c1_mse: 52.5828 - val_loss: 557.5809 - val_a_loss: 325.6768 - val_c1_loss: 231.9041 - val_a_mse: 325.6768 - val_c1_mse: 231.9041\n",
      "Epoch 67/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 47.6499 - a_loss: 28.8683 - c1_loss: 18.7817 - a_mse: 28.8683 - c1_mse: 18.7817 - val_loss: 78.7281 - val_a_loss: 29.6812 - val_c1_loss: 49.0469 - val_a_mse: 29.6812 - val_c1_mse: 49.0469\n",
      "Epoch 68/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 7.3223 - a_loss: 3.5242 - c1_loss: 3.7981 - a_mse: 3.5242 - c1_mse: 3.7981 - val_loss: 47.6102 - val_a_loss: 20.5596 - val_c1_loss: 27.0505 - val_a_mse: 20.5596 - val_c1_mse: 27.0505\n",
      "Epoch 69/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5803 - a_loss: 0.2655 - c1_loss: 0.3148 - a_mse: 0.2655 - c1_mse: 0.3148 - val_loss: 37.9386 - val_a_loss: 14.9690 - val_c1_loss: 22.9696 - val_a_mse: 14.9690 - val_c1_mse: 22.9696\n",
      "Epoch 70/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5750 - a_loss: 0.2078 - c1_loss: 0.3671 - a_mse: 0.2078 - c1_mse: 0.3671 - val_loss: 32.5385 - val_a_loss: 12.4844 - val_c1_loss: 20.0542 - val_a_mse: 12.4844 - val_c1_mse: 20.0542\n",
      "Epoch 71/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0620 - a_loss: 0.0261 - c1_loss: 0.0359 - a_mse: 0.0261 - c1_mse: 0.0359 - val_loss: 34.3057 - val_a_loss: 13.7249 - val_c1_loss: 20.5808 - val_a_mse: 13.7249 - val_c1_mse: 20.5808\n",
      "Epoch 72/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0249 - a_loss: 0.0120 - c1_loss: 0.0129 - a_mse: 0.0120 - c1_mse: 0.0129 - val_loss: 31.0522 - val_a_loss: 12.4151 - val_c1_loss: 18.6371 - val_a_mse: 12.4151 - val_c1_mse: 18.6371\n",
      "Epoch 73/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0070 - a_loss: 0.0034 - c1_loss: 0.0036 - a_mse: 0.0034 - c1_mse: 0.0036 - val_loss: 30.3280 - val_a_loss: 12.1626 - val_c1_loss: 18.1654 - val_a_mse: 12.1626 - val_c1_mse: 18.1654\n",
      "Epoch 74/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0028 - a_loss: 0.0014 - c1_loss: 0.0014 - a_mse: 0.0014 - c1_mse: 0.0014 - val_loss: 30.0130 - val_a_loss: 12.0204 - val_c1_loss: 17.9925 - val_a_mse: 12.0204 - val_c1_mse: 17.9925\n",
      "Epoch 75/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0016 - a_loss: 7.0396e-04 - c1_loss: 8.5736e-04 - a_mse: 7.0396e-04 - c1_mse: 8.5736e-04 - val_loss: 29.7065 - val_a_loss: 11.8651 - val_c1_loss: 17.8414 - val_a_mse: 11.8651 - val_c1_mse: 17.8414\n",
      "Epoch 76/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0013 - a_loss: 6.2637e-04 - c1_loss: 6.8073e-04 - a_mse: 6.2637e-04 - c1_mse: 6.8073e-04 - val_loss: 29.4785 - val_a_loss: 11.7975 - val_c1_loss: 17.6811 - val_a_mse: 11.7975 - val_c1_mse: 17.6811\n",
      "Epoch 77/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0011 - a_loss: 5.5671e-04 - c1_loss: 5.0869e-04 - a_mse: 5.5671e-04 - c1_mse: 5.0869e-04 - val_loss: 29.4405 - val_a_loss: 11.8151 - val_c1_loss: 17.6254 - val_a_mse: 11.8151 - val_c1_mse: 17.6254\n",
      "Epoch 78/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0056 - a_loss: 0.0033 - c1_loss: 0.0023 - a_mse: 0.0033 - c1_mse: 0.0023 - val_loss: 29.1967 - val_a_loss: 11.7436 - val_c1_loss: 17.4532 - val_a_mse: 11.7436 - val_c1_mse: 17.4532\n",
      "Epoch 79/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0437 - a_loss: 0.0207 - c1_loss: 0.0230 - a_mse: 0.0207 - c1_mse: 0.0230 - val_loss: 29.9569 - val_a_loss: 11.9745 - val_c1_loss: 17.9824 - val_a_mse: 11.9745 - val_c1_mse: 17.9824\n",
      "Epoch 80/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0081 - a_loss: 0.0039 - c1_loss: 0.0042 - a_mse: 0.0039 - c1_mse: 0.0042 - val_loss: 28.9518 - val_a_loss: 11.5693 - val_c1_loss: 17.3825 - val_a_mse: 11.5693 - val_c1_mse: 17.3825\n",
      "Epoch 81/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0066 - a_loss: 0.0024 - c1_loss: 0.0043 - a_mse: 0.0024 - c1_mse: 0.0043 - val_loss: 29.2387 - val_a_loss: 11.6726 - val_c1_loss: 17.5661 - val_a_mse: 11.6726 - val_c1_mse: 17.5661\n",
      "Epoch 82/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 9.8152e-04 - a_loss: 5.5525e-04 - c1_loss: 4.2627e-04 - a_mse: 5.5525e-04 - c1_mse: 4.2627e-04 - val_loss: 29.0380 - val_a_loss: 11.5851 - val_c1_loss: 17.4529 - val_a_mse: 11.5851 - val_c1_mse: 17.4529\n",
      "Epoch 83/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0783 - a_loss: 0.0398 - c1_loss: 0.0385 - a_mse: 0.0398 - c1_mse: 0.0385 - val_loss: 29.9259 - val_a_loss: 11.9552 - val_c1_loss: 17.9707 - val_a_mse: 11.9552 - val_c1_mse: 17.9707\n",
      "Epoch 84/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.7423 - a_loss: 1.2948 - c1_loss: 1.4475 - a_mse: 1.2948 - c1_mse: 1.4475 - val_loss: 29.4918 - val_a_loss: 12.1849 - val_c1_loss: 17.3070 - val_a_mse: 12.1849 - val_c1_mse: 17.3070\n",
      "Epoch 85/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 41.4089 - a_loss: 21.0006 - c1_loss: 20.4083 - a_mse: 21.0006 - c1_mse: 20.4083 - val_loss: 243.0163 - val_a_loss: 130.7456 - val_c1_loss: 112.2706 - val_a_mse: 130.7456 - val_c1_mse: 112.2706\n",
      "Epoch 86/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 163.4294 - a_loss: 90.9735 - c1_loss: 72.4559 - a_mse: 90.9735 - c1_mse: 72.4559 - val_loss: 264.4438 - val_a_loss: 135.5706 - val_c1_loss: 128.8732 - val_a_mse: 135.5706 - val_c1_mse: 128.8732\n",
      "Epoch 87/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 12.8993 - a_loss: 5.6288 - c1_loss: 7.2705 - a_mse: 5.6288 - c1_mse: 7.2705 - val_loss: 100.8141 - val_a_loss: 45.1315 - val_c1_loss: 55.6826 - val_a_mse: 45.1315 - val_c1_mse: 55.6826\n",
      "Epoch 88/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 45.4389 - a_loss: 19.3293 - c1_loss: 26.1096 - a_mse: 19.3293 - c1_mse: 26.1096 - val_loss: 56.4457 - val_a_loss: 33.0313 - val_c1_loss: 23.4144 - val_a_mse: 33.0313 - val_c1_mse: 23.4144\n",
      "Epoch 89/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 23.4584 - a_loss: 10.0206 - c1_loss: 13.4377 - a_mse: 10.0206 - c1_mse: 13.4377 - val_loss: 40.2332 - val_a_loss: 16.2580 - val_c1_loss: 23.9751 - val_a_mse: 16.2580 - val_c1_mse: 23.9751\n",
      "Epoch 90/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3470 - a_loss: 0.1826 - c1_loss: 0.1645 - a_mse: 0.1826 - c1_mse: 0.1645 - val_loss: 35.5059 - val_a_loss: 15.1487 - val_c1_loss: 20.3572 - val_a_mse: 15.1487 - val_c1_mse: 20.3572\n",
      "Epoch 91/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1117 - a_loss: 0.0506 - c1_loss: 0.0611 - a_mse: 0.0506 - c1_mse: 0.0611 - val_loss: 35.1731 - val_a_loss: 14.4540 - val_c1_loss: 20.7191 - val_a_mse: 14.4540 - val_c1_mse: 20.7191\n",
      "Epoch 92/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0900 - a_loss: 0.0401 - c1_loss: 0.0498 - a_mse: 0.0401 - c1_mse: 0.0498 - val_loss: 29.2914 - val_a_loss: 10.9810 - val_c1_loss: 18.3104 - val_a_mse: 10.9810 - val_c1_mse: 18.3104\n",
      "Epoch 93/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2391 - a_loss: 0.1230 - c1_loss: 0.1161 - a_mse: 0.1230 - c1_mse: 0.1161 - val_loss: 27.5066 - val_a_loss: 10.3095 - val_c1_loss: 17.1970 - val_a_mse: 10.3095 - val_c1_mse: 17.1970\n",
      "Epoch 94/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1145 - a_loss: 0.0557 - c1_loss: 0.0588 - a_mse: 0.0557 - c1_mse: 0.0588 - val_loss: 25.7562 - val_a_loss: 10.0191 - val_c1_loss: 15.7371 - val_a_mse: 10.0191 - val_c1_mse: 15.7371\n",
      "Epoch 95/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0544 - a_loss: 0.0260 - c1_loss: 0.0284 - a_mse: 0.0260 - c1_mse: 0.0284 - val_loss: 29.6007 - val_a_loss: 12.1883 - val_c1_loss: 17.4124 - val_a_mse: 12.1883 - val_c1_mse: 17.4124\n",
      "Epoch 96/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0460 - a_loss: 0.0235 - c1_loss: 0.0225 - a_mse: 0.0235 - c1_mse: 0.0225 - val_loss: 27.3013 - val_a_loss: 11.1066 - val_c1_loss: 16.1947 - val_a_mse: 11.1066 - val_c1_mse: 16.1947\n",
      "Epoch 97/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1132 - a_loss: 0.0682 - c1_loss: 0.0450 - a_mse: 0.0682 - c1_mse: 0.0450 - val_loss: 28.5198 - val_a_loss: 11.5342 - val_c1_loss: 16.9856 - val_a_mse: 11.5342 - val_c1_mse: 16.9856\n",
      "Epoch 98/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0275 - a_loss: 0.0145 - c1_loss: 0.0130 - a_mse: 0.0145 - c1_mse: 0.0130 - val_loss: 26.8506 - val_a_loss: 10.4169 - val_c1_loss: 16.4336 - val_a_mse: 10.4169 - val_c1_mse: 16.4336\n",
      "Epoch 99/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1179 - a_loss: 0.0707 - c1_loss: 0.0471 - a_mse: 0.0707 - c1_mse: 0.0471 - val_loss: 27.7218 - val_a_loss: 10.6848 - val_c1_loss: 17.0370 - val_a_mse: 10.6848 - val_c1_mse: 17.0370\n",
      "Epoch 100/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0176 - a_loss: 0.0068 - c1_loss: 0.0108 - a_mse: 0.0068 - c1_mse: 0.0108 - val_loss: 27.8349 - val_a_loss: 11.2592 - val_c1_loss: 16.5757 - val_a_mse: 11.2592 - val_c1_mse: 16.5757\n",
      "Epoch 101/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0237 - a_loss: 0.0133 - c1_loss: 0.0104 - a_mse: 0.0133 - c1_mse: 0.0104 - val_loss: 27.5394 - val_a_loss: 11.1079 - val_c1_loss: 16.4315 - val_a_mse: 11.1079 - val_c1_mse: 16.4315\n",
      "Epoch 102/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0061 - a_loss: 0.0030 - c1_loss: 0.0032 - a_mse: 0.0030 - c1_mse: 0.0032 - val_loss: 27.2808 - val_a_loss: 10.7906 - val_c1_loss: 16.4902 - val_a_mse: 10.7906 - val_c1_mse: 16.4902\n",
      "Epoch 103/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0187 - a_loss: 0.0083 - c1_loss: 0.0105 - a_mse: 0.0083 - c1_mse: 0.0105 - val_loss: 28.0926 - val_a_loss: 11.2686 - val_c1_loss: 16.8239 - val_a_mse: 11.2686 - val_c1_mse: 16.8239\n",
      "Epoch 104/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0487 - a_loss: 0.0206 - c1_loss: 0.0281 - a_mse: 0.0206 - c1_mse: 0.0281 - val_loss: 27.1764 - val_a_loss: 10.6088 - val_c1_loss: 16.5676 - val_a_mse: 10.6088 - val_c1_mse: 16.5676\n",
      "Epoch 105/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1004 - a_loss: 0.0430 - c1_loss: 0.0575 - a_mse: 0.0430 - c1_mse: 0.0575 - val_loss: 22.4893 - val_a_loss: 8.2820 - val_c1_loss: 14.2073 - val_a_mse: 8.2820 - val_c1_mse: 14.2073\n",
      "Epoch 106/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 4.1602 - a_loss: 2.0584 - c1_loss: 2.1018 - a_mse: 2.0584 - c1_mse: 2.1018 - val_loss: 29.1230 - val_a_loss: 13.1441 - val_c1_loss: 15.9789 - val_a_mse: 13.1441 - val_c1_mse: 15.9789\n",
      "Epoch 107/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 38.6446 - a_loss: 22.5876 - c1_loss: 16.0570 - a_mse: 22.5876 - c1_mse: 16.0570 - val_loss: 54.3706 - val_a_loss: 23.1800 - val_c1_loss: 31.1906 - val_a_mse: 23.1800 - val_c1_mse: 31.1906\n",
      "Epoch 108/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 19.5454 - a_loss: 10.1072 - c1_loss: 9.4381 - a_mse: 10.1072 - c1_mse: 9.4381 - val_loss: 37.1660 - val_a_loss: 14.0300 - val_c1_loss: 23.1360 - val_a_mse: 14.0300 - val_c1_mse: 23.1360\n",
      "Epoch 109/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.5676 - a_loss: 0.8058 - c1_loss: 0.7618 - a_mse: 0.8058 - c1_mse: 0.7618 - val_loss: 31.7107 - val_a_loss: 11.9956 - val_c1_loss: 19.7151 - val_a_mse: 11.9956 - val_c1_mse: 19.7151\n",
      "Epoch 110/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3659 - a_loss: 0.1729 - c1_loss: 0.1930 - a_mse: 0.1729 - c1_mse: 0.1930 - val_loss: 24.4031 - val_a_loss: 9.2439 - val_c1_loss: 15.1592 - val_a_mse: 9.2439 - val_c1_mse: 15.1592\n",
      "Epoch 111/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.4854 - a_loss: 0.7051 - c1_loss: 0.7803 - a_mse: 0.7051 - c1_mse: 0.7803 - val_loss: 27.5701 - val_a_loss: 9.8392 - val_c1_loss: 17.7309 - val_a_mse: 9.8392 - val_c1_mse: 17.7309\n",
      "Epoch 112/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 4.7339 - a_loss: 2.2263 - c1_loss: 2.5077 - a_mse: 2.2263 - c1_mse: 2.5077 - val_loss: 29.4857 - val_a_loss: 11.3330 - val_c1_loss: 18.1527 - val_a_mse: 11.3330 - val_c1_mse: 18.1527\n",
      "Epoch 113/150\n",
      "80/80 [==============================] - ETA: 0s - loss: 11.1233 - a_loss: 7.0492 - c1_loss: 4.0741 - a_mse: 7.0492 - c1_mse: 4.07 - 0s 5ms/step - loss: 13.9368 - a_loss: 8.6451 - c1_loss: 5.2917 - a_mse: 8.6451 - c1_mse: 5.2917 - val_loss: 49.2024 - val_a_loss: 30.1063 - val_c1_loss: 19.0961 - val_a_mse: 30.1063 - val_c1_mse: 19.0961\n",
      "Epoch 114/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 74.3707 - a_loss: 44.3242 - c1_loss: 30.0464 - a_mse: 44.3242 - c1_mse: 30.0464 - val_loss: 64.0093 - val_a_loss: 37.0105 - val_c1_loss: 26.9988 - val_a_mse: 37.0105 - val_c1_mse: 26.9988\n",
      "Epoch 115/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.7515 - a_loss: 1.5953 - c1_loss: 1.1563 - a_mse: 1.5953 - c1_mse: 1.1563 - val_loss: 18.9691 - val_a_loss: 7.2000 - val_c1_loss: 11.7690 - val_a_mse: 7.2000 - val_c1_mse: 11.7690\n",
      "Epoch 116/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8342 - a_loss: 0.7626 - c1_loss: 1.0715 - a_mse: 0.7626 - c1_mse: 1.0715 - val_loss: 27.3329 - val_a_loss: 10.8212 - val_c1_loss: 16.5117 - val_a_mse: 10.8212 - val_c1_mse: 16.5117\n",
      "Epoch 117/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0844 - a_loss: 0.0375 - c1_loss: 0.0469 - a_mse: 0.0375 - c1_mse: 0.0469 - val_loss: 29.2169 - val_a_loss: 11.7453 - val_c1_loss: 17.4715 - val_a_mse: 11.7453 - val_c1_mse: 17.4715\n",
      "Epoch 118/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0960 - a_loss: 0.0343 - c1_loss: 0.0617 - a_mse: 0.0343 - c1_mse: 0.0617 - val_loss: 30.0287 - val_a_loss: 12.5609 - val_c1_loss: 17.4678 - val_a_mse: 12.5609 - val_c1_mse: 17.4678\n",
      "Epoch 119/150\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.3762 - a_loss: 0.1618 - c1_loss: 0.2144 - a_mse: 0.1618 - c1_mse: 0.214 - 0s 5ms/step - loss: 0.3739 - a_loss: 0.1630 - c1_loss: 0.2108 - a_mse: 0.1630 - c1_mse: 0.2108 - val_loss: 28.0285 - val_a_loss: 11.4858 - val_c1_loss: 16.5427 - val_a_mse: 11.4858 - val_c1_mse: 16.5427\n",
      "Epoch 120/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7423 - a_loss: 0.3439 - c1_loss: 0.3984 - a_mse: 0.3439 - c1_mse: 0.3984 - val_loss: 32.6983 - val_a_loss: 14.0347 - val_c1_loss: 18.6636 - val_a_mse: 14.0347 - val_c1_mse: 18.6636\n",
      "Epoch 121/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.0967 - a_loss: 0.5364 - c1_loss: 0.5603 - a_mse: 0.5364 - c1_mse: 0.5603 - val_loss: 30.9009 - val_a_loss: 11.0082 - val_c1_loss: 19.8928 - val_a_mse: 11.0082 - val_c1_mse: 19.8928\n",
      "Epoch 122/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.7176 - a_loss: 0.3737 - c1_loss: 0.3439 - a_mse: 0.3737 - c1_mse: 0.3439 - val_loss: 26.4556 - val_a_loss: 9.5949 - val_c1_loss: 16.8606 - val_a_mse: 9.5949 - val_c1_mse: 16.8606\n",
      "Epoch 123/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4167 - a_loss: 0.1854 - c1_loss: 0.2313 - a_mse: 0.1854 - c1_mse: 0.2313 - val_loss: 26.5200 - val_a_loss: 8.8708 - val_c1_loss: 17.6492 - val_a_mse: 8.8708 - val_c1_mse: 17.6492\n",
      "Epoch 124/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4681 - a_loss: 0.2200 - c1_loss: 0.2481 - a_mse: 0.2200 - c1_mse: 0.2481 - val_loss: 28.4026 - val_a_loss: 9.6089 - val_c1_loss: 18.7937 - val_a_mse: 9.6089 - val_c1_mse: 18.7937\n",
      "Epoch 125/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 22.6275 - a_loss: 9.4480 - c1_loss: 13.1795 - a_mse: 9.4480 - c1_mse: 13.1795 - val_loss: 42.7878 - val_a_loss: 16.3062 - val_c1_loss: 26.4816 - val_a_mse: 16.3062 - val_c1_mse: 26.4816\n",
      "Epoch 126/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 5ms/step - loss: 10.0975 - a_loss: 5.2233 - c1_loss: 4.8743 - a_mse: 5.2233 - c1_mse: 4.8743 - val_loss: 36.5512 - val_a_loss: 14.3985 - val_c1_loss: 22.1526 - val_a_mse: 14.3985 - val_c1_mse: 22.1526\n",
      "Epoch 127/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8073 - a_loss: 0.6674 - c1_loss: 1.1399 - a_mse: 0.6674 - c1_mse: 1.1399 - val_loss: 44.3043 - val_a_loss: 14.5250 - val_c1_loss: 29.7793 - val_a_mse: 14.5250 - val_c1_mse: 29.7793\n",
      "Epoch 128/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.2082 - a_loss: 1.8076 - c1_loss: 1.4006 - a_mse: 1.8076 - c1_mse: 1.4006 - val_loss: 34.4544 - val_a_loss: 13.2906 - val_c1_loss: 21.1638 - val_a_mse: 13.2906 - val_c1_mse: 21.1638\n",
      "Epoch 129/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.4595 - a_loss: 2.1165 - c1_loss: 1.3430 - a_mse: 2.1165 - c1_mse: 1.3430 - val_loss: 32.6209 - val_a_loss: 11.4816 - val_c1_loss: 21.1393 - val_a_mse: 11.4816 - val_c1_mse: 21.1393\n",
      "Epoch 130/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6071 - a_loss: 0.9080 - c1_loss: 0.6991 - a_mse: 0.9080 - c1_mse: 0.6991 - val_loss: 39.3780 - val_a_loss: 15.1901 - val_c1_loss: 24.1879 - val_a_mse: 15.1901 - val_c1_mse: 24.1879\n",
      "Epoch 131/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 5.9385 - a_loss: 1.6152 - c1_loss: 4.3233 - a_mse: 1.6152 - c1_mse: 4.3233 - val_loss: 707.9649 - val_a_loss: 199.0772 - val_c1_loss: 508.8877 - val_a_mse: 199.0772 - val_c1_mse: 508.8877\n",
      "Epoch 132/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 780.6751 - a_loss: 312.3509 - c1_loss: 468.3243 - a_mse: 312.3509 - c1_mse: 468.3243 - val_loss: 65.6638 - val_a_loss: 22.7639 - val_c1_loss: 42.8999 - val_a_mse: 22.7639 - val_c1_mse: 42.8999\n",
      "Epoch 133/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 7.0868 - a_loss: 3.4129 - c1_loss: 3.6739 - a_mse: 3.4129 - c1_mse: 3.6739 - val_loss: 16.6109 - val_a_loss: 5.9231 - val_c1_loss: 10.6878 - val_a_mse: 5.9231 - val_c1_mse: 10.6878\n",
      "Epoch 134/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0378 - a_loss: 0.0206 - c1_loss: 0.0173 - a_mse: 0.0206 - c1_mse: 0.0173 - val_loss: 15.9016 - val_a_loss: 6.0452 - val_c1_loss: 9.8564 - val_a_mse: 6.0452 - val_c1_mse: 9.8564\n",
      "Epoch 135/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0014 - a_loss: 8.2215e-04 - c1_loss: 5.9998e-04 - a_mse: 8.2215e-04 - c1_mse: 5.9998e-04 - val_loss: 15.5899 - val_a_loss: 5.7685 - val_c1_loss: 9.8213 - val_a_mse: 5.7685 - val_c1_mse: 9.8213\n",
      "Epoch 136/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.2705e-04 - a_loss: 2.1276e-04 - c1_loss: 1.1429e-04 - a_mse: 2.1276e-04 - c1_mse: 1.1429e-04 - val_loss: 15.5587 - val_a_loss: 5.7242 - val_c1_loss: 9.8345 - val_a_mse: 5.7242 - val_c1_mse: 9.8345\n",
      "Epoch 137/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.2108e-04 - a_loss: 1.5658e-04 - c1_loss: 6.4505e-05 - a_mse: 1.5658e-04 - c1_mse: 6.4505e-05 - val_loss: 15.4991 - val_a_loss: 5.6743 - val_c1_loss: 9.8248 - val_a_mse: 5.6743 - val_c1_mse: 9.8248\n",
      "Epoch 138/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.4605e-04 - a_loss: 1.0555e-04 - c1_loss: 4.0505e-05 - a_mse: 1.0555e-04 - c1_mse: 4.0505e-05 - val_loss: 15.5536 - val_a_loss: 5.6968 - val_c1_loss: 9.8568 - val_a_mse: 5.6968 - val_c1_mse: 9.8568\n",
      "Epoch 139/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8897e-04 - a_loss: 1.3245e-04 - c1_loss: 5.6518e-05 - a_mse: 1.3245e-04 - c1_mse: 5.6518e-05 - val_loss: 15.5601 - val_a_loss: 5.7112 - val_c1_loss: 9.8489 - val_a_mse: 5.7112 - val_c1_mse: 9.8489\n",
      "Epoch 140/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.2402e-04 - a_loss: 8.7271e-05 - c1_loss: 3.6746e-05 - a_mse: 8.7271e-05 - c1_mse: 3.6746e-05 - val_loss: 15.5549 - val_a_loss: 5.7025 - val_c1_loss: 9.8524 - val_a_mse: 5.7025 - val_c1_mse: 9.8524\n",
      "Epoch 141/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 5.5146e-05 - a_loss: 4.0943e-05 - c1_loss: 1.4203e-05 - a_mse: 4.0943e-05 - c1_mse: 1.4203e-05 - val_loss: 15.5204 - val_a_loss: 5.6767 - val_c1_loss: 9.8437 - val_a_mse: 5.6767 - val_c1_mse: 9.8437\n",
      "Epoch 142/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 5.3953e-05 - a_loss: 3.9022e-05 - c1_loss: 1.4931e-05 - a_mse: 3.9022e-05 - c1_mse: 1.4931e-05 - val_loss: 15.4871 - val_a_loss: 5.6581 - val_c1_loss: 9.8290 - val_a_mse: 5.6581 - val_c1_mse: 9.8290\n",
      "Epoch 143/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.6904e-05 - a_loss: 2.6669e-05 - c1_loss: 1.0235e-05 - a_mse: 2.6669e-05 - c1_mse: 1.0235e-05 - val_loss: 15.5043 - val_a_loss: 5.6668 - val_c1_loss: 9.8375 - val_a_mse: 5.6668 - val_c1_mse: 9.8375\n",
      "Epoch 144/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6565e-05 - a_loss: 1.2799e-05 - c1_loss: 3.7660e-06 - a_mse: 1.2799e-05 - c1_mse: 3.7660e-06 - val_loss: 15.5282 - val_a_loss: 5.6806 - val_c1_loss: 9.8477 - val_a_mse: 5.6806 - val_c1_mse: 9.8477\n",
      "Epoch 145/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.5412e-05 - a_loss: 2.5124e-05 - c1_loss: 1.0288e-05 - a_mse: 2.5124e-05 - c1_mse: 1.0288e-05 - val_loss: 15.5210 - val_a_loss: 5.6761 - val_c1_loss: 9.8449 - val_a_mse: 5.6761 - val_c1_mse: 9.8449\n",
      "Epoch 146/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7665e-05 - a_loss: 1.2280e-05 - c1_loss: 5.3847e-06 - a_mse: 1.2280e-05 - c1_mse: 5.3847e-06 - val_loss: 15.4919 - val_a_loss: 5.6562 - val_c1_loss: 9.8357 - val_a_mse: 5.6562 - val_c1_mse: 9.8357\n",
      "Epoch 147/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 7.6837e-06 - a_loss: 5.5004e-06 - c1_loss: 2.1834e-06 - a_mse: 5.5004e-06 - c1_mse: 2.1834e-06 - val_loss: 15.4701 - val_a_loss: 5.6409 - val_c1_loss: 9.8292 - val_a_mse: 5.6409 - val_c1_mse: 9.8292\n",
      "Epoch 148/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.5514e-05 - a_loss: 1.7258e-05 - c1_loss: 8.2565e-06 - a_mse: 1.7258e-05 - c1_mse: 8.2565e-06 - val_loss: 15.5024 - val_a_loss: 5.6641 - val_c1_loss: 9.8383 - val_a_mse: 5.6641 - val_c1_mse: 9.8383\n",
      "Epoch 149/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 7.7150e-06 - a_loss: 5.2576e-06 - c1_loss: 2.4574e-06 - a_mse: 5.2576e-06 - c1_mse: 2.4574e-06 - val_loss: 15.4983 - val_a_loss: 5.6605 - val_c1_loss: 9.8377 - val_a_mse: 5.6605 - val_c1_mse: 9.8377\n",
      "Epoch 150/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 4.9241e-06 - a_loss: 3.4614e-06 - c1_loss: 1.4627e-06 - a_mse: 3.4614e-06 - c1_mse: 1.4627e-06 - val_loss: 15.5228 - val_a_loss: 5.6750 - val_c1_loss: 9.8478 - val_a_mse: 5.6750 - val_c1_mse: 9.8478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2ca7b6b7d48>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "model.fit([x1,x2],[y1,y2], epochs=150, batch_size=1, validation_split = 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step\n",
      "loss : [3.104566005865272, 1.1350101232528687, 1.9695559740066528, 1.1350101232528687, 1.9695559740066528]\n",
      "mse : 7.574532219223329e-07\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "loss = model.evaluate([x1,x2], [y1,y2], batch_size=1)\n",
    "print(\"loss :\", loss)\n",
    "print(\"mse :\", mse)\n",
    "# => 사실상 모델의 성능을 직관적으로 파악하기 어렵다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict 값은 [[[711. 711.  -0.]\n",
      "  [712. 712.   1.]\n",
      "  [713. 713.   2.]\n",
      "  [714. 714.   3.]\n",
      "  [715. 715.   4.]\n",
      "  [716. 716.   5.]\n",
      "  [717. 717.   6.]\n",
      "  [718. 718.   7.]\n",
      "  [719. 719.   8.]\n",
      "  [720. 720.   9.]\n",
      "  [721. 721.  10.]\n",
      "  [722. 722.  11.]\n",
      "  [723. 723.  12.]\n",
      "  [724. 724.  13.]\n",
      "  [725. 725.  14.]\n",
      "  [726. 726.  15.]\n",
      "  [727. 727.  16.]\n",
      "  [728. 728.  17.]\n",
      "  [729. 729.  18.]\n",
      "  [730. 730.  19.]\n",
      "  [731. 731.  20.]\n",
      "  [732. 732.  21.]\n",
      "  [733. 733.  22.]\n",
      "  [734. 734.  23.]\n",
      "  [735. 735.  24.]\n",
      "  [736. 736.  25.]\n",
      "  [737. 737.  26.]\n",
      "  [738. 738.  27.]\n",
      "  [739. 739.  28.]\n",
      "  [740. 740.  29.]\n",
      "  [741. 741.  30.]\n",
      "  [742. 742.  31.]\n",
      "  [743. 743.  32.]\n",
      "  [744. 744.  33.]\n",
      "  [745. 745.  34.]\n",
      "  [746. 746.  35.]\n",
      "  [747. 747.  36.]\n",
      "  [748. 748.  37.]\n",
      "  [749. 749.  38.]\n",
      "  [750. 750.  39.]\n",
      "  [751. 751.  40.]\n",
      "  [752. 752.  41.]\n",
      "  [753. 753.  42.]\n",
      "  [754. 754.  43.]\n",
      "  [755. 755.  44.]\n",
      "  [756. 756.  45.]\n",
      "  [757. 757.  46.]\n",
      "  [758. 758.  47.]\n",
      "  [759. 759.  48.]\n",
      "  [760. 760.  49.]\n",
      "  [761. 761.  50.]\n",
      "  [762. 762.  51.]\n",
      "  [763. 763.  52.]\n",
      "  [764. 764.  53.]\n",
      "  [765. 765.  54.]\n",
      "  [766. 766.  55.]\n",
      "  [767. 767.  56.]\n",
      "  [768. 768.  57.]\n",
      "  [769. 769.  58.]\n",
      "  [770. 770.  59.]\n",
      "  [771. 771.  60.]\n",
      "  [772. 772.  61.]\n",
      "  [773. 773.  62.]\n",
      "  [774. 774.  63.]\n",
      "  [775. 775.  64.]\n",
      "  [776. 776.  65.]\n",
      "  [777. 777.  66.]\n",
      "  [778. 778.  67.]\n",
      "  [779. 779.  68.]\n",
      "  [780. 780.  69.]\n",
      "  [781. 781.  70.]\n",
      "  [782. 782.  71.]\n",
      "  [783. 783.  72.]\n",
      "  [784. 784.  73.]\n",
      "  [785. 785.  74.]\n",
      "  [786. 786.  75.]\n",
      "  [787. 787.  76.]\n",
      "  [788. 788.  77.]\n",
      "  [789. 789.  78.]\n",
      "  [790. 790.  79.]\n",
      "  [791. 791.  80.]\n",
      "  [792. 792.  81.]\n",
      "  [793. 793.  82.]\n",
      "  [794. 794.  83.]\n",
      "  [795. 795.  84.]\n",
      "  [796. 796.  84.]\n",
      "  [797. 797.  85.]\n",
      "  [798. 798.  85.]\n",
      "  [799. 800.  86.]\n",
      "  [800. 801.  86.]\n",
      "  [801. 802.  87.]\n",
      "  [802. 803.  87.]\n",
      "  [804. 804.  88.]\n",
      "  [805. 805.  89.]\n",
      "  [806. 806.  89.]\n",
      "  [807. 808.  90.]\n",
      "  [808. 809.  90.]\n",
      "  [809. 810.  91.]\n",
      "  [810. 811.  91.]\n",
      "  [811. 812.  92.]]\n",
      "\n",
      " [[501. 711.  -0.]\n",
      "  [502. 712.   1.]\n",
      "  [503. 713.   2.]\n",
      "  [504. 714.   3.]\n",
      "  [505. 715.   4.]\n",
      "  [506. 716.   5.]\n",
      "  [507. 717.   6.]\n",
      "  [508. 718.   7.]\n",
      "  [509. 719.   8.]\n",
      "  [510. 720.   9.]\n",
      "  [511. 721.  10.]\n",
      "  [512. 722.  11.]\n",
      "  [513. 723.  12.]\n",
      "  [514. 724.  13.]\n",
      "  [515. 725.  14.]\n",
      "  [516. 726.  15.]\n",
      "  [517. 727.  16.]\n",
      "  [518. 728.  17.]\n",
      "  [519. 729.  18.]\n",
      "  [520. 730.  19.]\n",
      "  [521. 731.  20.]\n",
      "  [522. 732.  21.]\n",
      "  [523. 733.  22.]\n",
      "  [524. 734.  23.]\n",
      "  [525. 735.  24.]\n",
      "  [526. 736.  25.]\n",
      "  [527. 737.  26.]\n",
      "  [528. 738.  27.]\n",
      "  [529. 739.  28.]\n",
      "  [530. 740.  29.]\n",
      "  [531. 741.  30.]\n",
      "  [532. 742.  31.]\n",
      "  [533. 743.  32.]\n",
      "  [534. 744.  33.]\n",
      "  [535. 745.  34.]\n",
      "  [536. 746.  35.]\n",
      "  [537. 747.  36.]\n",
      "  [538. 748.  37.]\n",
      "  [539. 749.  38.]\n",
      "  [540. 750.  39.]\n",
      "  [541. 751.  40.]\n",
      "  [542. 752.  41.]\n",
      "  [543. 753.  42.]\n",
      "  [544. 754.  43.]\n",
      "  [545. 755.  44.]\n",
      "  [546. 756.  45.]\n",
      "  [547. 757.  46.]\n",
      "  [548. 758.  47.]\n",
      "  [549. 759.  48.]\n",
      "  [550. 760.  49.]\n",
      "  [551. 761.  50.]\n",
      "  [552. 762.  51.]\n",
      "  [553. 763.  52.]\n",
      "  [554. 764.  53.]\n",
      "  [555. 765.  54.]\n",
      "  [556. 766.  55.]\n",
      "  [557. 767.  56.]\n",
      "  [558. 768.  57.]\n",
      "  [559. 769.  58.]\n",
      "  [560. 770.  59.]\n",
      "  [561. 771.  60.]\n",
      "  [562. 772.  61.]\n",
      "  [563. 773.  62.]\n",
      "  [564. 774.  63.]\n",
      "  [565. 775.  64.]\n",
      "  [566. 776.  65.]\n",
      "  [567. 777.  66.]\n",
      "  [568. 778.  67.]\n",
      "  [569. 779.  68.]\n",
      "  [570. 780.  69.]\n",
      "  [571. 781.  70.]\n",
      "  [572. 782.  71.]\n",
      "  [573. 783.  72.]\n",
      "  [574. 784.  73.]\n",
      "  [575. 785.  74.]\n",
      "  [576. 786.  75.]\n",
      "  [577. 787.  76.]\n",
      "  [578. 788.  77.]\n",
      "  [579. 789.  78.]\n",
      "  [580. 790.  79.]\n",
      "  [581. 791.  80.]\n",
      "  [582. 792.  81.]\n",
      "  [583. 793.  82.]\n",
      "  [584. 794.  83.]\n",
      "  [585. 795.  84.]\n",
      "  [586. 797.  84.]\n",
      "  [588. 798.  84.]\n",
      "  [589. 799.  85.]\n",
      "  [590. 801.  85.]\n",
      "  [591. 802.  86.]\n",
      "  [592. 803.  86.]\n",
      "  [593. 804.  87.]\n",
      "  [595. 806.  87.]\n",
      "  [596. 807.  88.]\n",
      "  [597. 808.  88.]\n",
      "  [598. 810.  89.]\n",
      "  [599. 811.  89.]\n",
      "  [600. 812.  90.]\n",
      "  [601. 814.  90.]\n",
      "  [603. 815.  91.]]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict([x1,x2])\n",
    "print('y_predict 값은', np.round(y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc : 0.8683333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"acc :\",accuracy_score(np.array([y1,y2]).reshape(-1), np.round(y_predict).reshape(-1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 7.6616e-06 - a_loss: 5.2151e-06 - c1_loss: 2.4465e-06 - a_mse: 5.2151e-06 - c1_mse: 2.4465e-06 - val_loss: 15.5112 - val_a_loss: 5.6697 - val_c1_loss: 9.8415 - val_a_mse: 5.6697 - val_c1_mse: 9.8415\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.4646e-06 - a_loss: 2.4006e-06 - c1_loss: 1.0640e-06 - a_mse: 2.4006e-06 - c1_mse: 1.0640e-06 - val_loss: 15.5239 - val_a_loss: 5.6770 - val_c1_loss: 9.8469 - val_a_mse: 5.6770 - val_c1_mse: 9.8469\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.2629e-05 - a_loss: 8.2971e-06 - c1_loss: 4.3322e-06 - a_mse: 8.2971e-06 - c1_mse: 4.3322e-06 - val_loss: 15.5254 - val_a_loss: 5.6781 - val_c1_loss: 9.8472 - val_a_mse: 5.6781 - val_c1_mse: 9.8472\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 6.5340e-06 - a_loss: 4.1713e-06 - c1_loss: 2.3627e-06 - a_mse: 4.1713e-06 - c1_mse: 2.3627e-06 - val_loss: 15.4932 - val_a_loss: 5.6592 - val_c1_loss: 9.8340 - val_a_mse: 5.6592 - val_c1_mse: 9.8340\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.0341e-06 - a_loss: 2.0337e-06 - c1_loss: 1.0003e-06 - a_mse: 2.0337e-06 - c1_mse: 1.0003e-06 - val_loss: 15.5191 - val_a_loss: 5.6737 - val_c1_loss: 9.8454 - val_a_mse: 5.6737 - val_c1_mse: 9.8454\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6353e-05 - a_loss: 1.0852e-05 - c1_loss: 5.5009e-06 - a_mse: 1.0852e-05 - c1_mse: 5.5009e-06 - val_loss: 15.4996 - val_a_loss: 5.6630 - val_c1_loss: 9.8366 - val_a_mse: 5.6630 - val_c1_mse: 9.8366\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 5.0955e-06 - a_loss: 3.0842e-06 - c1_loss: 2.0113e-06 - a_mse: 3.0842e-06 - c1_mse: 2.0113e-06 - val_loss: 15.5005 - val_a_loss: 5.6622 - val_c1_loss: 9.8383 - val_a_mse: 5.6622 - val_c1_mse: 9.8383\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8503e-05 - a_loss: 1.1418e-05 - c1_loss: 7.0855e-06 - a_mse: 1.1418e-05 - c1_mse: 7.0855e-06 - val_loss: 15.4881 - val_a_loss: 5.6586 - val_c1_loss: 9.8295 - val_a_mse: 5.6586 - val_c1_mse: 9.8295\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.9941e-04 - a_loss: 1.1782e-04 - c1_loss: 8.1594e-05 - a_mse: 1.1782e-04 - c1_mse: 8.1594e-05 - val_loss: 15.4553 - val_a_loss: 5.6998 - val_c1_loss: 9.7555 - val_a_mse: 5.6998 - val_c1_mse: 9.7555\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0693 - a_loss: 0.0420 - c1_loss: 0.0273 - a_mse: 0.0420 - c1_mse: 0.0273 - val_loss: 17.8983 - val_a_loss: 6.2311 - val_c1_loss: 11.6672 - val_a_mse: 6.2311 - val_c1_mse: 11.6672\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 77.7157 - a_loss: 47.2017 - c1_loss: 30.5141 - a_mse: 47.2017 - c1_mse: 30.5141 - val_loss: 43.7484 - val_a_loss: 19.5817 - val_c1_loss: 24.1666 - val_a_mse: 19.5817 - val_c1_mse: 24.1666\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 203.1934 - a_loss: 116.2054 - c1_loss: 86.9880 - a_mse: 116.2054 - c1_mse: 86.9880 - val_loss: 82.6683 - val_a_loss: 37.1570 - val_c1_loss: 45.5113 - val_a_mse: 37.1570 - val_c1_mse: 45.5113\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 6.6615 - a_loss: 3.9462 - c1_loss: 2.7153 - a_mse: 3.9462 - c1_mse: 2.7153 - val_loss: 28.8971 - val_a_loss: 12.6732 - val_c1_loss: 16.2239 - val_a_mse: 12.6732 - val_c1_mse: 16.2239\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5325 - a_loss: 0.3206 - c1_loss: 0.2120 - a_mse: 0.3206 - c1_mse: 0.2120 - val_loss: 30.4674 - val_a_loss: 12.0791 - val_c1_loss: 18.3884 - val_a_mse: 12.0791 - val_c1_mse: 18.3884\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0235 - a_loss: 0.0130 - c1_loss: 0.0106 - a_mse: 0.0130 - c1_mse: 0.0106 - val_loss: 30.8559 - val_a_loss: 11.7766 - val_c1_loss: 19.0793 - val_a_mse: 11.7766 - val_c1_mse: 19.0793\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0038 - a_loss: 0.0019 - c1_loss: 0.0019 - a_mse: 0.0019 - c1_mse: 0.0019 - val_loss: 30.5739 - val_a_loss: 11.8025 - val_c1_loss: 18.7714 - val_a_mse: 11.8025 - val_c1_mse: 18.7714\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 9.4158e-04 - a_loss: 4.0214e-04 - c1_loss: 5.3944e-04 - a_mse: 4.0214e-04 - c1_mse: 5.3944e-04 - val_loss: 29.9465 - val_a_loss: 11.4824 - val_c1_loss: 18.4641 - val_a_mse: 11.4824 - val_c1_mse: 18.4641\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 6.6532e-04 - a_loss: 3.1941e-04 - c1_loss: 3.4591e-04 - a_mse: 3.1941e-04 - c1_mse: 3.4591e-04 - val_loss: 29.9356 - val_a_loss: 11.5658 - val_c1_loss: 18.3698 - val_a_mse: 11.5658 - val_c1_mse: 18.3698\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.4033e-04 - a_loss: 1.2991e-04 - c1_loss: 2.1042e-04 - a_mse: 1.2991e-04 - c1_mse: 2.1042e-04 - val_loss: 29.7630 - val_a_loss: 11.5002 - val_c1_loss: 18.2627 - val_a_mse: 11.5002 - val_c1_mse: 18.2627\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 8.2962e-05 - a_loss: 3.1882e-05 - c1_loss: 5.1080e-05 - a_mse: 3.1882e-05 - c1_mse: 5.1080e-05 - val_loss: 29.7149 - val_a_loss: 11.5367 - val_c1_loss: 18.1783 - val_a_mse: 11.5367 - val_c1_mse: 18.1783\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.4643e-04 - a_loss: 6.6102e-05 - c1_loss: 8.0332e-05 - a_mse: 6.6102e-05 - c1_mse: 8.0332e-05 - val_loss: 29.6575 - val_a_loss: 11.4987 - val_c1_loss: 18.1587 - val_a_mse: 11.4987 - val_c1_mse: 18.1587\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.4823e-05 - a_loss: 1.5107e-05 - c1_loss: 1.9717e-05 - a_mse: 1.5107e-05 - c1_mse: 1.9717e-05 - val_loss: 29.5577 - val_a_loss: 11.4385 - val_c1_loss: 18.1192 - val_a_mse: 11.4385 - val_c1_mse: 18.1192\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.4711e-05 - a_loss: 7.3866e-06 - c1_loss: 7.3242e-06 - a_mse: 7.3866e-06 - c1_mse: 7.3242e-06 - val_loss: 29.4866 - val_a_loss: 11.4075 - val_c1_loss: 18.0791 - val_a_mse: 11.4075 - val_c1_mse: 18.0791\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 5.7222e-06 - a_loss: 2.6996e-06 - c1_loss: 3.0225e-06 - a_mse: 2.6996e-06 - c1_mse: 3.0225e-06 - val_loss: 29.4560 - val_a_loss: 11.3998 - val_c1_loss: 18.0562 - val_a_mse: 11.3998 - val_c1_mse: 18.0562\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.7372e-06 - a_loss: 1.6706e-06 - c1_loss: 2.0667e-06 - a_mse: 1.6706e-06 - c1_mse: 2.0667e-06 - val_loss: 29.4415 - val_a_loss: 11.3920 - val_c1_loss: 18.0494 - val_a_mse: 11.3920 - val_c1_mse: 18.0494\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 7.8029e-07 - a_loss: 3.6536e-07 - c1_loss: 4.1493e-07 - a_mse: 3.6536e-07 - c1_mse: 4.1493e-07 - val_loss: 29.4367 - val_a_loss: 11.3891 - val_c1_loss: 18.0476 - val_a_mse: 11.3891 - val_c1_mse: 18.0476\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 4.3837e-07 - a_loss: 2.2787e-07 - c1_loss: 2.1051e-07 - a_mse: 2.2787e-07 - c1_mse: 2.1051e-07 - val_loss: 29.4313 - val_a_loss: 11.3873 - val_c1_loss: 18.0440 - val_a_mse: 11.3873 - val_c1_mse: 18.0440\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.2872e-06 - a_loss: 1.3637e-06 - c1_loss: 9.2358e-07 - a_mse: 1.3637e-06 - c1_mse: 9.2358e-07 - val_loss: 29.4191 - val_a_loss: 11.3843 - val_c1_loss: 18.0348 - val_a_mse: 11.3843 - val_c1_mse: 18.0348\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 7.8392e-06 - a_loss: 4.7082e-06 - c1_loss: 3.1310e-06 - a_mse: 4.7082e-06 - c1_mse: 3.1310e-06 - val_loss: 29.4405 - val_a_loss: 11.3875 - val_c1_loss: 18.0530 - val_a_mse: 11.3875 - val_c1_mse: 18.0530\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.5081e-04 - a_loss: 8.9823e-05 - c1_loss: 6.0989e-05 - a_mse: 8.9823e-05 - c1_mse: 6.0989e-05 - val_loss: 29.5314 - val_a_loss: 11.4135 - val_c1_loss: 18.1179 - val_a_mse: 11.4135 - val_c1_mse: 18.1179\n",
      "Epoch 31/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0058 - a_loss: 0.0035 - c1_loss: 0.0023 - a_mse: 0.0035 - c1_mse: 0.0023 - val_loss: 28.9731 - val_a_loss: 11.2717 - val_c1_loss: 17.7013 - val_a_mse: 11.2717 - val_c1_mse: 17.7013\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 11.9840 - a_loss: 7.0965 - c1_loss: 4.8875 - a_mse: 7.0965 - c1_mse: 4.8875 - val_loss: 100.0341 - val_a_loss: 44.9092 - val_c1_loss: 55.1248 - val_a_mse: 44.9092 - val_c1_mse: 55.1248\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 14.4709 - a_loss: 8.3059 - c1_loss: 6.1650 - a_mse: 8.3059 - c1_mse: 6.1650 - val_loss: 29.1975 - val_a_loss: 13.6001 - val_c1_loss: 15.5974 - val_a_mse: 13.6001 - val_c1_mse: 15.5974\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 7.7101 - a_loss: 4.3673 - c1_loss: 3.3427 - a_mse: 4.3673 - c1_mse: 3.3427 - val_loss: 36.5783 - val_a_loss: 20.4414 - val_c1_loss: 16.1369 - val_a_mse: 20.4414 - val_c1_mse: 16.1369\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6240 - a_loss: 0.3196 - c1_loss: 0.3044 - a_mse: 0.3196 - c1_mse: 0.3044 - val_loss: 27.6165 - val_a_loss: 10.5190 - val_c1_loss: 17.0975 - val_a_mse: 10.5190 - val_c1_mse: 17.0975\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0062 - a_loss: 0.0032 - c1_loss: 0.0030 - a_mse: 0.0032 - c1_mse: 0.0030 - val_loss: 27.7502 - val_a_loss: 10.5990 - val_c1_loss: 17.1512 - val_a_mse: 10.5990 - val_c1_mse: 17.1512\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0068 - a_loss: 0.0038 - c1_loss: 0.0030 - a_mse: 0.0038 - c1_mse: 0.0030 - val_loss: 27.5433 - val_a_loss: 10.5905 - val_c1_loss: 16.9529 - val_a_mse: 10.5905 - val_c1_mse: 16.9529\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.1241e-04 - a_loss: 1.1772e-04 - c1_loss: 9.4689e-05 - a_mse: 1.1772e-04 - c1_mse: 9.4689e-05 - val_loss: 27.5931 - val_a_loss: 10.6124 - val_c1_loss: 16.9806 - val_a_mse: 10.6124 - val_c1_mse: 16.9806\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.1534e-04 - a_loss: 1.6822e-04 - c1_loss: 1.4712e-04 - a_mse: 1.6822e-04 - c1_mse: 1.4712e-04 - val_loss: 27.6825 - val_a_loss: 10.6507 - val_c1_loss: 17.0319 - val_a_mse: 10.6507 - val_c1_mse: 17.0319\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 9.1221e-04 - a_loss: 4.9540e-04 - c1_loss: 4.1681e-04 - a_mse: 4.9540e-04 - c1_mse: 4.1681e-04 - val_loss: 27.6239 - val_a_loss: 10.6161 - val_c1_loss: 17.0078 - val_a_mse: 10.6161 - val_c1_mse: 17.0078\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0053 - a_loss: 0.0030 - c1_loss: 0.0023 - a_mse: 0.0030 - c1_mse: 0.0023 - val_loss: 26.4738 - val_a_loss: 10.3451 - val_c1_loss: 16.1286 - val_a_mse: 10.3451 - val_c1_mse: 16.1286\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.3862 - a_loss: 0.7720 - c1_loss: 0.6142 - a_mse: 0.7720 - c1_mse: 0.6142 - val_loss: 27.2502 - val_a_loss: 11.1399 - val_c1_loss: 16.1103 - val_a_mse: 11.1399 - val_c1_mse: 16.1103\n",
      "Epoch 43/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 102.7144 - a_loss: 54.5551 - c1_loss: 48.1592 - a_mse: 54.5551 - c1_mse: 48.1592 - val_loss: 31.0162 - val_a_loss: 15.5332 - val_c1_loss: 15.4829 - val_a_mse: 15.5332 - val_c1_mse: 15.4829\n",
      "Epoch 44/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 4.3203 - a_loss: 2.7625 - c1_loss: 1.5578 - a_mse: 2.7625 - c1_mse: 1.5578 - val_loss: 33.1307 - val_a_loss: 13.5210 - val_c1_loss: 19.6097 - val_a_mse: 13.5210 - val_c1_mse: 19.6097\n",
      "Epoch 45/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.9803 - a_loss: 0.5950 - c1_loss: 0.3853 - a_mse: 0.5950 - c1_mse: 0.3853 - val_loss: 38.2474 - val_a_loss: 14.8842 - val_c1_loss: 23.3633 - val_a_mse: 14.8842 - val_c1_mse: 23.3633\n",
      "Epoch 46/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2541 - a_loss: 0.1396 - c1_loss: 0.1145 - a_mse: 0.1396 - c1_mse: 0.1145 - val_loss: 32.3806 - val_a_loss: 14.5762 - val_c1_loss: 17.8044 - val_a_mse: 14.5762 - val_c1_mse: 17.8044\n",
      "Epoch 47/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2314 - a_loss: 0.1324 - c1_loss: 0.0989 - a_mse: 0.1324 - c1_mse: 0.0989 - val_loss: 30.9571 - val_a_loss: 11.8036 - val_c1_loss: 19.1535 - val_a_mse: 11.8036 - val_c1_mse: 19.1535\n",
      "Epoch 48/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0189 - a_loss: 0.0108 - c1_loss: 0.0081 - a_mse: 0.0108 - c1_mse: 0.0081 - val_loss: 30.1332 - val_a_loss: 11.7655 - val_c1_loss: 18.3676 - val_a_mse: 11.7655 - val_c1_mse: 18.3676\n",
      "Epoch 49/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0105 - a_loss: 0.0069 - c1_loss: 0.0036 - a_mse: 0.0069 - c1_mse: 0.0036 - val_loss: 30.5204 - val_a_loss: 11.8798 - val_c1_loss: 18.6406 - val_a_mse: 11.8798 - val_c1_mse: 18.6406\n",
      "Epoch 50/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0331 - a_loss: 0.0216 - c1_loss: 0.0114 - a_mse: 0.0216 - c1_mse: 0.0114 - val_loss: 30.5056 - val_a_loss: 11.8609 - val_c1_loss: 18.6446 - val_a_mse: 11.8609 - val_c1_mse: 18.6446\n",
      "Epoch 51/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2254 - a_loss: 0.1395 - c1_loss: 0.0859 - a_mse: 0.1395 - c1_mse: 0.0859 - val_loss: 30.3912 - val_a_loss: 10.3872 - val_c1_loss: 20.0040 - val_a_mse: 10.3872 - val_c1_mse: 20.0040\n",
      "Epoch 52/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6927 - a_loss: 0.4121 - c1_loss: 0.2805 - a_mse: 0.4121 - c1_mse: 0.2805 - val_loss: 30.2901 - val_a_loss: 12.1847 - val_c1_loss: 18.1053 - val_a_mse: 12.1847 - val_c1_mse: 18.1053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2ca7b158108>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=25, mode='auto')\n",
    "model.fit([x1,x2], [y1,y2], epochs=150, batch_size=1,validation_split = 0.2,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step\n",
      "loss : [6.149995670467615, 2.494446277618408, 3.6555495262145996, 2.494446277618408, 3.6555495262145996]\n",
      "acc : 0.01\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate([x1,x2], [y1,y2], batch_size=1)\n",
    "y_predict = model.predict([x1,x2])\n",
    "# print('y_predict 값은\\n', np.round(y_predict))\n",
    "print(\"loss :\", loss)\n",
    "# print(\"mse :\", mse)\n",
    "# print(\"model acc :\", acc)\n",
    "print(\"acc :\",accuracy_score(y.reshape(-1), np.round(y_predict).reshape(-1)) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(흠... 이로써 early_stopping의 역할은 model.save가 전부인가...ㅠ\n",
    "이거 자체로 도구로 사용하기에는 너무 부작용이 크다고 느껴진다..)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " 맞춘다.. 그러나 위에서 했듯이 합쳐서 해도 acc 1.0나오는데 왜 굳이 쪼개서 하는지 이해되지 않는다.\n",
    "(더 성능이 좋은 줄 알고 삼성 주가 맞추기 게임에서도 앙상블을 썼지만.. 그닥..)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " y가 더 많은 기괴한 데이터.. 이것도 먼저 합친다음에 돌려보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([range(1,101), range(301,401)]).T\n",
    "y = np.array([range(711,811), range(611, 711),range(101,201), range(411, 511)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1, 301],\n",
       "       [  2, 302],\n",
       "       [  3, 303],\n",
       "       [  4, 304],\n",
       "       [  5, 305],\n",
       "       [  6, 306],\n",
       "       [  7, 307],\n",
       "       [  8, 308],\n",
       "       [  9, 309],\n",
       "       [ 10, 310]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[711, 611, 101, 411],\n",
       "       [712, 612, 102, 412],\n",
       "       [713, 613, 103, 413],\n",
       "       [714, 614, 104, 414],\n",
       "       [715, 615, 105, 415],\n",
       "       [716, 616, 106, 416],\n",
       "       [717, 617, 107, 417],\n",
       "       [718, 618, 108, 418],\n",
       "       [719, 619, 109, 419],\n",
       "       [720, 620, 110, 420]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 35935.1903 - mse: 35935.1836 - acc: 0.8625 - val_loss: 130.0038 - val_mse: 130.0038 - val_acc: 1.0000\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 11.5603 - mse: 11.5603 - acc: 1.0000 - val_loss: 0.6644 - val_mse: 0.6644 - val_acc: 1.0000\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0783 - mse: 0.0783 - acc: 1.0000 - val_loss: 0.0137 - val_mse: 0.0137 - val_acc: 1.0000\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.0511e-04 - mse: 7.0511e-04 - acc: 1.0000 - val_loss: 5.7641e-05 - val_mse: 5.7641e-05 - val_acc: 1.0000\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.5846e-06 - mse: 2.5846e-06 - acc: 1.0000 - val_loss: 1.2151e-07 - val_mse: 1.2151e-07 - val_acc: 1.0000\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.1391e-08 - mse: 2.1391e-08 - acc: 1.0000 - val_loss: 3.8277e-08 - val_mse: 3.8277e-08 - val_acc: 1.0000\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0052e-08 - mse: 1.0052e-08 - acc: 1.0000 - val_loss: 1.2305e-08 - val_mse: 1.2305e-08 - val_acc: 1.0000\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.6342e-09 - mse: 6.6342e-09 - acc: 1.0000 - val_loss: 6.8627e-09 - val_mse: 6.8627e-09 - val_acc: 1.0000\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3367e-09 - mse: 4.3367e-09 - acc: 1.0000 - val_loss: 1.4048e-08 - val_mse: 1.4048e-08 - val_acc: 1.0000\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.8449e-09 - mse: 6.8449e-09 - acc: 1.0000 - val_loss: 3.4779e-09 - val_mse: 3.4779e-09 - val_acc: 1.0000\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.2209e-09 - mse: 6.2209e-09 - acc: 1.0000 - val_loss: 2.8085e-09 - val_mse: 2.8085e-09 - val_acc: 1.0000\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.5608e-09 - mse: 7.5608e-09 - acc: 1.0000 - val_loss: 1.5716e-08 - val_mse: 1.5716e-08 - val_acc: 1.0000\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.3954e-09 - mse: 8.3954e-09 - acc: 1.0000 - val_loss: 9.7993e-09 - val_mse: 9.7993e-09 - val_acc: 1.0000\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8953e-09 - mse: 4.8953e-09 - acc: 1.0000 - val_loss: 3.9872e-09 - val_mse: 3.9872e-09 - val_acc: 1.0000\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0937e-08 - mse: 3.0937e-08 - acc: 1.0000 - val_loss: 1.4779e-08 - val_mse: 1.4779e-08 - val_acc: 1.0000\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7207e-08 - mse: 1.7207e-08 - acc: 1.0000 - val_loss: 2.8632e-08 - val_mse: 2.8632e-08 - val_acc: 1.0000\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5786e-08 - mse: 1.5786e-08 - acc: 1.0000 - val_loss: 4.7294e-09 - val_mse: 4.7294e-09 - val_acc: 1.0000\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8280e-08 - mse: 2.8280e-08 - acc: 1.0000 - val_loss: 3.7739e-08 - val_mse: 3.7739e-08 - val_acc: 1.0000\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0436e-06 - mse: 1.0436e-06 - acc: 1.0000 - val_loss: 2.2804e-06 - val_mse: 2.2804e-06 - val_acc: 1.0000\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0029 - mse: 0.0029 - acc: 1.0000 - val_loss: 0.0045 - val_mse: 0.0045 - val_acc: 1.0000\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.1548 - mse: 0.1548 - acc: 1.0000 - val_loss: 0.1429 - val_mse: 0.1429 - val_acc: 1.0000\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 28.8288 - mse: 28.8288 - acc: 1.0000 - val_loss: 401.6038 - val_mse: 401.6038 - val_acc: 1.0000\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2400.7811 - mse: 2400.7810 - acc: 1.0000 - val_loss: 484.1190 - val_mse: 484.1190 - val_acc: 1.0000\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2542.2155 - mse: 2542.2161 - acc: 1.0000 - val_loss: 21.6802 - val_mse: 21.6802 - val_acc: 1.0000\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 81.9840 - mse: 81.9840 - acc: 1.0000 - val_loss: 43.7093 - val_mse: 43.7093 - val_acc: 1.0000\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4726 - mse: 1.4726 - acc: 1.0000 - val_loss: 0.0894 - val_mse: 0.0894 - val_acc: 1.0000\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0039 - mse: 0.0039 - acc: 1.0000 - val_loss: 1.4917e-04 - val_mse: 1.4917e-04 - val_acc: 1.0000\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5735e-05 - mse: 1.5735e-05 - acc: 1.0000 - val_loss: 2.9908e-07 - val_mse: 2.9908e-07 - val_acc: 1.0000\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7343e-08 - mse: 3.7343e-08 - acc: 1.0000 - val_loss: 6.5978e-09 - val_mse: 6.5978e-09 - val_acc: 1.0000\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.0444e-09 - mse: 7.0444e-09 - acc: 1.0000 - val_loss: 5.5064e-09 - val_mse: 5.5064e-09 - val_acc: 1.0000\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.1476e-09 - mse: 6.1476e-09 - acc: 1.0000 - val_loss: 5.3027e-09 - val_mse: 5.3027e-09 - val_acc: 1.0000\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.5807e-09 - mse: 6.5807e-09 - acc: 1.0000 - val_loss: 3.6380e-09 - val_mse: 3.6380e-09 - val_acc: 1.0000\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.9496e-09 - mse: 5.9496e-09 - acc: 1.0000 - val_loss: 2.6892e-08 - val_mse: 2.6892e-08 - val_acc: 1.0000\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 9.7123e-09 - mse: 9.7123e-09 - acc: 1.0000 - val_loss: 2.9802e-09 - val_mse: 2.9802e-09 - val_acc: 1.0000\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.2817e-09 - mse: 6.2817e-09 - acc: 1.0000 - val_loss: 4.5839e-09 - val_mse: 4.5839e-09 - val_acc: 1.0000\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.3167e-09 - mse: 7.3167e-09 - acc: 1.0000 - val_loss: 2.4156e-09 - val_mse: 2.4156e-09 - val_acc: 1.0000\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.4097e-09 - mse: 5.4097e-09 - acc: 1.0000 - val_loss: 1.4913e-08 - val_mse: 1.4913e-08 - val_acc: 1.0000\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.4379e-09 - mse: 6.4379e-09 - acc: 1.0000 - val_loss: 3.7049e-09 - val_mse: 3.7049e-09 - val_acc: 1.0000\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5155e-08 - mse: 1.5155e-08 - acc: 1.0000 - val_loss: 2.6388e-08 - val_mse: 2.6388e-08 - val_acc: 1.0000\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5052e-08 - mse: 1.5052e-08 - acc: 1.0000 - val_loss: 4.9156e-09 - val_mse: 4.9156e-09 - val_acc: 1.0000\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0065e-08 - mse: 1.0065e-08 - acc: 1.0000 - val_loss: 4.0859e-08 - val_mse: 4.0859e-08 - val_acc: 1.0000\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8985e-06 - mse: 2.8985e-06 - acc: 1.0000 - val_loss: 9.8133e-05 - val_mse: 9.8133e-05 - val_acc: 1.0000\n",
      "Epoch 43/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1747e-05 - mse: 1.1747e-05 - acc: 1.0000 - val_loss: 1.5413e-07 - val_mse: 1.5413e-07 - val_acc: 1.0000\n",
      "Epoch 44/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.5531e-08 - mse: 6.5531e-08 - acc: 1.0000 - val_loss: 1.7200e-08 - val_mse: 1.7200e-08 - val_acc: 1.0000\n",
      "Epoch 45/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.2220e-09 - mse: 6.2220e-09 - acc: 1.0000 - val_loss: 7.0839e-09 - val_mse: 7.0839e-09 - val_acc: 1.0000\n",
      "Epoch 46/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0234e-08 - mse: 1.0234e-08 - acc: 1.0000 - val_loss: 1.7075e-08 - val_mse: 1.7075e-08 - val_acc: 1.0000\n",
      "Epoch 47/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0874e-08 - mse: 1.0874e-08 - acc: 1.0000 - val_loss: 2.9197e-08 - val_mse: 2.9197e-08 - val_acc: 1.0000\n",
      "Epoch 48/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.0458e-08 - mse: 7.0458e-08 - acc: 1.0000 - val_loss: 5.4372e-08 - val_mse: 5.4372e-08 - val_acc: 1.0000\n",
      "Epoch 49/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.0136e-06 - mse: 6.0136e-06 - acc: 1.0000 - val_loss: 1.0263e-05 - val_mse: 1.0263e-05 - val_acc: 1.0000\n",
      "Epoch 50/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6707e-04 - mse: 1.6707e-04 - acc: 1.0000 - val_loss: 7.0660e-07 - val_mse: 7.0660e-07 - val_acc: 1.0000\n",
      "Epoch 51/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 0.0017 - acc: 1.0000 - val_loss: 0.0109 - val_mse: 0.0109 - val_acc: 1.0000\n",
      "Epoch 52/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2459 - mse: 1.2459 - acc: 1.0000 - val_loss: 5.4289 - val_mse: 5.4289 - val_acc: 1.0000\n",
      "Epoch 53/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 546.1739 - mse: 546.1741 - acc: 1.0000 - val_loss: 3.3905 - val_mse: 3.3905 - val_acc: 1.0000\n",
      "Epoch 54/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 61.7854 - mse: 61.7854 - acc: 1.0000 - val_loss: 90.4085 - val_mse: 90.4085 - val_acc: 1.0000\n",
      "Epoch 55/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 191.5462 - mse: 191.5462 - acc: 1.0000 - val_loss: 706.8068 - val_mse: 706.8068 - val_acc: 1.0000\n",
      "Epoch 56/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 649.0909 - mse: 649.0908 - acc: 1.0000 - val_loss: 4575.2597 - val_mse: 4575.2598 - val_acc: 1.0000\n",
      "Epoch 57/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 550.1577 - mse: 550.1575 - acc: 1.0000 - val_loss: 147.9426 - val_mse: 147.9426 - val_acc: 1.0000\n",
      "Epoch 58/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.9090 - mse: 8.9090 - acc: 1.0000 - val_loss: 0.0703 - val_mse: 0.0703 - val_acc: 1.0000\n",
      "Epoch 59/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0088 - mse: 0.0088 - acc: 1.0000 - val_loss: 0.0015 - val_mse: 0.0015 - val_acc: 1.0000\n",
      "Epoch 60/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0024 - mse: 0.0024 - acc: 1.0000 - val_loss: 6.6791e-05 - val_mse: 6.6791e-05 - val_acc: 1.0000\n",
      "Epoch 61/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.9752e-05 - mse: 1.9752e-05 - acc: 1.0000 - val_loss: 1.7623e-05 - val_mse: 1.7623e-05 - val_acc: 1.0000\n",
      "Epoch 62/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5944e-05 - mse: 1.5944e-05 - acc: 1.0000 - val_loss: 1.8898e-07 - val_mse: 1.8898e-07 - val_acc: 1.0000\n",
      "Epoch 63/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7782e-06 - mse: 1.7782e-06 - acc: 1.0000 - val_loss: 1.5785e-07 - val_mse: 1.5785e-07 - val_acc: 1.0000\n",
      "Epoch 64/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8272e-06 - mse: 2.8272e-06 - acc: 1.0000 - val_loss: 2.0348e-06 - val_mse: 2.0348e-06 - val_acc: 1.0000\n",
      "Epoch 65/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1273e-05 - mse: 1.1273e-05 - acc: 1.0000 - val_loss: 7.6684e-05 - val_mse: 7.6684e-05 - val_acc: 1.0000\n",
      "Epoch 66/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.6291e-05 - mse: 8.6291e-05 - acc: 1.0000 - val_loss: 2.6447e-05 - val_mse: 2.6447e-05 - val_acc: 1.0000\n",
      "Epoch 67/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1413e-05 - mse: 1.1413e-05 - acc: 1.0000 - val_loss: 1.3865e-08 - val_mse: 1.3865e-08 - val_acc: 1.0000\n",
      "Epoch 68/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.3316e-06 - mse: 8.3316e-06 - acc: 1.0000 - val_loss: 1.1549e-06 - val_mse: 1.1549e-06 - val_acc: 1.0000\n",
      "Epoch 69/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0019 - mse: 0.0019 - acc: 1.0000 - val_loss: 4.0051e-04 - val_mse: 4.0051e-04 - val_acc: 1.0000\n",
      "Epoch 70/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0747 - mse: 0.0747 - acc: 1.0000 - val_loss: 0.1504 - val_mse: 0.1504 - val_acc: 1.0000\n",
      "Epoch 71/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.0592 - mse: 7.0592 - acc: 1.0000 - val_loss: 20.5010 - val_mse: 20.5010 - val_acc: 1.0000\n",
      "Epoch 72/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1112.8295 - mse: 1112.8296 - acc: 1.0000 - val_loss: 173.5397 - val_mse: 173.5397 - val_acc: 1.0000\n",
      "Epoch 73/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 470.3204 - mse: 470.3205 - acc: 1.0000 - val_loss: 45.6455 - val_mse: 45.6455 - val_acc: 1.0000\n",
      "Epoch 74/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 65.1659 - mse: 65.1659 - acc: 1.0000 - val_loss: 6.2325 - val_mse: 6.2325 - val_acc: 1.0000\n",
      "Epoch 75/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.5276 - mse: 7.5276 - acc: 1.0000 - val_loss: 0.3349 - val_mse: 0.3349 - val_acc: 1.0000\n",
      "Epoch 76/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.1646 - mse: 0.1646 - acc: 1.0000 - val_loss: 2.2891 - val_mse: 2.2891 - val_acc: 1.0000\n",
      "Epoch 77/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3513 - mse: 0.3513 - acc: 1.0000 - val_loss: 0.0948 - val_mse: 0.0948 - val_acc: 1.0000\n",
      "Epoch 78/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0192 - mse: 0.0192 - acc: 1.0000 - val_loss: 4.3816e-05 - val_mse: 4.3816e-05 - val_acc: 1.0000\n",
      "Epoch 79/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.4919e-04 - mse: 2.4919e-04 - acc: 1.0000 - val_loss: 6.3460e-06 - val_mse: 6.3460e-06 - val_acc: 1.0000\n",
      "Epoch 80/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 0.0014 - acc: 1.0000 - val_loss: 1.7331e-04 - val_mse: 1.7331e-04 - val_acc: 1.0000\n",
      "Epoch 81/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5884e-04 - mse: 1.5884e-04 - acc: 1.0000 - val_loss: 5.0782e-05 - val_mse: 5.0782e-05 - val_acc: 1.0000\n",
      "Epoch 82/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.8477e-05 - mse: 8.8477e-05 - acc: 1.0000 - val_loss: 3.9493e-05 - val_mse: 3.9493e-05 - val_acc: 1.0000\n",
      "Epoch 83/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8258e-05 - mse: 2.8258e-05 - acc: 1.0000 - val_loss: 5.9631e-06 - val_mse: 5.9631e-06 - val_acc: 1.0000\n",
      "Epoch 84/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.0164e-06 - mse: 6.0164e-06 - acc: 1.0000 - val_loss: 2.2477e-08 - val_mse: 2.2477e-08 - val_acc: 1.0000\n",
      "Epoch 85/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7026e-04 - mse: 1.7026e-04 - acc: 1.0000 - val_loss: 6.6548e-04 - val_mse: 6.6548e-04 - val_acc: 1.0000\n",
      "Epoch 86/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.0080 - acc: 1.0000 - val_loss: 7.8510e-04 - val_mse: 7.8510e-04 - val_acc: 1.0000\n",
      "Epoch 87/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - acc: 1.0000 - val_loss: 0.2253 - val_mse: 0.2253 - val_acc: 1.0000\n",
      "Epoch 88/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0913 - mse: 0.0913 - acc: 1.0000 - val_loss: 0.0158 - val_mse: 0.0158 - val_acc: 1.0000\n",
      "Epoch 89/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0007 - mse: 1.0007 - acc: 1.0000 - val_loss: 2.0663 - val_mse: 2.0663 - val_acc: 1.0000\n",
      "Epoch 90/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 20.7807 - mse: 20.7807 - acc: 1.0000 - val_loss: 10.4046 - val_mse: 10.4046 - val_acc: 1.0000\n",
      "Epoch 91/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 227.3049 - mse: 227.3049 - acc: 1.0000 - val_loss: 106.3704 - val_mse: 106.3704 - val_acc: 1.0000\n",
      "Epoch 92/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1294.5568 - mse: 1294.5566 - acc: 1.0000 - val_loss: 1219.2700 - val_mse: 1219.2699 - val_acc: 1.0000\n",
      "Epoch 93/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 262.4577 - mse: 262.4576 - acc: 1.0000 - val_loss: 314.0990 - val_mse: 314.0990 - val_acc: 1.0000\n",
      "Epoch 94/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 362.6986 - mse: 362.6986 - acc: 1.0000 - val_loss: 108.7139 - val_mse: 108.7139 - val_acc: 1.0000\n",
      "Epoch 95/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2811 - mse: 4.2811 - acc: 1.0000 - val_loss: 0.1840 - val_mse: 0.1840 - val_acc: 1.0000\n",
      "Epoch 96/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.1531 - mse: 0.1531 - acc: 1.0000 - val_loss: 0.0019 - val_mse: 0.0019 - val_acc: 1.0000\n",
      "Epoch 97/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.1771e-04 - mse: 2.1771e-04 - acc: 1.0000 - val_loss: 1.4852e-06 - val_mse: 1.4852e-06 - val_acc: 1.0000\n",
      "Epoch 98/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1419e-07 - mse: 4.1419e-07 - acc: 1.0000 - val_loss: 5.1875e-08 - val_mse: 5.1875e-08 - val_acc: 1.0000\n",
      "Epoch 99/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.0808e-08 - mse: 2.0808e-08 - acc: 1.0000 - val_loss: 5.1659e-09 - val_mse: 5.1659e-09 - val_acc: 1.0000\n",
      "Epoch 100/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4343e-08 - mse: 4.4343e-08 - acc: 1.0000 - val_loss: 2.5146e-09 - val_mse: 2.5146e-09 - val_acc: 1.0000\n",
      "Epoch 101/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5316e-08 - mse: 1.5316e-08 - acc: 1.0000 - val_loss: 7.4215e-09 - val_mse: 7.4215e-09 - val_acc: 1.0000\n",
      "Epoch 102/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6674e-08 - mse: 1.6674e-08 - acc: 1.0000 - val_loss: 1.7200e-07 - val_mse: 1.7200e-07 - val_acc: 1.0000\n",
      "Epoch 103/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9046e-07 - mse: 2.9046e-07 - acc: 1.0000 - val_loss: 3.6132e-06 - val_mse: 3.6132e-06 - val_acc: 1.0000\n",
      "Epoch 104/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.7642e-06 - mse: 6.7642e-06 - acc: 1.0000 - val_loss: 4.0539e-06 - val_mse: 4.0539e-06 - val_acc: 1.0000\n",
      "Epoch 105/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.6185e-06 - mse: 5.6185e-06 - acc: 1.0000 - val_loss: 1.6853e-04 - val_mse: 1.6853e-04 - val_acc: 1.0000\n",
      "Epoch 106/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 9.6353e-05 - mse: 9.6353e-05 - acc: 1.0000 - val_loss: 7.8121e-06 - val_mse: 7.8121e-06 - val_acc: 1.0000\n",
      "Epoch 107/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 9.8221e-04 - mse: 9.8221e-04 - acc: 1.0000 - val_loss: 0.0028 - val_mse: 0.0028 - val_acc: 1.0000\n",
      "Epoch 108/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0523 - mse: 0.0523 - acc: 1.0000 - val_loss: 0.1171 - val_mse: 0.1171 - val_acc: 1.0000\n",
      "Epoch 109/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 14.6786 - mse: 14.6786 - acc: 1.0000 - val_loss: 55.1758 - val_mse: 55.1758 - val_acc: 1.0000\n",
      "Epoch 110/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 131.9076 - mse: 131.9076 - acc: 1.0000 - val_loss: 151.6516 - val_mse: 151.6516 - val_acc: 1.0000\n",
      "Epoch 111/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 94.1458 - mse: 94.1458 - acc: 1.0000 - val_loss: 7.8735 - val_mse: 7.8735 - val_acc: 1.0000\n",
      "Epoch 112/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 126.8752 - mse: 126.8752 - acc: 1.0000 - val_loss: 386.7615 - val_mse: 386.7615 - val_acc: 1.0000\n",
      "Epoch 113/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 394.6767 - mse: 394.6767 - acc: 1.0000 - val_loss: 31.9510 - val_mse: 31.9510 - val_acc: 1.0000\n",
      "Epoch 114/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 167.5087 - mse: 167.5088 - acc: 1.0000 - val_loss: 222.4132 - val_mse: 222.4132 - val_acc: 1.0000\n",
      "Epoch 115/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 80.0298 - mse: 80.0298 - acc: 1.0000 - val_loss: 22.2932 - val_mse: 22.2932 - val_acc: 1.0000\n",
      "Epoch 116/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.0716 - mse: 2.0716 - acc: 1.0000 - val_loss: 0.0444 - val_mse: 0.0444 - val_acc: 1.0000\n",
      "Epoch 117/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.1299 - mse: 0.1299 - acc: 1.0000 - val_loss: 0.0247 - val_mse: 0.0247 - val_acc: 1.0000\n",
      "Epoch 118/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0024 - mse: 0.0024 - acc: 1.0000 - val_loss: 2.1300e-05 - val_mse: 2.1300e-05 - val_acc: 1.0000\n",
      "Epoch 119/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0866e-04 - mse: 1.0866e-04 - acc: 1.0000 - val_loss: 2.3599e-04 - val_mse: 2.3599e-04 - val_acc: 1.0000\n",
      "Epoch 120/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.0629e-04 - mse: 7.0629e-04 - acc: 1.0000 - val_loss: 9.7109e-05 - val_mse: 9.7109e-05 - val_acc: 1.0000\n",
      "Epoch 121/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8738e-04 - mse: 4.8738e-04 - acc: 1.0000 - val_loss: 5.8035e-06 - val_mse: 5.8035e-06 - val_acc: 1.0000\n",
      "Epoch 122/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.0706e-04 - mse: 6.0706e-04 - acc: 1.0000 - val_loss: 4.0507e-05 - val_mse: 4.0507e-05 - val_acc: 1.0000\n",
      "Epoch 123/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3000e-04 - mse: 3.3000e-04 - acc: 1.0000 - val_loss: 9.2248e-05 - val_mse: 9.2248e-05 - val_acc: 1.0000\n",
      "Epoch 124/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.1030 - mse: 0.1030 - acc: 1.0000 - val_loss: 0.5466 - val_mse: 0.5466 - val_acc: 1.0000\n",
      "Epoch 125/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 36.8874 - mse: 36.8874 - acc: 1.0000 - val_loss: 546.7583 - val_mse: 546.7584 - val_acc: 1.0000\n",
      "Epoch 126/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 41913.9769 - mse: 41913.9727 - acc: 0.7875 - val_loss: 5291.7637 - val_mse: 5291.7637 - val_acc: 1.0000\n",
      "Epoch 127/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2457.5025 - mse: 2457.5017 - acc: 0.9625 - val_loss: 542.1969 - val_mse: 542.1969 - val_acc: 1.0000\n",
      "Epoch 128/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 692.2496 - mse: 692.2494 - acc: 1.0000 - val_loss: 177.9618 - val_mse: 177.9619 - val_acc: 1.0000\n",
      "Epoch 129/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 51.2913 - mse: 51.2913 - acc: 1.0000 - val_loss: 3.9899 - val_mse: 3.9899 - val_acc: 1.0000\n",
      "Epoch 130/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4298 - mse: 0.4298 - acc: 1.0000 - val_loss: 0.5471 - val_mse: 0.5471 - val_acc: 1.0000\n",
      "Epoch 131/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.0756 - mse: 0.0756 - acc: 1.0000 - val_loss: 0.0131 - val_mse: 0.0131 - val_acc: 1.0000\n",
      "Epoch 132/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.9440e-04 - mse: 8.9440e-04 - acc: 1.0000 - val_loss: 7.9004e-05 - val_mse: 7.9004e-05 - val_acc: 1.0000\n",
      "Epoch 133/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5714e-05 - mse: 3.5714e-05 - acc: 1.0000 - val_loss: 8.9750e-05 - val_mse: 8.9750e-05 - val_acc: 1.0000\n",
      "Epoch 134/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1268e-05 - mse: 1.1268e-05 - acc: 1.0000 - val_loss: 2.0367e-07 - val_mse: 2.0367e-07 - val_acc: 1.0000\n",
      "Epoch 135/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4298e-08 - mse: 1.4298e-08 - acc: 1.0000 - val_loss: 7.1392e-09 - val_mse: 7.1392e-09 - val_acc: 1.0000\n",
      "Epoch 136/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.0108e-09 - mse: 5.0108e-09 - acc: 1.0000 - val_loss: 1.2462e-08 - val_mse: 1.2462e-08 - val_acc: 1.0000\n",
      "Epoch 137/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8862e-09 - mse: 4.8862e-09 - acc: 1.0000 - val_loss: 7.3022e-09 - val_mse: 7.3022e-09 - val_acc: 1.0000\n",
      "Epoch 138/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4744e-09 - mse: 4.4744e-09 - acc: 1.0000 - val_loss: 5.8411e-09 - val_mse: 5.8411e-09 - val_acc: 1.0000\n",
      "Epoch 139/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5355e-09 - mse: 4.5355e-09 - acc: 1.0000 - val_loss: 5.2736e-09 - val_mse: 5.2736e-09 - val_acc: 1.0000\n",
      "Epoch 140/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.6867e-09 - mse: 7.6867e-09 - acc: 1.0000 - val_loss: 3.1106e-08 - val_mse: 3.1106e-08 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8405e-09 - mse: 4.8405e-09 - acc: 1.0000 - val_loss: 8.5623e-09 - val_mse: 8.5623e-09 - val_acc: 1.0000\n",
      "Epoch 142/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5401e-08 - mse: 3.5401e-08 - acc: 1.0000 - val_loss: 2.3427e-07 - val_mse: 2.3427e-07 - val_acc: 1.0000\n",
      "Epoch 143/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7861e-08 - mse: 4.7861e-08 - acc: 1.0000 - val_loss: 6.6822e-09 - val_mse: 6.6822e-09 - val_acc: 1.0000\n",
      "Epoch 144/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.3971e-09 - mse: 5.3971e-09 - acc: 1.0000 - val_loss: 4.9971e-09 - val_mse: 4.9971e-09 - val_acc: 1.0000\n",
      "Epoch 145/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.3777e-09 - mse: 8.3777e-09 - acc: 1.0000 - val_loss: 9.3438e-08 - val_mse: 9.3438e-08 - val_acc: 1.0000\n",
      "Epoch 146/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2856e-08 - mse: 3.2856e-08 - acc: 1.0000 - val_loss: 2.7666e-08 - val_mse: 2.7666e-08 - val_acc: 1.0000\n",
      "Epoch 147/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5516e-08 - mse: 1.5516e-08 - acc: 1.0000 - val_loss: 1.4299e-08 - val_mse: 1.4299e-08 - val_acc: 1.0000\n",
      "Epoch 148/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.7272e-09 - mse: 6.7272e-09 - acc: 1.0000 - val_loss: 7.5903e-09 - val_mse: 7.5903e-09 - val_acc: 1.0000\n",
      "Epoch 149/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1944e-08 - mse: 5.1944e-08 - acc: 1.0000 - val_loss: 2.9366e-08 - val_mse: 2.9366e-08 - val_acc: 1.0000\n",
      "Epoch 150/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.8998e-06 - mse: 1.8998e-06 - acc: 1.0000 - val_loss: 3.5485e-06 - val_mse: 3.5485e-06 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2ca6997f8c8>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델구성 (거의 dnn의 최강자 아닌가 이정도면..? 그나저나 model.save는 언제 나오려나..)\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=2)) \n",
    "model.add(Dense(48)) \n",
    "model.add(Dense(100)) \n",
    "model.add(Dense(848))\n",
    "model.add(Dense(1048))\n",
    "model.add(Dense(100)) \n",
    "model.add(Dense(5))\n",
    "model.add(Dense(4)) \n",
    "# 훈련\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['mse','acc'])\n",
    "model.fit(x,y, epochs=150, batch_size=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n",
      "y_predict 값은\n",
      " [[711. 611. 101. 411.]\n",
      " [712. 612. 102. 412.]\n",
      " [713. 613. 103. 413.]\n",
      " [714. 614. 104. 414.]\n",
      " [715. 615. 105. 415.]\n",
      " [716. 616. 106. 416.]\n",
      " [717. 617. 107. 417.]\n",
      " [718. 618. 108. 418.]\n",
      " [719. 619. 109. 419.]\n",
      " [720. 620. 110. 420.]\n",
      " [721. 621. 111. 421.]\n",
      " [722. 622. 112. 422.]\n",
      " [723. 623. 113. 423.]\n",
      " [724. 624. 114. 424.]\n",
      " [725. 625. 115. 425.]\n",
      " [726. 626. 116. 426.]\n",
      " [727. 627. 117. 427.]\n",
      " [728. 628. 118. 428.]\n",
      " [729. 629. 119. 429.]\n",
      " [730. 630. 120. 430.]\n",
      " [731. 631. 121. 431.]\n",
      " [732. 632. 122. 432.]\n",
      " [733. 633. 123. 433.]\n",
      " [734. 634. 124. 434.]\n",
      " [735. 635. 125. 435.]\n",
      " [736. 636. 126. 436.]\n",
      " [737. 637. 127. 437.]\n",
      " [738. 638. 128. 438.]\n",
      " [739. 639. 129. 439.]\n",
      " [740. 640. 130. 440.]\n",
      " [741. 641. 131. 441.]\n",
      " [742. 642. 132. 442.]\n",
      " [743. 643. 133. 443.]\n",
      " [744. 644. 134. 444.]\n",
      " [745. 645. 135. 445.]\n",
      " [746. 646. 136. 446.]\n",
      " [747. 647. 137. 447.]\n",
      " [748. 648. 138. 448.]\n",
      " [749. 649. 139. 449.]\n",
      " [750. 650. 140. 450.]\n",
      " [751. 651. 141. 451.]\n",
      " [752. 652. 142. 452.]\n",
      " [753. 653. 143. 453.]\n",
      " [754. 654. 144. 454.]\n",
      " [755. 655. 145. 455.]\n",
      " [756. 656. 146. 456.]\n",
      " [757. 657. 147. 457.]\n",
      " [758. 658. 148. 458.]\n",
      " [759. 659. 149. 459.]\n",
      " [760. 660. 150. 460.]\n",
      " [761. 661. 151. 461.]\n",
      " [762. 662. 152. 462.]\n",
      " [763. 663. 153. 463.]\n",
      " [764. 664. 154. 464.]\n",
      " [765. 665. 155. 465.]\n",
      " [766. 666. 156. 466.]\n",
      " [767. 667. 157. 467.]\n",
      " [768. 668. 158. 468.]\n",
      " [769. 669. 159. 469.]\n",
      " [770. 670. 160. 470.]\n",
      " [771. 671. 161. 471.]\n",
      " [772. 672. 162. 472.]\n",
      " [773. 673. 163. 473.]\n",
      " [774. 674. 164. 474.]\n",
      " [775. 675. 165. 475.]\n",
      " [776. 676. 166. 476.]\n",
      " [777. 677. 167. 477.]\n",
      " [778. 678. 168. 478.]\n",
      " [779. 679. 169. 479.]\n",
      " [780. 680. 170. 480.]\n",
      " [781. 681. 171. 481.]\n",
      " [782. 682. 172. 482.]\n",
      " [783. 683. 173. 483.]\n",
      " [784. 684. 174. 484.]\n",
      " [785. 685. 175. 485.]\n",
      " [786. 686. 176. 486.]\n",
      " [787. 687. 177. 487.]\n",
      " [788. 688. 178. 488.]\n",
      " [789. 689. 179. 489.]\n",
      " [790. 690. 180. 490.]\n",
      " [791. 691. 181. 491.]\n",
      " [792. 692. 182. 492.]\n",
      " [793. 693. 183. 493.]\n",
      " [794. 694. 184. 494.]\n",
      " [795. 695. 185. 495.]\n",
      " [796. 696. 186. 496.]\n",
      " [797. 697. 187. 497.]\n",
      " [798. 698. 188. 498.]\n",
      " [799. 699. 189. 499.]\n",
      " [800. 700. 190. 500.]\n",
      " [801. 701. 191. 501.]\n",
      " [802. 702. 192. 502.]\n",
      " [803. 703. 193. 503.]\n",
      " [804. 704. 194. 504.]\n",
      " [805. 705. 195. 505.]\n",
      " [806. 706. 196. 506.]\n",
      " [807. 707. 197. 507.]\n",
      " [808. 708. 198. 508.]\n",
      " [809. 709. 199. 509.]\n",
      " [810. 710. 200. 510.]]\n",
      "loss : 1.1101732234237715e-05\n",
      "mse : 1.1101721611339599e-05\n",
      "acc : 1.0\n"
     ]
    }
   ],
   "source": [
    "loss, mse, acc = model.evaluate(x, y, batch_size=1)\n",
    "y_predict = model.predict(x)\n",
    "print('y_predict 값은\\n', np.round(y_predict))\n",
    "print(\"loss :\", loss)\n",
    "print(\"mse :\", mse)\n",
    "print(\"acc :\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"acc :\",accuracy_score(y.reshape(-1), np.round(y_predict).reshape(-1)) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "크... 이건 앙상블 적용 안하겠습니다 ^^ !\n",
    "복잡해보인다고 절대 좋은게 아니다, 가장 단순하고 직관적인 모델링, 코딩이 결국 내 것이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/150\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 6.6695e-06 - mse: 6.6695e-06 - acc: 1.0000 - val_loss: 1.7600e-06 - val_mse: 1.7600e-06 - val_acc: 1.0000\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 860us/step - loss: 1.4803e-06 - mse: 1.4803e-06 - acc: 1.0000 - val_loss: 3.8746e-06 - val_mse: 3.8746e-06 - val_acc: 1.0000\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 873us/step - loss: 2.2442e-07 - mse: 2.2442e-07 - acc: 1.0000 - val_loss: 1.1729e-07 - val_mse: 1.1729e-07 - val_acc: 1.0000\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 876us/step - loss: 7.7968e-08 - mse: 7.7968e-08 - acc: 1.0000 - val_loss: 2.6298e-08 - val_mse: 2.6298e-08 - val_acc: 1.0000\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 868us/step - loss: 1.9891e-08 - mse: 1.9891e-08 - acc: 1.0000 - val_loss: 4.6828e-09 - val_mse: 4.6828e-09 - val_acc: 1.0000\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 855us/step - loss: 6.4992e-09 - mse: 6.4992e-09 - acc: 1.0000 - val_loss: 5.3638e-09 - val_mse: 5.3638e-09 - val_acc: 1.0000\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 849us/step - loss: 3.7779e-09 - mse: 3.7779e-09 - acc: 1.0000 - val_loss: 6.0827e-09 - val_mse: 6.0827e-09 - val_acc: 1.0000\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 840us/step - loss: 3.7669e-09 - mse: 3.7669e-09 - acc: 1.0000 - val_loss: 6.0332e-09 - val_mse: 6.0332e-09 - val_acc: 1.0000\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 828us/step - loss: 3.0546e-09 - mse: 3.0546e-09 - acc: 1.0000 - val_loss: 5.0117e-09 - val_mse: 5.0117e-09 - val_acc: 1.0000\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 844us/step - loss: 3.6798e-09 - mse: 3.6798e-09 - acc: 1.0000 - val_loss: 4.1269e-09 - val_mse: 4.1269e-09 - val_acc: 1.0000\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 898us/step - loss: 3.4875e-09 - mse: 3.4875e-09 - acc: 1.0000 - val_loss: 5.0932e-09 - val_mse: 5.0932e-09 - val_acc: 1.0000\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 885us/step - loss: 3.4075e-09 - mse: 3.4075e-09 - acc: 1.0000 - val_loss: 6.4902e-09 - val_mse: 6.4902e-09 - val_acc: 1.0000\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 897us/step - loss: 3.3644e-09 - mse: 3.3644e-09 - acc: 1.0000 - val_loss: 6.1176e-09 - val_mse: 6.1176e-09 - val_acc: 1.0000\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 960us/step - loss: 3.6547e-09 - mse: 3.6547e-09 - acc: 1.0000 - val_loss: 4.9477e-09 - val_mse: 4.9477e-09 - val_acc: 1.0000\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 909us/step - loss: 3.6036e-09 - mse: 3.6036e-09 - acc: 1.0000 - val_loss: 6.4087e-09 - val_mse: 6.4087e-09 - val_acc: 1.0000\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 830us/step - loss: 3.7846e-09 - mse: 3.7846e-09 - acc: 1.0000 - val_loss: 5.7480e-09 - val_mse: 5.7480e-09 - val_acc: 1.0000\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 832us/step - loss: 3.5418e-09 - mse: 3.5418e-09 - acc: 1.0000 - val_loss: 5.6898e-09 - val_mse: 5.6898e-09 - val_acc: 1.0000\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 860us/step - loss: 2.9726e-09 - mse: 2.9726e-09 - acc: 1.0000 - val_loss: 5.2765e-09 - val_mse: 5.2765e-09 - val_acc: 1.0000\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 831us/step - loss: 4.4876e-09 - mse: 4.4876e-09 - acc: 1.0000 - val_loss: 4.8429e-09 - val_mse: 4.8429e-09 - val_acc: 1.0000\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 831us/step - loss: 4.2388e-09 - mse: 4.2388e-09 - acc: 1.0000 - val_loss: 4.6421e-09 - val_mse: 4.6421e-09 - val_acc: 1.0000\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 844us/step - loss: 3.4779e-09 - mse: 3.4779e-09 - acc: 1.0000 - val_loss: 5.4948e-09 - val_mse: 5.4948e-09 - val_acc: 1.0000\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 834us/step - loss: 2.9873e-09 - mse: 2.9873e-09 - acc: 1.0000 - val_loss: 4.3801e-09 - val_mse: 4.3801e-09 - val_acc: 1.0000\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 822us/step - loss: 3.5527e-09 - mse: 3.5527e-09 - acc: 1.0000 - val_loss: 6.1118e-09 - val_mse: 6.1118e-09 - val_acc: 1.0000\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 815us/step - loss: 3.2915e-09 - mse: 3.2915e-09 - acc: 1.0000 - val_loss: 5.3638e-09 - val_mse: 5.3638e-09 - val_acc: 1.0000\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 840us/step - loss: 3.5910e-09 - mse: 3.5910e-09 - acc: 1.0000 - val_loss: 3.8242e-09 - val_mse: 3.8242e-09 - val_acc: 1.0000\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 910us/step - loss: 4.2828e-09 - mse: 4.2828e-09 - acc: 1.0000 - val_loss: 4.3772e-09 - val_mse: 4.3772e-09 - val_acc: 1.0000\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 885us/step - loss: 3.1576e-09 - mse: 3.1576e-09 - acc: 1.0000 - val_loss: 3.7835e-09 - val_mse: 3.7835e-09 - val_acc: 1.0000\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 885us/step - loss: 3.2385e-09 - mse: 3.2385e-09 - acc: 1.0000 - val_loss: 6.1962e-09 - val_mse: 6.1962e-09 - val_acc: 1.0000\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 957us/step - loss: 4.2912e-09 - mse: 4.2912e-09 - acc: 1.0000 - val_loss: 5.3289e-09 - val_mse: 5.3289e-09 - val_acc: 1.0000\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 868us/step - loss: 4.7456e-09 - mse: 4.7456e-09 - acc: 1.0000 - val_loss: 5.5064e-09 - val_mse: 5.5064e-09 - val_acc: 1.0000\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 880us/step - loss: 5.1186e-09 - mse: 5.1186e-09 - acc: 1.0000 - val_loss: 4.4965e-09 - val_mse: 4.4965e-09 - val_acc: 1.0000\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 886us/step - loss: 3.6529e-09 - mse: 3.6529e-09 - acc: 1.0000 - val_loss: 5.8033e-09 - val_mse: 5.8033e-09 - val_acc: 1.0000\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 846us/step - loss: 4.3483e-09 - mse: 4.3483e-09 - acc: 1.0000 - val_loss: 5.4890e-09 - val_mse: 5.4890e-09 - val_acc: 1.0000\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 850us/step - loss: 3.5488e-09 - mse: 3.5488e-09 - acc: 1.0000 - val_loss: 4.4762e-09 - val_mse: 4.4762e-09 - val_acc: 1.0000\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 872us/step - loss: 3.9263e-09 - mse: 3.9263e-09 - acc: 1.0000 - val_loss: 4.4703e-09 - val_mse: 4.4703e-09 - val_acc: 1.0000\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 818us/step - loss: 3.7575e-09 - mse: 3.7575e-09 - acc: 1.0000 - val_loss: 4.0163e-09 - val_mse: 4.0163e-09 - val_acc: 1.0000\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 858us/step - loss: 3.4865e-09 - mse: 3.4865e-09 - acc: 1.0000 - val_loss: 4.4936e-09 - val_mse: 4.4936e-09 - val_acc: 1.0000\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 837us/step - loss: 3.4423e-09 - mse: 3.4423e-09 - acc: 1.0000 - val_loss: 4.9680e-09 - val_mse: 4.9680e-09 - val_acc: 1.0000\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 882us/step - loss: 3.1996e-09 - mse: 3.1996e-09 - acc: 1.0000 - val_loss: 5.3522e-09 - val_mse: 5.3522e-09 - val_acc: 1.0000\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 937us/step - loss: 3.9880e-09 - mse: 3.9880e-09 - acc: 1.0000 - val_loss: 4.9797e-09 - val_mse: 4.9797e-09 - val_acc: 1.0000\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 923us/step - loss: 3.4344e-09 - mse: 3.4344e-09 - acc: 1.0000 - val_loss: 5.4220e-09 - val_mse: 5.4220e-09 - val_acc: 1.0000\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 923us/step - loss: 3.7007e-09 - mse: 3.7007e-09 - acc: 1.0000 - val_loss: 5.8848e-09 - val_mse: 5.8848e-09 - val_acc: 1.0000\n",
      "Epoch 43/150\n",
      "80/80 [==============================] - 0s 899us/step - loss: 3.1116e-09 - mse: 3.1116e-09 - acc: 1.0000 - val_loss: 4.3277e-09 - val_mse: 4.3277e-09 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2ca6b26aa88>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping \n",
    "early_stopping = EarlyStopping(monitor='loss', patience=25, mode='auto')\n",
    "model.fit(x,y, epochs=150, batch_size=5,validation_split = 0.2,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n",
      "loss : 4.168250598013401e-09\n",
      "mse : 4.168250633540538e-09\n",
      "model acc : 1.0\n",
      "acc : 1.0\n"
     ]
    }
   ],
   "source": [
    "loss, mse, acc = model.evaluate(x, y, batch_size=1)\n",
    "y_predict = model.predict(x)\n",
    "# print('y_predict 값은\\n', np.round(y_predict))\n",
    "print(\"loss :\", loss)\n",
    "print(\"mse :\", mse)\n",
    "print(\"model acc :\", acc)\n",
    "print(\"acc :\",accuracy_score(y.reshape(-1), np.round(y_predict).reshape(-1)) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit에 callbacks=[early_stopping] 적용 전,\n",
    "loss : 1.1101732234237715e-05\n",
    "mse : 1.1101721611339599e-05\n",
    "model acc : 1.0\n",
    "acc : 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
