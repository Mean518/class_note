{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = ['너무 재밌어요', '참 최고에요', '참 잘 만든 영화에요',\n",
    "        '추천하고 싶은 영화입니다', '한 번 더 보고 싶네요', '글쎄요',\n",
    "        '별로에요', '생각보다 지루해요',  '연기가 어색해요',\n",
    "        '재미없어요', '너무 재미없다', '참 재밌네요']\n",
    "\n",
    "text_tokener = Tokenizer() \n",
    "\n",
    "# 1 fit_on_texts\n",
    "text_tokener.fit_on_texts(docs)\n",
    "\n",
    "# 1_1. word_index\n",
    "text_tokener.word_index\n",
    "\n",
    "# 1_2) texts_to_sequences 각 문장을 빈도순으로 숫자로 바꿔서 나타내줌\n",
    "x = text_tokener.texts_to_sequences(docs)\n",
    "x\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences # 패딩으로 와꾸 맞춰주기\n",
    "\n",
    "pad_x = pad_sequences(x, padding='post', value=0)\n",
    "pad_x \n",
    "# 알아서 최대 자릿수로 패딩을 채워준다.\n",
    "\n",
    "pad_x = pad_sequences(x, padding='pre', value=0) # 근데 1부터는 값(value)를 가지고 있으니까.. 0으로 해주는게 좋겠다\n",
    "pad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  2  3]\n",
      " [ 0  0  0  1  4]\n",
      " [ 0  1  5  6  7]\n",
      " [ 0  0  8  9 10]\n",
      " [11 12 13 14 15]\n",
      " [ 0  0  0  0 16]\n",
      " [ 0  0  0  0 17]\n",
      " [ 0  0  0 18 19]\n",
      " [ 0  0  0 20 21]\n",
      " [ 0  0  0  0 22]\n",
      " [ 0  0  0  2 23]\n",
      " [ 0  0  0  1 24]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 정리\n",
    "import numpy as np\n",
    "print(pad_x)\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0,0,1])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5개로 이루어진(패딩상태) 12개의 문장과 그에 따른 1, 0 매칭 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, LSTM, Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 5)\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "print(pad_x.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "현재 독립변수 x가 상대적인 크기를 갖고 있는 것이 아니므로\n",
    "원-핫 인코딩 처리를 해주어야하는데, 그러면 데이터cost가 너무 커지니까\n",
    "Embedding을 사용하여 벡터화를 해준다.\n",
    "따라서 0을 포함한 25개를 하는게 가장 경제적이라고 인식된다.\n",
    "**Embedding(25, 100, input_length=5)**\n",
    "각각 범주화 갯수, ouput node 갯수, input와꾸(띄어쓰기 갯수) 명시이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Embedding, Flatten, Dense 조합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5, 100)            2500      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 3,001\n",
      "Trainable params: 3,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(25, 100, input_length=5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary는 언제봐도 예쁘단말이지..? ㅎ ㅡ ㅎ 케케\n",
    "1) 임베딩은 3차원-> 3차원이다\n",
    "1-1) 임베딩 param은 벡터범주화 갯수 * output 이다 (여기서 25*100)\n",
    "2) 따라서 Dense로 연결할때는 Flatten이 필요하다\n",
    "3) Flatten은 이전 단계에서 input_shape명시를 요구하는 아이이다\n",
    "4) 따라서, 임베딩(와꾸포함) -> flatten -> Dense\n",
    "5) Dense param은 out * (in + 1 ) 였지~!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Embedding, LSTM, Dense 조합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         2500      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20)                9680      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(25, 100))\n",
    "model.add(LSTM(20))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) 3차원으로 보내서 LSTM이 바로 받을 수 있다.\n",
    "2) LSTM은 shape명시를 요구하지 않기 때문에 input_length 명시 안해줘도 된다.\n",
    "3) 임베딩 다음으로 주로 LSTM이 쓰인다는데 시계열이랑 관련지어서 얘기했는데 왜지?\n",
    "4) LSTM param은  out * ( in + out + 1 ) * 4 였지~!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Embedding, Conv1D, Dense 조합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 5, 100)            2500      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 4, 20)             4020      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 6,601\n",
      "Trainable params: 6,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(25, 100, input_length=5))\n",
    "model.add(Conv1D(20,2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) LSTM과 똑같은 3차원 들어가는 친구지만, \n",
    "2) 3 -> 3 차원이라 Flatten 써줘야하고 (걍 자동으로 바뀌는 LSTM이 별종임)\n",
    "3) flatten이 들어가니까 input_shape 명시해줘야함\n",
    "2) 다만 CNN 구조에 대한 명확한 이해가 없어서 knerl_size 쓸때 너무 애매하게 쓰게된다 -> 공부할때 params도 같이 공부 ㄱㄱ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bitcamp\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 0.6975 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.6844 - acc: 0.5833\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.6715 - acc: 0.7500\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 0s 331us/step - loss: 0.6587 - acc: 0.7500\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.6461 - acc: 0.8333\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.6336 - acc: 0.8333\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 0s 247us/step - loss: 0.6210 - acc: 0.9167\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.6084 - acc: 0.9167\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 0s 252us/step - loss: 0.5957 - acc: 0.9167\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.5828 - acc: 1.0000\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.5698 - acc: 1.0000\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.5566 - acc: 1.0000\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.5431 - acc: 1.0000\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.5295 - acc: 1.0000\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 0s 168us/step - loss: 0.5156 - acc: 1.0000\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 0s 330us/step - loss: 0.5015 - acc: 1.0000\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.4872 - acc: 1.0000\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.4726 - acc: 1.0000\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.4579 - acc: 1.0000\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 0s 252us/step - loss: 0.4431 - acc: 1.0000\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 0s 247us/step - loss: 0.4281 - acc: 1.0000\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 0s 247us/step - loss: 0.4130 - acc: 1.0000\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.3979 - acc: 1.0000\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.3828 - acc: 1.0000\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 0s 252us/step - loss: 0.3678 - acc: 1.0000\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 0s 252us/step - loss: 0.3528 - acc: 1.0000\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 0s 169us/step - loss: 0.3380 - acc: 1.0000\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.3234 - acc: 1.0000\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.3090 - acc: 1.0000\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 0s 170us/step - loss: 0.2948 - acc: 1.0000\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "acc : 1.0\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "model.fit(pad_x, labels, epochs=30)\n",
    "acc = model.evaluate(pad_x,labels)[1]\n",
    "print('acc :',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결론 : 범주화 갯수, ouput node 갯수, input와꾸(띄어쓰기 갯수) 명시가 embedding의 순서고 범주화 갯수와 띄어쓰기 갯수 (즉 list shape가 두 차원으로 움직임) 를 이해하는게 제일 중요한 부분임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
